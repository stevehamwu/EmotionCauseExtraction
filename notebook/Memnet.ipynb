{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T11:09:13.738902Z",
     "start_time": "2019-03-27T11:09:13.405154Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T11:09:14.028006Z",
     "start_time": "2019-03-27T11:09:13.740350Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T23:23:20.263350Z",
     "start_time": "2019-03-19T23:23:20.259052Z"
    }
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:30:44.616798Z",
     "start_time": "2019-03-26T15:30:44.602525Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read vocab\n",
    "with open('/data/wujipeng/ec/data/raw_data/ltp_vocab.txt') as f:\n",
    "    word_unk = f.readline().strip()\n",
    "    vocab = ['<pad>', word_unk] + f.readline().strip().split(' ')\n",
    "i2w = {i: w for i, w in enumerate(vocab)}\n",
    "w2i = {i: w for w, i in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:30:45.923979Z",
     "start_time": "2019-03-26T15:30:45.909326Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load pos vocab\n",
    "pos_label = [\n",
    "    'AAAA', 'AAAB', 'AAAC', 'AAAD', 'AABA', 'AABB', 'AABC', 'AABD', 'AACA',\n",
    "    'AACB', 'AACC', 'AACD', 'AADA', 'AADB', 'AADC', 'AADD', 'ABAA', 'ABAB',\n",
    "    'ABAC', 'ABAD', 'ABBA', 'ABBB', 'ABBC', 'ABBD', 'ABCA', 'ABCB', 'ABCC',\n",
    "    'ABCD', 'ABDA', 'ABDB', 'ABDC', 'ABDD', 'ACAA', 'ACAB', 'ACAC', 'ACAD',\n",
    "    'ACBA', 'ACBB', 'ACBC', 'ACBD', 'ACCA', 'ACCB', 'ACCC', 'ACCD', 'ACDA',\n",
    "    'ACDB', 'ACDC', 'ACDD', 'ADAA', 'ADAB', 'ADAC', 'ADAD', 'ADBA', 'ADBB',\n",
    "    'ADBC', 'ADBD', 'ADCA', 'ADCB', 'ADCC', 'ADCD', 'ADDA', 'ADDB', 'ADDC',\n",
    "    'ADDD', 'BAAA', 'BAAB', 'BAAC', 'BAAD', 'BABA', 'BABB', 'BABC', 'BABD',\n",
    "    'BACA', 'BACB', 'BACC', 'BACD', 'BADA', 'BADB', 'BADC', 'BADD', 'BBAA',\n",
    "    'BBAB', 'BBAC', 'BBAD', 'BBBA', 'BBBB', 'BBBC', 'BBBD', 'BBCA', 'BBCB',\n",
    "    'BBCC', 'BBCD', 'BCAA', 'BCAB', 'BCAC', 'BCAD', 'BDAA', 'BDAB', 'BDAC',\n",
    "    'BDAD', 'BCBA', 'BCBB', 'BCBC'\n",
    "]\n",
    "p2l = {i+1: w for i, w in enumerate(pos_label)}\n",
    "l2p = {w: i+1 for i, w in enumerate(pos_label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:30:47.479141Z",
     "start_time": "2019-03-26T15:30:47.475857Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab += pos_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T10:26:55.421685Z",
     "start_time": "2019-03-20T10:26:55.412780Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/data/wujipeng/ec/data/raw_data/memnet_ltp_vocab.txt', 'w') as f:\n",
    "    f.write('<unk>\\n')\n",
    "    f.write(' '.join(vocab[2:]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:30:55.949815Z",
     "start_time": "2019-03-26T15:30:50.316757Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LTP_DATA_DIR = '/home/wujipeng/data/ltp_data_v3.4.0/'  # ltp模型目录的路径\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')\n",
    "\n",
    "from pyltp import Segmentor, Postagger\n",
    "\n",
    "segmentor = Segmentor()  # 初始化实例\n",
    "segmentor.load(cws_model_path)  # 加载模型\n",
    "postagger = Postagger()\n",
    "postagger.load(pos_model_path)\n",
    "\n",
    "def ltp_seg(sentence):\n",
    "    words, natures = [], []\n",
    "    for sen in sentence.split('\\x01'):\n",
    "        seg_words = segmentor.segment(sen)\n",
    "        seg_natures = postagger.postag(seg_words)\n",
    "        words += list(seg_words) + ['\\x01']\n",
    "        natures += [nature[0] for nature in list(seg_natures)] + ['\\x01']\n",
    "    \n",
    "    return words[:-1], natures[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:36:39.765353Z",
     "start_time": "2019-03-26T15:36:39.752954Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_corpus(raw_corpus):\n",
    "    #  Hanlp分词\n",
    "    print('开始分词...')\n",
    "    corpus = []\n",
    "    natures = []\n",
    "    with open('/data/wujipeng/ec/data/raw_data/ltp_corpus.txt', 'w') as fc:\n",
    "        with open('/data/wujipeng/ec/data/raw_data/ltp_natures.txt', 'w') as fn:\n",
    "            for line in raw_corpus:\n",
    "                words, tags = ltp_seg(line)\n",
    "                words = ' '.join(words)\n",
    "                tags = ' '.join(tags)\n",
    "                corpus.append(words)\n",
    "                natures.append(tags)\n",
    "                fc.write(words + '\\n')\n",
    "                fn.write(tags + '\\n')\n",
    "    print('完成分词')\n",
    "    return corpus, natures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:39:26.802191Z",
     "start_time": "2019-03-26T15:39:26.767435Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "pos_label = [\n",
    "    'AAAA', 'AAAB', 'AAAC', 'AAAD', 'AABA', 'AABB', 'AABC', 'AABD', 'AACA',\n",
    "    'AACB', 'AACC', 'AACD', 'AADA', 'AADB', 'AADC', 'AADD', 'ABAA', 'ABAB',\n",
    "    'ABAC', 'ABAD', 'ABBA', 'ABBB', 'ABBC', 'ABBD', 'ABCA', 'ABCB', 'ABCC',\n",
    "    'ABCD', 'ABDA', 'ABDB', 'ABDC', 'ABDD', 'ACAA', 'ACAB', 'ACAC', 'ACAD',\n",
    "    'ACBA', 'ACBB', 'ACBC', 'ACBD', 'ACCA', 'ACCB', 'ACCC', 'ACCD', 'ACDA',\n",
    "    'ACDB', 'ACDC', 'ACDD', 'ADAA', 'ADAB', 'ADAC', 'ADAD', 'ADBA', 'ADBB',\n",
    "    'ADBC', 'ADBD', 'ADCA', 'ADCB', 'ADCC', 'ADCD', 'ADDA', 'ADDB', 'ADDC',\n",
    "    'ADDD', 'BAAA', 'BAAB', 'BAAC', 'BAAD', 'BABA', 'BABB', 'BABC', 'BABD',\n",
    "    'BACA', 'BACB', 'BACC', 'BACD', 'BADA', 'BADB', 'BADC', 'BADD', 'BBAA',\n",
    "    'BBAB', 'BBAC', 'BBAD', 'BBBA', 'BBBB', 'BBBC', 'BBBD', 'BBCA', 'BBCB',\n",
    "    'BBCC', 'BBCD', 'BCAA', 'BCAB', 'BCAC', 'BCAD', 'BDAA', 'BDAB', 'BDAC',\n",
    "    'BDAD', 'BCBA', 'BCBB', 'BCBC'\n",
    "]\n",
    "\n",
    "def build_vocab(corpus):\n",
    "    # build vocab\n",
    "    vocab_cnt = Counter()\n",
    "    for line in corpus:\n",
    "        for word in line.replace('\\x01', '').split(' '):\n",
    "            if word.strip():\n",
    "                vocab_cnt[word] += 1\n",
    "#     vocab = [word for word, freq in vocab_cnt.most_common()]\n",
    "    vocab = [word for word, freq in vocab_cnt.items()]\n",
    "    vocab += pos_label\n",
    "    with open('/data/wujipeng/ec/data/raw_data/memnet_ltp_vocab.txt', 'w') as f:\n",
    "        f.write('<unk>\\n')\n",
    "        f.write(' '.join(vocab))\n",
    "    print('词典大小: ', len(vocab) + 2)\n",
    "    print('保存词典')\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:31:04.440243Z",
     "start_time": "2019-03-26T15:31:04.436022Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_data(data):\n",
    "    data['id'] = list(range(1, len(data) + 1))\n",
    "    data[['id', 'clause', 'nature', 'keyword', 'emotion', 'clause_pos', 'label']].to_csv(\n",
    "        '/data/wujipeng/ec/data/han/memnet_ltp_processed_data.csv', index=False)\n",
    "    print('保存处理后数据')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:39:36.679758Z",
     "start_time": "2019-03-26T15:39:28.418992Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始分词...\n",
      "完成分词\n",
      "词典大小:  19909\n",
      "保存词典\n",
      "保存处理后数据\n"
     ]
    }
   ],
   "source": [
    "data_root = '/data/wujipeng/ec/data/'\n",
    "data = pd.read_csv(os.path.join(data_root, 'raw_data', 'process_data_3.csv'), index_col=0)\n",
    "corpus = [text.replace(' ', '') for text in data['clause'].tolist()]\n",
    "keyword = data['keyword'].tolist()\n",
    "poses = [list(map(int, pos.split(' '))) for pos in data['clause_pos'].tolist()]\n",
    "min_pos = min(min(poses))\n",
    "poses = [' '.join([str(p - min_pos + 1) for p in pos]) for pos in poses]\n",
    "\n",
    "# corpus = load_corpus(corpus)\n",
    "corpus, natures = load_corpus(corpus)\n",
    "vocab = build_vocab(corpus + keyword)\n",
    "\n",
    "data['clause'] = corpus\n",
    "data['nature'] = natures\n",
    "data['keyword'] = keyword\n",
    "data['clause_pos'] = poses\n",
    "save_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:20:18.285561Z",
     "start_time": "2019-03-20T20:20:18.263227Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read vocab\n",
    "with open('/data/wujipeng/ec/data/raw_data/memnet_ltp_vocab.txt') as f:\n",
    "    word_unk = f.readline().strip()\n",
    "    vocab = ['<pad>', word_unk] + f.readline().strip().split(' ')\n",
    "i2w = {i: w for i, w in enumerate(vocab)}\n",
    "w2i = {i: w for w, i in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T09:49:35.064196Z",
     "start_time": "2019-03-27T09:49:35.050942Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def load_data(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError\n",
    "    data = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data.append(line.strip().split(','))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:07:21.025357Z",
     "start_time": "2019-03-20T20:07:21.000324Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "raw_data = load_data('/data/wujipeng/ec/data/ltp_test/train_set.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:12:20.159519Z",
     "start_time": "2019-03-20T20:12:19.923307Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for item in raw_data:\n",
    "    n_id, n_clauses, n_natures, n_keyword, n_emotion, n_pos, n_label = item\n",
    "    n_id = int(n_id)\n",
    "    n_clauses = [clause.strip().split(' ') for clause in n_clauses.split('\\x01')]\n",
    "    n_natures = [nature.strip().split(' ') for nature in n_natures.split('\\x01')]\n",
    "    n_keyword = n_keyword.replace(' ', '')\n",
    "    n_pos = list(map(int, n_pos.strip().split(' ')))\n",
    "    n_label = list(map(int, n_label.strip().split(' ')))\n",
    "    for cid, (clause, nature, pos, label) in enumerate(zip(n_clauses, n_natures, n_pos, n_label)):\n",
    "        data.append(tuple([n_id, cid+1, clause, nature, n_keyword, n_emotion, pos, label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:12:21.604185Z",
     "start_time": "2019-03-20T20:12:21.597439Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 1,\n",
       " ['当', '我', '看到', '建议', '被', '采纳'],\n",
       " ['p', 'r', 'v', 'n', 'p', 'v'],\n",
       " '激动',\n",
       " '0',\n",
       " 63,\n",
       " 0)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:13:11.947809Z",
     "start_time": "2019-03-20T20:13:11.932102Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get item\n",
    "batched_data = []\n",
    "for item in data:\n",
    "    batched_data.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:18:50.406002Z",
     "start_time": "2019-03-20T20:18:50.386184Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word2idx(words, batched=False):\n",
    "    if not batched:\n",
    "        indices = [w2i[w] if w in w2i else w2i[word_unk] for w in words]\n",
    "    else:\n",
    "        indices = [[w2i[w] if w in w2i else w2i[word_unk] for w in item] for item in words]\n",
    "    return indices\n",
    "\n",
    "def idx2word(indices, batched=False):\n",
    "    if not batched:\n",
    "        words = [i2w[i] for i in indices]\n",
    "    else:\n",
    "        words = [[i2w[i] for i in item] for item in indices]\n",
    "    return words\n",
    "\n",
    "def pos2label(poses, batched=False):\n",
    "    if not batched:\n",
    "        indices = [p2l[p] for p in poses]\n",
    "    else:\n",
    "        indices = [[p2l[p] for p in pos] for pos in poses]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:52:56.925281Z",
     "start_time": "2019-03-20T20:52:56.908790Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "memory_size = 40\n",
    "sequence_size = 3\n",
    "batch_size = 16\n",
    "batch_data = list(zip(*batched_data[:batch_size-6]))\n",
    "ids, cids, clauses, natures, keywords, emotions, poses, labels = batch_data\n",
    "ids = list(ids)\n",
    "cids = list(cids)\n",
    "clauses = word2idx(clauses, batched=True)\n",
    "keywords = word2idx(keywords, batched=False)\n",
    "emotions = list(map(int, emotions))\n",
    "poses = word2idx(pos2label(poses, batched=False))\n",
    "labels = list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:40:45.369108Z",
     "start_time": "2019-03-20T20:40:45.361734Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T15:26:43.081499Z",
     "start_time": "2019-03-20T15:26:43.070565Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pad_memory(sequences, poses, memory_size, sequence_size, pad=0):\n",
    "    paded_sequences = []\n",
    "    for sequence, pos in zip(sequences, poses):\n",
    "        if len(sequence) < sequence_size:\n",
    "            sequence += [pad] * (sequence_size - len(sequence))\n",
    "        paded_sequence = [sequence[:sequence_size]]\n",
    "        for i in range(memory_size-1):\n",
    "            if i < len(sequence) - sequence_size + 1:\n",
    "                paded_sequence += [sequence[i:i + 3]]\n",
    "            elif i == len(sequence) - sequence_size + 1:\n",
    "                paded_sequence += [[pos] * 3]\n",
    "            else:\n",
    "                paded_sequence += [[pad] * 3]\n",
    "        paded_sequences.append(np.array(paded_sequence))\n",
    "    return paded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:52:58.802338Z",
     "start_time": "2019-03-20T20:52:58.792917Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clauses = pad_memory(clauses, poses, memory_size, sequence_size, pad=0)\n",
    "keywords = [[keyword] * sequence_size for keyword in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:52:59.040536Z",
     "start_time": "2019-03-20T20:52:59.032614Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(ids) < batch_size:\n",
    "    bs = batch_size - len(ids)\n",
    "    ids += [0] * bs\n",
    "    cids += [0] * bs\n",
    "    clauses += [[[0] * sequence_size] * memory_size] * bs\n",
    "    keywords += [[0] * sequence_size] * bs\n",
    "    emotions += [0] * bs\n",
    "    labels += [-100] * bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T09:50:24.371802Z",
     "start_time": "2019-03-27T09:50:24.361422Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/data/wujipeng/ec/data/raw_data/memnet_ltp_vocab.txt', 'r') as f:\n",
    "    vocab = ['<pad>', f.readline().strip()]\n",
    "    vocab = f.readline().strip().split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T09:50:30.338896Z",
     "start_time": "2019-03-27T09:50:30.316187Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19909"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted(vocab)) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T09:50:33.588512Z",
     "start_time": "2019-03-27T09:50:33.115804Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取预训练Embbeding\n",
      "Embedding shape (19909, 20)\n",
      "Embedding rate: 62.58%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from gensim.models import KeyedVectors\n",
    "data_root = '/data/wujipeng/ec/data/'\n",
    "print('读取预训练Embbeding')\n",
    "word2vec = KeyedVectors.load_word2vec_format('/data/wujipeng/ec/data/embedding/seg_resource.bin', binary=False)\n",
    "dim = word2vec.vector_size\n",
    "embedding = [np.zeros(dim), np.random.normal(loc=0., scale=0.1, size=dim)]  # pad unk\n",
    "cnt = 0\n",
    "for word in vocab:\n",
    "    if word2vec.vocab.get(word):\n",
    "        embedding.append(word2vec.get_vector(word))\n",
    "        cnt += 1\n",
    "    else:\n",
    "        embedding.append(np.random.normal(loc=0., scale=0.1, size=dim))\n",
    "embedding = np.array(embedding)\n",
    "print('Embedding shape', embedding.shape)\n",
    "print('Embedding rate: {:.2f}%'.format(cnt / len(vocab) * 100))\n",
    "pickle.dump(embedding,\n",
    "            open(os.path.join(data_root, 'embedding', '{}_embedding{}d.pkl'.format('memnet', dim)), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:08:58.239248Z",
     "start_time": "2019-03-27T14:08:57.844045Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path = ['..'] + sys.path\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from utils.data.process import load_data, pad_sequence, pad_memory\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MECDataset(Dataset):\n",
    "    def __init__(self, data_root, vocab_root, batch_size=16, train=True):\n",
    "        super(MECDataset, self).__init__()\n",
    "        self.train = train\n",
    "        self.data_path = os.path.join(data_root, '{}_set.txt'.format('train' if train else 'val'))\n",
    "        self.vocab_root = vocab_root\n",
    "        self.batch_size = batch_size\n",
    "        self.data = []\n",
    "        self.read_data()\n",
    "        self.read_vocab()\n",
    "        self.read_pos()\n",
    "\n",
    "    def read_data(self):\n",
    "        data = load_data(self.data_path)\n",
    "        for item in data:\n",
    "            n_id, n_clauses, n_natures, n_keyword, n_emotion, n_pos, n_label = item\n",
    "            n_id = int(n_id)\n",
    "            n_clauses = [clause.strip().split(' ') for clause in n_clauses.split('\\x01')]\n",
    "            n_natures = [nature.strip().split(' ') for nature in n_natures.split('\\x01')]\n",
    "            n_keyword = n_keyword.replace(' ', '')\n",
    "            n_pos = list(map(int, n_pos.strip().split(' ')))\n",
    "            n_label = list(map(int, n_label.strip().split(' ')))\n",
    "            for cid, (clause, nature, pos, label) in enumerate(zip(n_clauses, n_natures, n_pos, n_label)):\n",
    "                self.data.append(tuple([n_id, cid + 1, clause, nature, n_keyword, n_emotion, pos, label]))\n",
    "\n",
    "    def read_vocab(self):\n",
    "        if os.path.isdir(self.vocab_root):\n",
    "            self.vocab_root = os.path.join(self.vocab_root, 'vocab.txt')\n",
    "        with open(self.vocab_root, 'r') as f:\n",
    "            self.word_unk = f.readline().strip()\n",
    "            self.vocab = ['<pad>', self.word_unk] + f.readline().strip().split(' ')\n",
    "        self.i2w = {i: w for i, w in enumerate(self.vocab)}\n",
    "        self.w2i = {w: i for i, w in enumerate(self.vocab)}\n",
    "\n",
    "    def read_pos(self):\n",
    "        pos_label = [\n",
    "            'AAAA', 'AAAB', 'AAAC', 'AAAD', 'AABA', 'AABB', 'AABC', 'AABD', 'AACA',\n",
    "            'AACB', 'AACC', 'AACD', 'AADA', 'AADB', 'AADC', 'AADD', 'ABAA', 'ABAB',\n",
    "            'ABAC', 'ABAD', 'ABBA', 'ABBB', 'ABBC', 'ABBD', 'ABCA', 'ABCB', 'ABCC',\n",
    "            'ABCD', 'ABDA', 'ABDB', 'ABDC', 'ABDD', 'ACAA', 'ACAB', 'ACAC', 'ACAD',\n",
    "            'ACBA', 'ACBB', 'ACBC', 'ACBD', 'ACCA', 'ACCB', 'ACCC', 'ACCD', 'ACDA',\n",
    "            'ACDB', 'ACDC', 'ACDD', 'ADAA', 'ADAB', 'ADAC', 'ADAD', 'ADBA', 'ADBB',\n",
    "            'ADBC', 'ADBD', 'ADCA', 'ADCB', 'ADCC', 'ADCD', 'ADDA', 'ADDB', 'ADDC',\n",
    "            'ADDD', 'BAAA', 'BAAB', 'BAAC', 'BAAD', 'BABA', 'BABB', 'BABC', 'BABD',\n",
    "            'BACA', 'BACB', 'BACC', 'BACD', 'BADA', 'BADB', 'BADC', 'BADD', 'BBAA',\n",
    "            'BBAB', 'BBAC', 'BBAD', 'BBBA', 'BBBB', 'BBBC', 'BBBD', 'BBCA', 'BBCB',\n",
    "            'BBCC', 'BBCD', 'BCAA', 'BCAB', 'BCAC', 'BCAD', 'BDAA', 'BDAB', 'BDAC',\n",
    "            'BDAD', 'BCBA', 'BCBB', 'BCBC'\n",
    "        ]\n",
    "        self.p2l = {i + 1: w for i, w in enumerate(pos_label)}\n",
    "        self.l2p = {w: i + 1 for i, w in enumerate(pos_label)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def word2idx(self, words, batched=False):\n",
    "        if not batched:\n",
    "            indices = [self.w2i[w] if w in self.w2i else self.w2i[self.word_unk] for w in words]\n",
    "        else:\n",
    "            indices = [[self.w2i[w] if w in self.w2i else self.w2i[self.word_unk] for w in item] for item in words]\n",
    "        return indices\n",
    "\n",
    "    def idx2word(self, indices, batched=False):\n",
    "        if not batched:\n",
    "            words = [self.i2w[i] for i in indices]\n",
    "        else:\n",
    "            words = [[self.i2w[i] for i in item] for item in indices]\n",
    "        return words\n",
    "\n",
    "    def pos2label(self, poses, batched=False):\n",
    "        if not batched:\n",
    "            indices = [self.p2l[p] for p in poses]\n",
    "        else:\n",
    "            indices = [[self.p2l[p] for p in pos] for pos in poses]\n",
    "        return indices\n",
    "\n",
    "    def collate_fn(self, batch_data, pad=True, memory_size=41, sequence_size=3, batch_size=16):\n",
    "        batch_data = list(zip(*batch_data))\n",
    "        ids, cids, clauses, natures, keywords, emotions, poses, labels = batch_data\n",
    "        ids = list(ids)\n",
    "        cids = list(cids)\n",
    "        clauses = self.word2idx(clauses, batched=True)\n",
    "        keywords = self.word2idx(keywords, batched=False)\n",
    "        emotions = list(map(int, emotions))\n",
    "        poses = self.word2idx(self.pos2label(poses, batched=False))\n",
    "        labels = list(labels)\n",
    "        if pad:\n",
    "            clauses = pad_memory(clauses, poses, memory_size, sequence_size, pad=0)\n",
    "            keywords = [[keyword] * sequence_size for keyword in keywords]\n",
    "            if len(ids) < batch_size:\n",
    "                bs = batch_size - len(ids)\n",
    "                ids += [0] * bs\n",
    "                cids += [0] * bs\n",
    "                clauses += [[[0] * sequence_size] * memory_size] * bs\n",
    "                keywords += [[0] * sequence_size] * bs\n",
    "                emotions += [0] * bs\n",
    "                labels += [-100] * bs\n",
    "\n",
    "        return ids, cids, np.array(clauses), natures, np.array(keywords), np.array(emotions), poses, np.array(\n",
    "            labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def batch2input(batch):\n",
    "        return batch[2], batch[4]\n",
    "\n",
    "    @staticmethod\n",
    "    def batch2target(batch):\n",
    "        return batch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:08:58.880144Z",
     "start_time": "2019-03-27T14:08:58.874186Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.dataset.memnet import MECDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:08:59.140958Z",
     "start_time": "2019-03-27T14:08:59.131619Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils.dataloader.memnet import MECDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:08:59.646352Z",
     "start_time": "2019-03-27T14:08:59.444975Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = MECDataset(data_root='/data/wujipeng/ec/data/ltp_static/static.1/', vocab_root='/data/wujipeng/ec/data/raw_data/memnet_ltp_vocab.txt', batch_size=16, train=True)\n",
    "eval_dataset = MECDataset(data_root='/data/wujipeng/ec/data/ltp_static/static.1/', vocab_root='/data/wujipeng/ec/data/raw_data/memnet_ltp_vocab.txt', batch_size=16, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:09:08.006752Z",
     "start_time": "2019-03-27T14:09:08.002745Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = train_dataset + eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:09:08.440678Z",
     "start_time": "2019-03-27T14:09:08.252804Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = list(zip(*dataset))[2] + list(zip(*dataset))[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:09:08.464569Z",
     "start_time": "2019-03-27T14:09:08.445662Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_size = max([len(sentence) for sentence in sentences])\n",
    "memory_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:09:09.786413Z",
     "start_time": "2019-03-27T14:09:09.662951Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class ECDataLoader:\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 memory_size,\n",
    "                 sequence_size,\n",
    "                 batch_size=16,\n",
    "                 shuffle=True,\n",
    "                 auto_refresh=True,\n",
    "                 collate_fn=None):\n",
    "        self.dataset = dataset\n",
    "        self.size = len(dataset)\n",
    "        self.memory_size = memory_size\n",
    "        self.sequence_size = sequence_size\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.batches = None\n",
    "        self.num_batches = None\n",
    "        self.auto_refresh = auto_refresh\n",
    "        self.collate_fn = collate_fn\n",
    "        self.inst_count = 0\n",
    "        self.batch_count = 0\n",
    "        self._curr_batch = 0\n",
    "        self._curr_num_insts = None\n",
    "        self.dataset = list(dataset)\n",
    "        if self.auto_refresh:\n",
    "            self.refresh()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def next(self):\n",
    "        if self._curr_batch and self._curr_batch + 1 >= self.num_batches:\n",
    "            if self.auto_refresh:\n",
    "                self.refresh()\n",
    "            raise StopIteration\n",
    "        data = self.get_data()\n",
    "        return data\n",
    "\n",
    "    def get_data(self):\n",
    "        self._curr_batch = (\n",
    "            self._curr_batch + 1) if self._curr_batch is not None else 0\n",
    "        self._curr_num_insts = len(self.batches[self._curr_batch])\n",
    "\n",
    "        self.inst_count += self._curr_num_insts\n",
    "        self.batch_count += 1\n",
    "        data = self.batches[self._curr_batch]\n",
    "        if self.collate_fn:\n",
    "            data = self.collate_fn(\n",
    "                data,\n",
    "                memory_size=self.memory_size,\n",
    "                sequence_size=self.sequence_size,\n",
    "                batch_size=self.batch_size)\n",
    "        return data\n",
    "\n",
    "    def refresh(self):\n",
    "        self.batches = []\n",
    "        batch_start = 0\n",
    "        for i in range(self.size // self.batch_size):\n",
    "            self.batches.append(\n",
    "                self.dataset[batch_start:batch_start + self.batch_size])\n",
    "            batch_start += self.batch_size\n",
    "        if batch_start != self.size:\n",
    "            self.batches.append(self.dataset[batch_start:])\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.batches)\n",
    "        self.num_batches = len(self.batches)\n",
    "        self._curr_batch = None\n",
    "        self._curr_num_insts = None\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"\n",
    "        Warning! side effect: np_randomstate will influence other\n",
    "            potion of the program.\n",
    "        \"\"\"\n",
    "        state = {\n",
    "            \"batch_size\": self.batch_size,\n",
    "            \"shuffle\": self.shuffle,\n",
    "            \"batches\": self.batches,\n",
    "            \"num_batches\": self.num_batches,\n",
    "            \"auto_refresh\": self.auto_refresh,\n",
    "            \"inst_count\": self.inst_count,\n",
    "            \"batch_count\": self.batch_count,\n",
    "            \"_curr_batch\": self._curr_batch,\n",
    "            \"_curr_num_insts\": self._curr_num_insts,\n",
    "            \"np_randomstate\": np.random.get_state(),\n",
    "        }\n",
    "        return state\n",
    "\n",
    "    def load_state_dict(self, state):\n",
    "        \"\"\"\n",
    "        Warning! side effect: np_randomstate will influence other\n",
    "            potion of the program.\n",
    "        \"\"\"\n",
    "        self.batch_size = state[\"batch_size\"]\n",
    "        self.shuffle = state[\"shuffle\"]\n",
    "        self.batches = state[\"batches\"]\n",
    "        self.num_batches = state[\"num_batches\"]\n",
    "        self.auto_refresh = state[\"auto_refresh\"]\n",
    "        self.inst_count = state[\"inst_count\"]\n",
    "        self.batch_count = state[\"batch_count\"]\n",
    "        self._curr_batch = state[\"_curr_batch\"]\n",
    "        self._curr_num_insts = state[\"_curr_num_insts\"]\n",
    "        np.random.set_state(state[\"np_randomstate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:09:10.251583Z",
     "start_time": "2019-03-27T14:09:10.234852Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = ECDataLoader(dataset=train_dataset, memory_size=memory_size, sequence_size=3, batch_size=16, shuffle=True, collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T08:33:34.143152Z",
     "start_time": "2019-03-21T08:33:34.133110Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1755"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T08:33:38.331070Z",
     "start_time": "2019-03-21T08:33:38.320677Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:56:26.344794Z",
     "start_time": "2019-03-20T20:56:26.338794Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stories, queries = train_dataset.batch2input(batch)\n",
    "stories, queries = torch.from_numpy(stories), torch.from_numpy(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:56:27.365329Z",
     "start_time": "2019-03-20T20:56:27.356159Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 40, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T20:56:28.556927Z",
     "start_time": "2019-03-20T20:56:28.548378Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:09:13.331516Z",
     "start_time": "2019-03-27T14:09:13.322913Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:09:13.511849Z",
     "start_time": "2019-03-27T14:09:13.507036Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:09:13.720957Z",
     "start_time": "2019-03-27T14:09:13.716511Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda: 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:09:13.973211Z",
     "start_time": "2019-03-27T14:09:13.939061Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_loader = MECDataLoader(dataset=train_dataset, memory_size=memory_size, sequence_size=3, batch_size=16, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "eval_loader = MECDataLoader(dataset=eval_dataset, memory_size=memory_size, sequence_size=3, batch_size=16, shuffle=False, collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T16:39:44.279410Z",
     "start_time": "2019-03-21T16:39:44.273205Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "vocab_size = 19909\n",
    "sentence_size = 3\n",
    "memory_size = 40\n",
    "embedding_dim = 20\n",
    "num_classes = 2\n",
    "hops = 3\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T16:08:00.637134Z",
     "start_time": "2019-03-21T16:08:00.628167Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def position_encoding(sentence_size, embedding_size):\n",
    "    encoding = np.ones((embedding_size, sentence_size), dtype=np.float32)\n",
    "    ls = sentence_size + 1\n",
    "    le = embedding_size + 1\n",
    "    for i in range(1, le):\n",
    "        for j in range(1, ls):\n",
    "            encoding[i - 1, j - 1] = (i - (le - 1) / 2) * (j - (ls - 1) / 2)\n",
    "    encoding = 1 + 4 * encoding / embedding_size / sentence_size\n",
    "    return np.transpose(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T16:14:10.501748Z",
     "start_time": "2019-03-21T16:14:10.487021Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19909, 20)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = pickle.load(open('/data/wujipeng/ec/data/embedding/memnet_embedding20d.pkl', 'rb')).astype(np.float32)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T15:52:48.276831Z",
     "start_time": "2019-03-21T15:52:48.269306Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0).from_pretrained(torch.from_numpy(embeddings), freeze=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T21:00:26.459639Z",
     "start_time": "2019-03-20T21:00:26.454480Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoding = torch.from_numpy(position_encoding(1, 3 * embedding_dim)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T21:00:26.714625Z",
     "start_time": "2019-03-20T21:00:26.710389Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dp = nn.Dropout(dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T21:00:27.113377Z",
     "start_time": "2019-03-20T21:00:27.108448Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc = nn.Linear(3 * embedding_dim, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T21:00:24.860678Z",
     "start_time": "2019-03-20T21:00:22.135810Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 40, 3]), torch.Size([16, 3]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories, queries = stories.to(device), queries.to(device)\n",
    "stories.size(), queries.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T21:01:04.526255Z",
     "start_time": "2019-03-20T21:01:04.512290Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 20]), torch.Size([16, 1, 60]), torch.Size([16, 60]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_emb0 = Embedding(queries)\n",
    "q_emb = q_emb0.view(-1, 1, 3*embedding_dim)\n",
    "u_0 = torch.sum(q_emb * encoding, 1)\n",
    "u = [u_0]\n",
    "q_emb0.size(), q_emb.size(), u_0.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T21:01:53.119930Z",
     "start_time": "2019-03-20T21:01:53.110301Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 40, 3, 20]),\n",
       " torch.Size([16, 40, 1, 60]),\n",
       " torch.Size([16, 40, 60]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_emb0 = Embedding(stories)\n",
    "m_emb = m_emb0.view(-1, memory_size, 1, 3 * embedding_dim)\n",
    "m = torch.sum(m_emb * encoding, -2)\n",
    "m_emb0.size(), m_emb.size(), m.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T21:02:17.636042Z",
     "start_time": "2019-03-20T21:02:17.623945Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 1, 60]),\n",
       " torch.Size([16, 40]),\n",
       " torch.Size([16, 40]),\n",
       " torch.Size([16, 1, 40]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_temp = u[-1].unsqueeze(-1).transpose(-2, -1)\n",
    "dotted = torch.sum(m * u_temp, -1)\n",
    "probs = F.softmax(dotted, -1)\n",
    "probs_temp = probs.unsqueeze(-1).transpose(-2, -1)\n",
    "u_temp.size(), dotted.size(), probs.size(), probs_temp.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T21:02:27.122692Z",
     "start_time": "2019-03-20T21:02:27.111522Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 40, 3, 20]),\n",
       " torch.Size([16, 40, 1, 60]),\n",
       " torch.Size([16, 40, 60]),\n",
       " torch.Size([16, 60, 40]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_emb0 = Embedding(stories)\n",
    "c_emb = c_emb0.view(-1, memory_size, 1, 3 * embedding_dim)\n",
    "c_temp = torch.sum(c_emb * encoding, -2)\n",
    "c = c_temp.transpose(-2, -1)\n",
    "c_emb0.size(), c_emb.size(), c_temp.size(), c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T21:02:31.945855Z",
     "start_time": "2019-03-20T21:02:31.935670Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 60]), torch.Size([16, 60]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_k = torch.sum(c * probs_temp, -1)\n",
    "u_k = u[-1] + o_k\n",
    "u.append(u_k)\n",
    "o_k.size(), u_k.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T16:21:21.827171Z",
     "start_time": "2019-03-20T16:21:21.810536Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(hops):\n",
    "    m_emb0 = Embedding(stories)\n",
    "    m_emb = m_emb0.view(batch_size, -1, memory_size, 1, 3 * embedding_dim)\n",
    "    m = torch.sum(m_emb * encoding, -2)\n",
    "    \n",
    "    u_temp = u[-1].unsqueeze(-1).transpose(-2, -1)\n",
    "    dotted = torch.sum(m * u_temp, -1)\n",
    "    probs = F.softmax(dotted, -1)\n",
    "    probs_temp = probs.unsqueeze(-1).transpose(-2, -1)\n",
    "    \n",
    "    c_emb0 = Embedding(stories)\n",
    "    c_emb = c_emb0.view(batch_size, -1, memory_size, 1, 3 * embedding_dim)\n",
    "    c_temp = torch.sum(c_emb * encoding, -2)\n",
    "    c = c_temp.transpose(-2, -1)\n",
    "    o_k = torch.sum(c * probs_temp, -1)\n",
    "    u_k = u[-1] + o_k\n",
    "    u.append(u_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T16:22:09.704541Z",
     "start_time": "2019-03-20T16:22:09.691021Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 19, 2])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = fc(dp(u_k))\n",
    "outputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:09:19.107091Z",
     "start_time": "2019-03-27T14:09:18.991610Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def position_encoding(sentence_size, embedding_size):\n",
    "    encoding = np.ones((embedding_size, sentence_size), dtype=np.float32)\n",
    "    ls = sentence_size + 1\n",
    "    le = embedding_size + 1\n",
    "    for i in range(1, le):\n",
    "        for j in range(1, ls):\n",
    "            encoding[i - 1, j - 1] = (i - (le - 1) / 2) * (j - (ls - 1) / 2)\n",
    "    encoding = 1 + 4 * encoding / embedding_size / sentence_size\n",
    "    return np.transpose(encoding)\n",
    "\n",
    "\n",
    "class MemN2N(nn.Module):\n",
    "    def __init__(self,\n",
    "                 batch_size,\n",
    "                 vocab_size,\n",
    "                 embedding_dim,\n",
    "                 sentence_size,\n",
    "                 memory_size,\n",
    "                 hops,\n",
    "                 num_classes,\n",
    "                 dropout=0.5,\n",
    "                 fix_embed=True,\n",
    "                 name='MemN2N'):\n",
    "        super(MemN2N, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sentence_size = sentence_size\n",
    "        self.memory_size = memory_size\n",
    "        self.hops = hops\n",
    "        self.num_classes = num_classes\n",
    "        self.fix_embed = fix_embed\n",
    "        self.name = name\n",
    "\n",
    "        self.encoding = torch.from_numpy(position_encoding(1, sentence_size * embedding_dim))\n",
    "        self.Embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.fc = nn.Linear(3 * embedding_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # self.fc = nn.Sequential(\n",
    "        #     nn.Linear(2 * self.sentence_rnn_size, linear_hidden_dim),\n",
    "        #     nn.ReLU(inplace=True),\n",
    "        #     nn.Dropout(dropout),\n",
    "        #     nn.Linear(linear_hidden_dim, num_classes)\n",
    "        # )\n",
    "\n",
    "    def init_weights(self, embeddings):\n",
    "        if embeddings is not None:\n",
    "            self.Embedding.weight.data.copy_(embeddings)\n",
    "#             self.Embedding = self.Embedding.from_pretrained(\n",
    "#                 embeddings, freeze=self.fix_embed)\n",
    "\n",
    "    def set_device(self, device):\n",
    "        self.encoding = self.encoding.to(device)\n",
    "\n",
    "    def forward(self, stories, queries):\n",
    "        q_emb0 = self.Embedding(queries)\n",
    "        q_emb = q_emb0.view(-1, 1, 3 * self.embedding_dim)\n",
    "        u_0 = torch.sum(q_emb * self.encoding, 1)\n",
    "        u = [u_0]\n",
    "\n",
    "        for i in range(self.hops):\n",
    "            m_emb0 = self.Embedding(stories)\n",
    "            m_emb = m_emb0.view(-1, self.memory_size, 1, 3 * self.embedding_dim)\n",
    "            m = torch.sum(m_emb * self.encoding, -2)\n",
    "\n",
    "            u_temp = u[-1].unsqueeze(-1).transpose(-2, -1)\n",
    "            dotted = torch.sum(m * u_temp, -1)\n",
    "            probs = F.softmax(dotted, -1)\n",
    "            probs_temp = probs.unsqueeze(-1).transpose(-2, -1)\n",
    "\n",
    "            c_emb0 = self.Embedding(stories)\n",
    "            c_emb = c_emb0.view(-1, self.memory_size, 1, 3 * self.embedding_dim)\n",
    "            c_temp = torch.sum(c_emb * self.encoding, -2)\n",
    "            c = c_temp.transpose(-2, -1)\n",
    "\n",
    "            o_k = torch.sum(c * probs_temp, -1)\n",
    "            u_k = u[-1] + o_k\n",
    "            u.append(u_k)\n",
    "\n",
    "        outputs = self.dropout(self.fc(u_k))\n",
    "        return outputs\n",
    "    \n",
    "    def gradient_noise_and_clip(self, parameters, device,\n",
    "                                 noise_stddev=1e-3, max_clip=40.0):\n",
    "        parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
    "        norm = nn.utils.clip_grad_norm_(parameters, max_clip)\n",
    "\n",
    "        for p in parameters:\n",
    "            noise = torch.randn(p.size()) * noise_stddev\n",
    "            noise = noise.to(device)\n",
    "            p.grad.data.add_(noise)\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T15:38:39.633473Z",
     "start_time": "2019-03-27T15:38:39.623875Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path = ['..'] + sys.path\n",
    "import numpy as np\n",
    "import torch\n",
    "from utils.dataset.memnet import MECDataset\n",
    "from utils.dataloader.memnet import MECDataLoader\n",
    "from utils.data.process import load_data, pad_sequence, pad_memory\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T15:38:39.825468Z",
     "start_time": "2019-03-27T15:38:39.797263Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "vocab_size = 19909\n",
    "sentence_size = 3\n",
    "memory_size = 40\n",
    "embedding_dim = 20\n",
    "num_classes = 2\n",
    "hops = 3\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T15:38:48.923126Z",
     "start_time": "2019-03-27T15:38:48.915441Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda: 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T15:38:49.186493Z",
     "start_time": "2019-03-27T15:38:49.172404Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MemN2N(batch_size, vocab_size, embedding_dim, sentence_size, memory_size, hops, num_classes, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T15:38:49.410491Z",
     "start_time": "2019-03-27T15:38:49.404274Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='sum').to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T15:38:49.647341Z",
     "start_time": "2019-03-27T15:38:49.636056Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T15:38:50.535928Z",
     "start_time": "2019-03-27T15:38:50.523224Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embeddings = pickle.load(open('/data/wujipeng/ec/data/embedding/memnet_embedding20d.pkl', 'rb')).astype(np.float32)\n",
    "model.init_weights(torch.from_numpy(embeddings).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T15:39:04.586252Z",
     "start_time": "2019-03-27T15:39:04.365189Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = MECDataset(data_root='/data/wujipeng/ec/data/ltp_static/static.2/', vocab_root='/data/wujipeng/ec/data/raw_data/memnet_ltp_vocab.txt', batch_size=16, train=True)\n",
    "eval_dataset = MECDataset(data_root='/data/wujipeng/ec/data/ltp_static/static.2/', vocab_root='/data/wujipeng/ec/data/raw_data/memnet_ltp_vocab.txt', batch_size=16, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T15:39:24.780730Z",
     "start_time": "2019-03-27T15:39:24.755880Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loader = MECDataLoader(dataset=train_dataset, memory_size=memory_size, sequence_size=3, batch_size=16, shuffle=True, collate_fn=train_dataset.collate_fn)\n",
    "eval_loader = MECDataLoader(dataset=eval_dataset, memory_size=memory_size, sequence_size=3, batch_size=16, shuffle=False, collate_fn=train_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T16:22:28.650343Z",
     "start_time": "2019-03-27T15:39:27.344646Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: [  0/10][  0/1759] L: 9.1852 A: 0.8929 P: 0.0000 R: 0.0000 F: 0.0000 N: 23.1952\n",
      "E: [  0/10][100/1759] L: 6.8052 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 19.5181\n",
      "E: [  0/10][200/1759] L: 6.0974 A: 0.6000 P: 0.0000 R: 0.0000 F: 0.0000 N: 5.3694\n",
      "E: [  0/10][300/1759] L: 5.7711 A: 0.9667 P: 0.0000 R: 0.0000 F: 0.0000 N: 6.5366\n",
      "E: [  0/10][400/1759] L: 5.4885 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 12.4790\n",
      "E: [  0/10][500/1759] L: 5.3106 A: 0.9667 P: 0.0000 R: 0.0000 F: 0.0000 N: 7.4174\n",
      "E: [  0/10][600/1759] L: 5.1997 A: 0.8667 P: 0.0000 R: 0.0000 F: 0.0000 N: 2.8252\n",
      "E: [  0/10][700/1759] L: 5.0862 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 2.9057\n",
      "E: [  0/10][800/1759] L: 4.9731 A: 0.7333 P: 0.0000 R: 0.0000 F: 0.0000 N: 2.9998\n",
      "E: [  0/10][900/1759] L: 4.8507 A: 0.6667 P: 0.0000 R: 0.0000 F: 0.0000 N: 1.8835\n",
      "E: [  0/10][1000/1759] L: 4.7985 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 6.1600\n",
      "E: [  0/10][1100/1759] L: 4.7486 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 5.6694\n",
      "E: [  0/10][1200/1759] L: 4.7016 A: 0.6786 P: 0.0000 R: 0.0000 F: 0.0000 N: 10.5090\n",
      "E: [  0/10][1300/1759] L: 4.6667 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 7.6075\n",
      "E: [  0/10][1400/1759] L: 4.6503 A: 0.3846 P: 0.0000 R: 0.0000 F: 0.0000 N: 16.9231\n",
      "E: [  0/10][1500/1759] L: 4.6309 A: 0.4667 P: 0.0000 R: 0.0000 F: 0.0000 N: 3.6086\n",
      "E: [  0/10][1600/1759] L: 4.5839 A: 0.8000 P: 0.0000 R: 0.0000 F: 0.0000 N: 5.3166\n",
      "E: [  0/10][1700/1759] L: 4.5458 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 5.5710\n",
      "L: 703.3024 A: 0.7911 P: 0.0000 R: 0.0000 F: 0.0000\n",
      "E: [  1/10][  0/1759] L: 1.3392 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 10.3113\n",
      "E: [  1/10][100/1759] L: 3.4569 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 2.8402\n",
      "E: [  1/10][200/1759] L: 3.4497 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 8.3038\n",
      "E: [  1/10][300/1759] L: 3.4883 A: 0.7083 P: 0.0000 R: 0.0000 F: 0.0000 N: 23.1882\n",
      "E: [  1/10][400/1759] L: 3.4202 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 9.9021\n",
      "E: [  1/10][500/1759] L: 3.3610 A: 0.7857 P: 0.0000 R: 0.0000 F: 0.0000 N: 11.7358\n",
      "E: [  1/10][600/1759] L: 3.3717 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 5.9672\n",
      "E: [  1/10][700/1759] L: 3.3298 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 10.1416\n",
      "E: [  1/10][800/1759] L: 3.2849 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 5.7282\n",
      "E: [  1/10][900/1759] L: 3.2229 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 4.5115\n",
      "E: [  1/10][1000/1759] L: 3.1902 A: 0.9286 P: 0.0000 R: 0.0000 F: 0.0000 N: 5.9849\n",
      "E: [  1/10][1100/1759] L: 3.1568 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 11.9954\n",
      "E: [  1/10][1200/1759] L: 3.1196 A: 1.0000 P: 1.0000 R: 0.5000 F: 0.6667 N: 8.2807\n",
      "E: [  1/10][1300/1759] L: 3.0941 A: 0.6410 P: 1.0000 R: 0.3333 F: 0.5000 N: 29.6291\n",
      "E: [  1/10][1400/1759] L: 3.0654 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 8.9676\n",
      "E: [  1/10][1500/1759] L: 3.0483 A: 0.9643 P: 1.0000 R: 0.5000 F: 0.6667 N: 7.7082\n",
      "E: [  1/10][1600/1759] L: 3.0284 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.9739\n",
      "E: [  1/10][1700/1759] L: 3.0085 A: 0.9643 P: 1.0000 R: 0.5000 F: 0.6667 N: 4.9233\n",
      "L: 511.6317 A: 0.9240 P: 0.5702 R: 0.3023 F: 0.3951\n",
      "E: [  2/10][  0/1759] L: 1.0883 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.1216\n",
      "E: [  2/10][100/1759] L: 2.6086 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 6.3982\n",
      "E: [  2/10][200/1759] L: 2.6903 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 10.7221\n",
      "E: [  2/10][300/1759] L: 2.6136 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.7131\n",
      "E: [  2/10][400/1759] L: 2.5877 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 5.5899\n",
      "E: [  2/10][500/1759] L: 2.5430 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 6.5848\n",
      "E: [  2/10][600/1759] L: 2.5515 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 8.4721\n",
      "E: [  2/10][700/1759] L: 2.5497 A: 0.8571 P: 0.0000 R: 0.0000 F: 0.0000 N: 18.7181\n",
      "E: [  2/10][800/1759] L: 2.5491 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 8.4604\n",
      "E: [  2/10][900/1759] L: 2.5143 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 5.5095\n",
      "E: [  2/10][1000/1759] L: 2.5203 A: 0.5714 P: 0.0000 R: 0.0000 F: 0.0000 N: 30.6610\n",
      "E: [  2/10][1100/1759] L: 2.5306 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.9621\n",
      "E: [  2/10][1200/1759] L: 2.5071 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 4.5993\n",
      "E: [  2/10][1300/1759] L: 2.5194 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 4.1451\n",
      "E: [  2/10][1400/1759] L: 2.5107 A: 0.6667 P: 0.0000 R: 0.0000 F: 0.0000 N: 16.6601\n",
      "E: [  2/10][1500/1759] L: 2.5082 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 11.1642\n",
      "E: [  2/10][1600/1759] L: 2.5077 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 4.6367\n",
      "E: [  2/10][1700/1759] L: 2.5009 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 5.0903\n",
      "L: 460.8558 A: 0.9336 P: 0.5986 R: 0.4093 F: 0.4862\n",
      "E: [  3/10][  0/1759] L: 3.3072 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 6.5602\n",
      "E: [  3/10][100/1759] L: 2.3144 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 4.7833\n",
      "E: [  3/10][200/1759] L: 2.2561 A: 0.9643 P: 1.0000 R: 0.5000 F: 0.6667 N: 8.9054\n",
      "E: [  3/10][300/1759] L: 2.2534 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 9.0848\n",
      "E: [  3/10][400/1759] L: 2.3004 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 7.2622\n",
      "E: [  3/10][500/1759] L: 2.2903 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.5920\n",
      "E: [  3/10][600/1759] L: 2.3264 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 5.4513\n",
      "E: [  3/10][700/1759] L: 2.3611 A: 0.8929 P: 0.5000 R: 0.5000 F: 0.5000 N: 10.7950\n",
      "E: [  3/10][800/1759] L: 2.3485 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.7837\n",
      "E: [  3/10][900/1759] L: 2.3344 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 3.8363\n",
      "E: [  3/10][1000/1759] L: 2.3198 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.4624\n",
      "E: [  3/10][1100/1759] L: 2.2952 A: 0.5333 P: 0.0000 R: 0.0000 F: 0.0000 N: 17.0925\n",
      "E: [  3/10][1200/1759] L: 2.2933 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 7.2844\n",
      "E: [  3/10][1300/1759] L: 2.2718 A: 0.9643 P: 1.0000 R: 0.5000 F: 0.6667 N: 10.5832\n",
      "E: [  3/10][1400/1759] L: 2.2775 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 8.6631\n",
      "E: [  3/10][1500/1759] L: 2.2618 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 7.9544\n",
      "E: [  3/10][1600/1759] L: 2.2578 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 15.2681\n",
      "E: [  3/10][1700/1759] L: 2.2495 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 11.4057\n",
      "L: 443.2611 A: 0.9368 P: 0.5858 R: 0.4605 F: 0.5156\n",
      "E: [  4/10][  0/1759] L: 2.4959 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 12.1211\n",
      "E: [  4/10][100/1759] L: 2.1824 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 5.8658\n",
      "E: [  4/10][200/1759] L: 2.2618 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 9.3914\n",
      "E: [  4/10][300/1759] L: 2.1751 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 6.5112\n",
      "E: [  4/10][400/1759] L: 2.1724 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 5.2326\n",
      "E: [  4/10][500/1759] L: 2.1576 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 7.7688\n",
      "E: [  4/10][600/1759] L: 2.1467 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 5.0156\n",
      "E: [  4/10][700/1759] L: 2.1361 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 3.8275\n",
      "E: [  4/10][800/1759] L: 2.1076 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 3.5932\n",
      "E: [  4/10][900/1759] L: 2.1088 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 10.5123\n",
      "E: [  4/10][1000/1759] L: 2.1087 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 10.5861\n",
      "E: [  4/10][1100/1759] L: 2.1075 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 12.3882\n",
      "E: [  4/10][1200/1759] L: 2.1045 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 7.1962\n",
      "E: [  4/10][1300/1759] L: 2.0891 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 9.6238\n",
      "E: [  4/10][1400/1759] L: 2.1077 A: 1.0000 P: 0.5000 R: 1.0000 F: 0.6667 N: 8.6078\n",
      "E: [  4/10][1500/1759] L: 2.0949 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 11.7127\n",
      "E: [  4/10][1600/1759] L: 2.0897 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 5.1153\n",
      "E: [  4/10][1700/1759] L: 2.0787 A: 0.9643 P: 0.5000 R: 0.5000 F: 0.5000 N: 14.9725\n",
      "L: 432.2026 A: 0.9393 P: 0.6082 R: 0.4837 F: 0.5389\n",
      "E: [  5/10][  0/1759] L: 2.8504 A: 0.8667 P: 0.0000 R: 0.0000 F: 0.0000 N: 6.2252\n",
      "E: [  5/10][100/1759] L: 1.8241 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 6.6935\n",
      "E: [  5/10][200/1759] L: 1.9535 A: 0.9333 P: 0.5000 R: 1.0000 F: 0.6667 N: 8.5538\n",
      "E: [  5/10][300/1759] L: 1.9891 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 1.9684\n",
      "E: [  5/10][400/1759] L: 1.8848 A: 0.8667 P: 0.0000 R: 0.0000 F: 0.0000 N: 9.3167\n",
      "E: [  5/10][500/1759] L: 1.9410 A: 0.8667 P: 0.0000 R: 0.0000 F: 0.0000 N: 8.8661\n",
      "E: [  5/10][600/1759] L: 1.9033 A: 0.9744 P: 0.6667 R: 0.6667 F: 0.6667 N: 12.1199\n",
      "E: [  5/10][700/1759] L: 1.8939 A: 0.6000 P: 0.0000 R: 0.0000 F: 0.0000 N: 13.1120\n",
      "E: [  5/10][800/1759] L: 1.9160 A: 0.7333 P: 0.0000 R: 0.0000 F: 0.0000 N: 7.7466\n",
      "E: [  5/10][900/1759] L: 1.9024 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 9.2763\n",
      "E: [  5/10][1000/1759] L: 1.9386 A: 0.8571 P: 0.5000 R: 0.5000 F: 0.5000 N: 8.9097\n",
      "E: [  5/10][1100/1759] L: 1.9310 A: 1.0000 P: 0.5000 R: 1.0000 F: 0.6667 N: 6.5774\n",
      "E: [  5/10][1200/1759] L: 1.9004 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 4.4573\n",
      "E: [  5/10][1300/1759] L: 1.8975 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.3995\n",
      "E: [  5/10][1400/1759] L: 1.8921 A: 1.0000 P: 0.6667 R: 1.0000 F: 0.8000 N: 10.2624\n",
      "E: [  5/10][1500/1759] L: 1.8793 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 5.3159\n",
      "E: [  5/10][1600/1759] L: 1.9031 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 27.3268\n",
      "E: [  5/10][1700/1759] L: 1.9013 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.6620\n",
      "L: 425.5397 A: 0.9408 P: 0.6763 R: 0.4372 F: 0.5311\n",
      "E: [  6/10][  0/1759] L: 2.3105 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 6.7255\n",
      "E: [  6/10][100/1759] L: 1.8130 A: 0.9286 P: 1.0000 R: 0.5000 F: 0.6667 N: 17.8286\n",
      "E: [  6/10][200/1759] L: 1.7228 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 7.6883\n",
      "E: [  6/10][300/1759] L: 1.6419 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.2041\n",
      "E: [  6/10][400/1759] L: 1.6188 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.0995\n",
      "E: [  6/10][500/1759] L: 1.5704 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.6418\n",
      "E: [  6/10][600/1759] L: 1.6314 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 9.2476\n",
      "E: [  6/10][700/1759] L: 1.6463 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 1.1116\n",
      "E: [  6/10][800/1759] L: 1.6582 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.3895\n",
      "E: [  6/10][900/1759] L: 1.6761 A: 0.9643 P: 0.5000 R: 0.5000 F: 0.5000 N: 28.4597\n",
      "E: [  6/10][1000/1759] L: 1.7143 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 9.3722\n",
      "E: [  6/10][1100/1759] L: 1.7023 A: 1.0000 P: 0.6667 R: 1.0000 F: 0.8000 N: 9.2422\n",
      "E: [  6/10][1200/1759] L: 1.7089 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 10.3337\n",
      "E: [  6/10][1300/1759] L: 1.7500 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.2036\n",
      "E: [  6/10][1400/1759] L: 1.7627 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 8.4950\n",
      "E: [  6/10][1500/1759] L: 1.7649 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 11.4991\n",
      "E: [  6/10][1600/1759] L: 1.7601 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.5873\n",
      "E: [  6/10][1700/1759] L: 1.7540 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 4.1662\n",
      "L: 426.5408 A: 0.9404 P: 0.6783 R: 0.4512 F: 0.5419\n",
      "E: [  7/10][  0/1759] L: 0.6121 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.8238\n",
      "E: [  7/10][100/1759] L: 1.6488 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.8857\n",
      "E: [  7/10][200/1759] L: 1.5963 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.2447\n",
      "E: [  7/10][300/1759] L: 1.6189 A: 0.8667 P: 0.0000 R: 0.0000 F: 0.0000 N: 8.3983\n",
      "E: [  7/10][400/1759] L: 1.6465 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 10.8234\n",
      "E: [  7/10][500/1759] L: 1.6546 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 9.0428\n",
      "E: [  7/10][600/1759] L: 1.6296 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.3693\n",
      "E: [  7/10][700/1759] L: 1.5897 A: 0.8667 P: 0.0000 R: 0.0000 F: 0.0000 N: 16.0386\n",
      "E: [  7/10][800/1759] L: 1.6123 A: 0.9286 P: 0.5000 R: 0.5000 F: 0.5000 N: 30.5695\n",
      "E: [  7/10][900/1759] L: 1.6146 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.4709\n",
      "E: [  7/10][1000/1759] L: 1.6153 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 1.1294\n",
      "E: [  7/10][1100/1759] L: 1.6415 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.4121\n",
      "E: [  7/10][1200/1759] L: 1.6302 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.5019\n",
      "E: [  7/10][1300/1759] L: 1.5990 A: 1.0000 P: 0.5000 R: 1.0000 F: 0.6667 N: 15.5967\n",
      "E: [  7/10][1400/1759] L: 1.6006 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.0249\n",
      "E: [  7/10][1500/1759] L: 1.6120 A: 1.0000 P: 0.5000 R: 1.0000 F: 0.6667 N: 20.7151\n",
      "E: [  7/10][1600/1759] L: 1.6181 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 2.6618\n",
      "E: [  7/10][1700/1759] L: 1.6167 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 13.8576\n",
      "L: 433.5987 A: 0.9390 P: 0.6645 R: 0.4791 F: 0.5568\n",
      "E: [  8/10][  0/1759] L: 7.3259 A: 0.7143 P: 0.0000 R: 0.0000 F: 0.0000 N: 24.7087\n",
      "E: [  8/10][100/1759] L: 1.5409 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.6462\n",
      "E: [  8/10][200/1759] L: 1.5420 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 13.0836\n",
      "E: [  8/10][300/1759] L: 1.5327 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 16.6532\n",
      "E: [  8/10][400/1759] L: 1.5646 A: 0.9643 P: 0.5000 R: 0.5000 F: 0.5000 N: 19.1480\n",
      "E: [  8/10][500/1759] L: 1.5431 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.7659\n",
      "E: [  8/10][600/1759] L: 1.5516 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 11.4381\n",
      "E: [  8/10][700/1759] L: 1.5636 A: 0.8462 P: 1.0000 R: 0.3333 F: 0.5000 N: 29.9051\n",
      "E: [  8/10][800/1759] L: 1.5798 A: 0.9286 P: 1.0000 R: 0.5000 F: 0.6667 N: 9.3731\n",
      "E: [  8/10][900/1759] L: 1.5742 A: 0.8000 P: 0.0000 R: 0.0000 F: 0.0000 N: 39.9004\n",
      "E: [  8/10][1000/1759] L: 1.5499 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 10.5843\n",
      "E: [  8/10][1100/1759] L: 1.5510 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 8.4873\n",
      "E: [  8/10][1200/1759] L: 1.5409 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.0065\n",
      "E: [  8/10][1300/1759] L: 1.5257 A: 0.9643 P: 0.5000 R: 1.0000 F: 0.6667 N: 41.4926\n",
      "E: [  8/10][1400/1759] L: 1.5182 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.8323\n",
      "E: [  8/10][1500/1759] L: 1.5155 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 22.6119\n",
      "E: [  8/10][1600/1759] L: 1.5152 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 8.6998\n",
      "E: [  8/10][1700/1759] L: 1.5097 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.4353\n",
      "L: 445.3055 A: 0.9384 P: 0.6731 R: 0.4884 F: 0.5660\n",
      "E: [  9/10][  0/1759] L: 0.5663 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.6427\n",
      "E: [  9/10][100/1759] L: 1.2888 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 7.0213\n",
      "E: [  9/10][200/1759] L: 1.3158 A: 0.9643 P: 1.0000 R: 0.5000 F: 0.6667 N: 21.2941\n",
      "E: [  9/10][300/1759] L: 1.4058 A: 0.8667 P: 0.0000 R: 0.0000 F: 0.0000 N: 43.0409\n",
      "E: [  9/10][400/1759] L: 1.3355 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 15.9109\n",
      "E: [  9/10][500/1759] L: 1.3617 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 27.3545\n",
      "E: [  9/10][600/1759] L: 1.3758 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.6788\n",
      "E: [  9/10][700/1759] L: 1.3916 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 1.2141\n",
      "E: [  9/10][800/1759] L: 1.4036 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 12.7181\n",
      "E: [  9/10][900/1759] L: 1.3925 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.4128\n",
      "E: [  9/10][1000/1759] L: 1.3804 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.7179\n",
      "E: [  9/10][1100/1759] L: 1.3987 A: 0.9643 P: 0.6667 R: 1.0000 F: 0.8000 N: 20.0141\n",
      "E: [  9/10][1200/1759] L: 1.3809 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 11.3423\n",
      "E: [  9/10][1300/1759] L: 1.3662 A: 0.5357 P: 0.5000 R: 0.5000 F: 0.5000 N: 46.3402\n",
      "E: [  9/10][1400/1759] L: 1.3693 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 13.0424\n",
      "E: [  9/10][1500/1759] L: 1.3942 A: 0.8000 P: 0.0000 R: 0.0000 F: 0.0000 N: 20.0910\n",
      "E: [  9/10][1600/1759] L: 1.3863 A: 1.0000 P: 1.0000 R: 0.3333 F: 0.5000 N: 29.7140\n",
      "E: [  9/10][1700/1759] L: 1.3986 A: 0.9286 P: 0.5000 R: 0.5000 F: 0.5000 N: 20.6303\n",
      "L: 452.6810 A: 0.9370 P: 0.6923 R: 0.5023 F: 0.5822\n",
      "E: [ 10/10][  0/1759] L: 0.0412 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.6055\n",
      "E: [ 10/10][100/1759] L: 1.2410 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 1.5418\n",
      "E: [ 10/10][200/1759] L: 1.1688 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 5.7859\n",
      "E: [ 10/10][300/1759] L: 1.2601 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.9095\n",
      "E: [ 10/10][400/1759] L: 1.2265 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.1108\n",
      "E: [ 10/10][500/1759] L: 1.3127 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 21.7584\n",
      "E: [ 10/10][600/1759] L: 1.3060 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 9.5648\n",
      "E: [ 10/10][700/1759] L: 1.3061 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 1.1881\n",
      "E: [ 10/10][800/1759] L: 1.3130 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.7670\n",
      "E: [ 10/10][900/1759] L: 1.3426 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 13.7492\n",
      "E: [ 10/10][1000/1759] L: 1.3413 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 6.8687\n",
      "E: [ 10/10][1100/1759] L: 1.3369 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 115.7259\n",
      "E: [ 10/10][1200/1759] L: 1.3242 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.9307\n",
      "E: [ 10/10][1300/1759] L: 1.3220 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 5.2232\n",
      "E: [ 10/10][1400/1759] L: 1.3114 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 21.8463\n",
      "E: [ 10/10][1500/1759] L: 1.3155 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.2345\n",
      "E: [ 10/10][1600/1759] L: 1.3020 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.4126\n",
      "E: [ 10/10][1700/1759] L: 1.2973 A: 1.0000 P: 1.0000 R: 0.5000 F: 0.6667 N: 9.9837\n",
      "L: 467.5648 A: 0.9361 P: 0.6928 R: 0.4930 F: 0.5761\n",
      "E: [ 11/10][  0/1759] L: 3.6142 A: 1.0000 P: 1.0000 R: 0.5000 F: 0.6667 N: 36.1081\n",
      "E: [ 11/10][100/1759] L: 1.6628 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 10.8261\n",
      "E: [ 11/10][200/1759] L: 1.5550 A: 0.8667 P: 0.0000 R: 0.0000 F: 0.0000 N: 38.0813\n",
      "E: [ 11/10][300/1759] L: 1.3328 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.7655\n",
      "E: [ 11/10][400/1759] L: 1.2788 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.3150\n",
      "E: [ 11/10][500/1759] L: 1.2364 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 16.8756\n",
      "E: [ 11/10][600/1759] L: 1.2367 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 24.5333\n",
      "E: [ 11/10][700/1759] L: 1.2281 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.7516\n",
      "E: [ 11/10][800/1759] L: 1.2061 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.1150\n",
      "E: [ 11/10][900/1759] L: 1.1948 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 10.0180\n",
      "E: [ 11/10][1000/1759] L: 1.2001 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 5.2436\n",
      "E: [ 11/10][1100/1759] L: 1.1893 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.8950\n",
      "E: [ 11/10][1200/1759] L: 1.2148 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.0731\n",
      "E: [ 11/10][1300/1759] L: 1.2088 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.1179\n",
      "E: [ 11/10][1400/1759] L: 1.2038 A: 0.6429 P: 0.0000 R: 0.0000 F: 0.0000 N: 53.1524\n",
      "E: [ 11/10][1500/1759] L: 1.2051 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.0479\n",
      "E: [ 11/10][1600/1759] L: 1.1912 A: 1.0000 P: 0.5000 R: 1.0000 F: 0.6667 N: 18.5815\n",
      "E: [ 11/10][1700/1759] L: 1.1820 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.1807\n",
      "L: 478.7977 A: 0.9341 P: 0.6871 R: 0.5209 F: 0.5926\n",
      "E: [ 12/10][  0/1759] L: 0.1673 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.0708\n",
      "E: [ 12/10][100/1759] L: 0.9886 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 18.9901\n",
      "E: [ 12/10][200/1759] L: 1.0118 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.0668\n",
      "E: [ 12/10][300/1759] L: 1.0487 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.5548\n",
      "E: [ 12/10][400/1759] L: 1.0463 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 13.4729\n",
      "E: [ 12/10][500/1759] L: 1.0199 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.1490\n",
      "E: [ 12/10][600/1759] L: 1.0713 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 19.1826\n",
      "E: [ 12/10][700/1759] L: 1.0655 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 20.9038\n",
      "E: [ 12/10][800/1759] L: 1.0679 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 42.4342\n",
      "E: [ 12/10][900/1759] L: 1.0681 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 21.9528\n",
      "E: [ 12/10][1000/1759] L: 1.0961 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 13.2159\n",
      "E: [ 12/10][1100/1759] L: 1.1074 A: 1.0000 P: 0.6667 R: 1.0000 F: 0.8000 N: 26.0153\n",
      "E: [ 12/10][1200/1759] L: 1.0983 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.4989\n",
      "E: [ 12/10][1300/1759] L: 1.1057 A: 0.8667 P: 0.0000 R: 0.0000 F: 0.0000 N: 13.0352\n",
      "E: [ 12/10][1400/1759] L: 1.0954 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.9852\n",
      "E: [ 12/10][1500/1759] L: 1.1070 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 21.0482\n",
      "E: [ 12/10][1600/1759] L: 1.1005 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 49.6321\n",
      "E: [ 12/10][1700/1759] L: 1.0880 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.4281\n",
      "L: 498.8391 A: 0.9332 P: 0.6687 R: 0.5163 F: 0.5827\n",
      "E: [ 13/10][  0/1759] L: 0.4079 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.0198\n",
      "E: [ 13/10][100/1759] L: 0.7323 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.3893\n",
      "E: [ 13/10][200/1759] L: 0.8500 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.5531\n",
      "E: [ 13/10][300/1759] L: 0.9405 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.0975\n",
      "E: [ 13/10][400/1759] L: 0.9130 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.5504\n",
      "E: [ 13/10][500/1759] L: 0.8989 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.3400\n",
      "E: [ 13/10][600/1759] L: 0.9321 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 7.3274\n",
      "E: [ 13/10][700/1759] L: 0.9666 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.3971\n",
      "E: [ 13/10][800/1759] L: 0.9404 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.5747\n",
      "E: [ 13/10][900/1759] L: 0.9483 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 10.3322\n",
      "E: [ 13/10][1000/1759] L: 0.9654 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 4.4458\n",
      "E: [ 13/10][1100/1759] L: 0.9603 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 5.5581\n",
      "E: [ 13/10][1200/1759] L: 0.9767 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 9.7178\n",
      "E: [ 13/10][1300/1759] L: 0.9795 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 1.8999\n",
      "E: [ 13/10][1400/1759] L: 0.9940 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 57.3609\n",
      "E: [ 13/10][1500/1759] L: 0.9971 A: 0.9643 P: 1.0000 R: 0.5000 F: 0.6667 N: 17.5605\n",
      "E: [ 13/10][1600/1759] L: 1.0034 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 24.3113\n",
      "E: [ 13/10][1700/1759] L: 0.9925 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 18.6156\n",
      "L: 518.5900 A: 0.9309 P: 0.6667 R: 0.4837 F: 0.5606\n",
      "E: [ 14/10][  0/1759] L: 5.6244 A: 0.6000 P: 0.0000 R: 0.0000 F: 0.0000 N: 26.4417\n",
      "E: [ 14/10][100/1759] L: 0.8714 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 27.1183\n",
      "E: [ 14/10][200/1759] L: 0.8347 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.3792\n",
      "E: [ 14/10][300/1759] L: 0.9005 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.5882\n",
      "E: [ 14/10][400/1759] L: 0.8856 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.0672\n",
      "E: [ 14/10][500/1759] L: 0.8635 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.0822\n",
      "E: [ 14/10][600/1759] L: 0.8922 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 9.6173\n",
      "E: [ 14/10][700/1759] L: 0.9126 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 16.3154\n",
      "E: [ 14/10][800/1759] L: 0.9118 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 10.3422\n",
      "E: [ 14/10][900/1759] L: 0.9249 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 12.2662\n",
      "E: [ 14/10][1000/1759] L: 0.9397 A: 1.0000 P: 1.0000 R: 0.6000 F: 0.7500 N: 28.6055\n",
      "E: [ 14/10][1100/1759] L: 0.9364 A: 0.9286 P: 1.0000 R: 0.5000 F: 0.6667 N: 10.6175\n",
      "E: [ 14/10][1200/1759] L: 0.9197 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 1.1739\n",
      "E: [ 14/10][1300/1759] L: 0.9082 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.5009\n",
      "E: [ 14/10][1400/1759] L: 0.9050 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 17.4340\n",
      "E: [ 14/10][1500/1759] L: 0.8999 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.2604\n",
      "E: [ 14/10][1600/1759] L: 0.9060 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.8210\n",
      "E: [ 14/10][1700/1759] L: 0.9102 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 28.2900\n",
      "L: 544.9200 A: 0.9296 P: 0.6463 R: 0.4930 F: 0.5594\n",
      "E: [ 15/10][  0/1759] L: 0.1349 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 2.9085\n",
      "E: [ 15/10][100/1759] L: 0.8020 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 14.5047\n",
      "E: [ 15/10][200/1759] L: 0.8501 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 12.0068\n",
      "E: [ 15/10][300/1759] L: 0.8611 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 23.9270\n",
      "E: [ 15/10][400/1759] L: 0.8335 A: 0.8667 P: 0.0000 R: 0.0000 F: 0.0000 N: 74.3395\n",
      "E: [ 15/10][500/1759] L: 0.8063 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 2.6347\n",
      "E: [ 15/10][600/1759] L: 0.7828 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 10.3824\n",
      "E: [ 15/10][700/1759] L: 0.7823 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.9631\n",
      "E: [ 15/10][800/1759] L: 0.7963 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 9.9416\n",
      "E: [ 15/10][900/1759] L: 0.8057 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.4432\n",
      "E: [ 15/10][1000/1759] L: 0.8091 A: 0.9333 P: 0.5000 R: 1.0000 F: 0.6667 N: 196.0846\n",
      "E: [ 15/10][1100/1759] L: 0.8011 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 34.5331\n",
      "E: [ 15/10][1200/1759] L: 0.8123 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.7072\n",
      "E: [ 15/10][1300/1759] L: 0.8254 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.5779\n",
      "E: [ 15/10][1400/1759] L: 0.8286 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.3607\n",
      "E: [ 15/10][1500/1759] L: 0.8379 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 9.9013\n",
      "E: [ 15/10][1600/1759] L: 0.8534 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 52.4697\n",
      "E: [ 15/10][1700/1759] L: 0.8483 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 17.4208\n",
      "L: 572.6819 A: 0.9282 P: 0.6461 R: 0.5349 F: 0.5852\n",
      "E: [ 16/10][  0/1759] L: 0.1780 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.5253\n",
      "E: [ 16/10][100/1759] L: 0.7018 A: 0.6000 P: 0.0000 R: 0.0000 F: 0.0000 N: 37.3653\n",
      "E: [ 16/10][200/1759] L: 0.6757 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 16.3512\n",
      "E: [ 16/10][300/1759] L: 0.6603 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.5246\n",
      "E: [ 16/10][400/1759] L: 0.7165 A: 1.0000 P: 1.0000 R: 0.5000 F: 0.6667 N: 13.3073\n",
      "E: [ 16/10][500/1759] L: 0.7244 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 33.5171\n",
      "E: [ 16/10][600/1759] L: 0.7248 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.5307\n",
      "E: [ 16/10][700/1759] L: 0.7221 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.9404\n",
      "E: [ 16/10][800/1759] L: 0.7229 A: 1.0000 P: 1.0000 R: 0.6667 F: 0.8000 N: 21.4322\n",
      "E: [ 16/10][900/1759] L: 0.7311 A: 0.9643 P: 1.0000 R: 0.5000 F: 0.6667 N: 26.2452\n",
      "E: [ 16/10][1000/1759] L: 0.7495 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.2904\n",
      "E: [ 16/10][1100/1759] L: 0.7701 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.0425\n",
      "E: [ 16/10][1200/1759] L: 0.7565 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.8988\n",
      "E: [ 16/10][1300/1759] L: 0.7801 A: 1.0000 P: 0.5000 R: 1.0000 F: 0.6667 N: 34.6344\n",
      "E: [ 16/10][1400/1759] L: 0.7794 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 28.3831\n",
      "E: [ 16/10][1500/1759] L: 0.7833 A: 0.9487 P: 1.0000 R: 0.6667 F: 0.8000 N: 29.8624\n",
      "E: [ 16/10][1600/1759] L: 0.7752 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.0127\n",
      "E: [ 16/10][1700/1759] L: 0.7608 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 14.7878\n",
      "L: 609.4381 A: 0.9265 P: 0.6154 R: 0.4837 F: 0.5417\n",
      "E: [ 17/10][  0/1759] L: 0.0078 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.2072\n",
      "E: [ 17/10][100/1759] L: 0.9129 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 38.6679\n",
      "E: [ 17/10][200/1759] L: 0.7319 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.1915\n",
      "E: [ 17/10][300/1759] L: 0.6372 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.1936\n",
      "E: [ 17/10][400/1759] L: 0.6579 A: 0.9643 P: 1.0000 R: 0.5000 F: 0.6667 N: 105.5896\n",
      "E: [ 17/10][500/1759] L: 0.6539 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 5.1644\n",
      "E: [ 17/10][600/1759] L: 0.7145 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 17.4227\n",
      "E: [ 17/10][700/1759] L: 0.6948 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.4281\n",
      "E: [ 17/10][800/1759] L: 0.6921 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.7861\n",
      "E: [ 17/10][900/1759] L: 0.6922 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 10.4398\n",
      "E: [ 17/10][1000/1759] L: 0.6848 A: 1.0000 P: 0.5000 R: 1.0000 F: 0.6667 N: 68.2810\n",
      "E: [ 17/10][1100/1759] L: 0.6756 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.7931\n",
      "E: [ 17/10][1200/1759] L: 0.6906 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 1.0081\n",
      "E: [ 17/10][1300/1759] L: 0.6925 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 9.2704\n",
      "E: [ 17/10][1400/1759] L: 0.6930 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 13.2547\n",
      "E: [ 17/10][1500/1759] L: 0.6865 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 25.8296\n",
      "E: [ 17/10][1600/1759] L: 0.6792 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.5653\n",
      "E: [ 17/10][1700/1759] L: 0.6891 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.2897\n",
      "L: 653.8351 A: 0.9237 P: 0.5866 R: 0.4884 F: 0.5330\n",
      "E: [ 18/10][  0/1759] L: 0.1193 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.7957\n",
      "E: [ 18/10][100/1759] L: 0.5169 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.6315\n",
      "E: [ 18/10][200/1759] L: 0.5733 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.3018\n",
      "E: [ 18/10][300/1759] L: 0.6333 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 9.1364\n",
      "E: [ 18/10][400/1759] L: 0.6404 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.5657\n",
      "E: [ 18/10][500/1759] L: 0.6093 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 66.7450\n",
      "E: [ 18/10][600/1759] L: 0.6324 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 13.6189\n",
      "E: [ 18/10][700/1759] L: 0.6267 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.8712\n",
      "E: [ 18/10][800/1759] L: 0.6407 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 11.2791\n",
      "E: [ 18/10][900/1759] L: 0.6255 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 18.0852\n",
      "E: [ 18/10][1000/1759] L: 0.6301 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.8697\n",
      "E: [ 18/10][1100/1759] L: 0.6281 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.5101\n",
      "E: [ 18/10][1200/1759] L: 0.6226 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 17.6938\n",
      "E: [ 18/10][1300/1759] L: 0.6320 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.7566\n",
      "E: [ 18/10][1400/1759] L: 0.6241 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.5275\n",
      "E: [ 18/10][1500/1759] L: 0.6228 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 22.5305\n",
      "E: [ 18/10][1600/1759] L: 0.6291 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 5.3632\n",
      "E: [ 18/10][1700/1759] L: 0.6341 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 7.1481\n",
      "L: 687.5211 A: 0.9226 P: 0.5988 R: 0.4791 F: 0.5323\n",
      "E: [ 19/10][  0/1759] L: 0.0433 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 1.3049\n",
      "E: [ 19/10][100/1759] L: 0.6491 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.7448\n",
      "E: [ 19/10][200/1759] L: 0.5440 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 2.9023\n",
      "E: [ 19/10][300/1759] L: 0.5094 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.0387\n",
      "E: [ 19/10][400/1759] L: 0.5121 A: 1.0000 P: 0.5000 R: 1.0000 F: 0.6667 N: 79.2262\n",
      "E: [ 19/10][500/1759] L: 0.4948 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.6175\n",
      "E: [ 19/10][600/1759] L: 0.5233 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 43.8787\n",
      "E: [ 19/10][700/1759] L: 0.5418 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.7410\n",
      "E: [ 19/10][800/1759] L: 0.5473 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 10.9184\n",
      "E: [ 19/10][900/1759] L: 0.5335 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.0850\n",
      "E: [ 19/10][1000/1759] L: 0.5383 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.3785\n",
      "E: [ 19/10][1100/1759] L: 0.5473 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.8250\n",
      "E: [ 19/10][1200/1759] L: 0.5368 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 15.5771\n",
      "E: [ 19/10][1300/1759] L: 0.5673 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 8.7276\n",
      "E: [ 19/10][1400/1759] L: 0.5790 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.0718\n",
      "E: [ 19/10][1500/1759] L: 0.5746 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.2359\n",
      "E: [ 19/10][1600/1759] L: 0.5758 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.3490\n",
      "E: [ 19/10][1700/1759] L: 0.5727 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.9590\n",
      "L: 711.9604 A: 0.9213 P: 0.5799 R: 0.4558 F: 0.5104\n",
      "E: [ 20/10][  0/1759] L: 0.0449 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.2750\n",
      "E: [ 20/10][100/1759] L: 0.4943 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.8596\n",
      "E: [ 20/10][200/1759] L: 0.4916 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.6847\n",
      "E: [ 20/10][300/1759] L: 0.4480 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 40.3113\n",
      "E: [ 20/10][400/1759] L: 0.4638 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.3931\n",
      "E: [ 20/10][500/1759] L: 0.4779 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 5.1921\n",
      "E: [ 20/10][600/1759] L: 0.4551 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.4254\n",
      "E: [ 20/10][700/1759] L: 0.4678 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.6764\n",
      "E: [ 20/10][800/1759] L: 0.4613 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.0408\n",
      "E: [ 20/10][900/1759] L: 0.4625 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 20.1835\n",
      "E: [ 20/10][1000/1759] L: 0.4728 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 57.6377\n",
      "E: [ 20/10][1100/1759] L: 0.4743 A: 0.9643 P: 1.0000 R: 0.5000 F: 0.6667 N: 49.9285\n",
      "E: [ 20/10][1200/1759] L: 0.4957 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.4952\n",
      "E: [ 20/10][1300/1759] L: 0.5224 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 13.1569\n",
      "E: [ 20/10][1400/1759] L: 0.5198 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.2803\n",
      "E: [ 20/10][1500/1759] L: 0.5133 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.8601\n",
      "E: [ 20/10][1600/1759] L: 0.5145 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.2716\n",
      "E: [ 20/10][1700/1759] L: 0.5091 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 15.5580\n",
      "L: 752.1395 A: 0.9210 P: 0.5862 R: 0.5535 F: 0.5694\n",
      "E: [ 21/10][  0/1759] L: 0.0119 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.5901\n",
      "E: [ 21/10][100/1759] L: 0.3803 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.2641\n",
      "E: [ 21/10][200/1759] L: 0.4324 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.2846\n",
      "E: [ 21/10][300/1759] L: 0.4587 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 1.9940\n",
      "E: [ 21/10][400/1759] L: 0.4359 A: 1.0000 P: 0.5000 R: 1.0000 F: 0.6667 N: 84.6677\n",
      "E: [ 21/10][500/1759] L: 0.4263 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 43.2734\n",
      "E: [ 21/10][600/1759] L: 0.4247 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.9378\n",
      "E: [ 21/10][700/1759] L: 0.4437 A: 0.9333 P: 0.5000 R: 1.0000 F: 0.6667 N: 148.5647\n",
      "E: [ 21/10][800/1759] L: 0.4660 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 23.5521\n",
      "E: [ 21/10][900/1759] L: 0.4669 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.9443\n",
      "E: [ 21/10][1000/1759] L: 0.4714 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.0134\n",
      "E: [ 21/10][1100/1759] L: 0.4613 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.3194\n",
      "E: [ 21/10][1200/1759] L: 0.4523 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.9040\n",
      "E: [ 21/10][1300/1759] L: 0.4725 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 8.9626\n",
      "E: [ 21/10][1400/1759] L: 0.4643 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.7556\n",
      "E: [ 21/10][1500/1759] L: 0.4649 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.2999\n",
      "E: [ 21/10][1600/1759] L: 0.4569 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.0597\n",
      "E: [ 21/10][1700/1759] L: 0.4566 A: 0.9333 P: 0.0000 R: 0.0000 F: 0.0000 N: 107.0312\n",
      "L: 780.8439 A: 0.9189 P: 0.5838 R: 0.4698 F: 0.5206\n",
      "E: [ 22/10][  0/1759] L: 0.1213 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.5126\n",
      "E: [ 22/10][100/1759] L: 0.2978 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 3.9214\n",
      "E: [ 22/10][200/1759] L: 0.2901 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.8395\n",
      "E: [ 22/10][300/1759] L: 0.2828 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.0036\n",
      "E: [ 22/10][400/1759] L: 0.3075 A: 1.0000 P: 0.5000 R: 1.0000 F: 0.6667 N: 18.0169\n",
      "E: [ 22/10][500/1759] L: 0.3229 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 16.3491\n",
      "E: [ 22/10][600/1759] L: 0.3438 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 20.9361\n",
      "E: [ 22/10][700/1759] L: 0.3315 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.1266\n",
      "E: [ 22/10][800/1759] L: 0.3641 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 63.2702\n",
      "E: [ 22/10][900/1759] L: 0.3717 A: 0.9643 P: 1.0000 R: 0.5000 F: 0.6667 N: 113.2223\n",
      "E: [ 22/10][1000/1759] L: 0.3887 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.7156\n",
      "E: [ 22/10][1100/1759] L: 0.4079 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 69.0945\n",
      "E: [ 22/10][1200/1759] L: 0.4047 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 9.5651\n",
      "E: [ 22/10][1300/1759] L: 0.4097 A: 1.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 84.9658\n",
      "E: [ 22/10][1400/1759] L: 0.4081 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 1.5436\n",
      "E: [ 22/10][1500/1759] L: 0.4143 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 5.1046\n",
      "E: [ 22/10][1600/1759] L: 0.4176 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 2.0690\n",
      "E: [ 22/10][1700/1759] L: 0.4215 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 21.4299\n",
      "L: 821.3309 A: 0.9178 P: 0.5789 R: 0.4605 F: 0.5130\n",
      "E: [ 23/10][  0/1759] L: 0.0137 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.2726\n",
      "E: [ 23/10][100/1759] L: 0.6893 A: 0.9286 P: 0.6667 R: 1.0000 F: 0.8000 N: 107.0073\n",
      "E: [ 23/10][200/1759] L: 0.5254 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.1184\n",
      "E: [ 23/10][300/1759] L: 0.4743 A: 0.9643 P: 0.6667 R: 1.0000 F: 0.8000 N: 222.0481\n",
      "E: [ 23/10][400/1759] L: 0.4172 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.0276\n",
      "E: [ 23/10][500/1759] L: 0.4193 A: 1.0000 P: 0.6667 R: 1.0000 F: 0.8000 N: 15.5086\n",
      "E: [ 23/10][600/1759] L: 0.4071 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 6.3689\n",
      "E: [ 23/10][700/1759] L: 0.4227 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 4.9066\n",
      "E: [ 23/10][800/1759] L: 0.4035 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 8.5080\n",
      "E: [ 23/10][900/1759] L: 0.4246 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.1806\n",
      "E: [ 23/10][1000/1759] L: 0.4308 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 11.2657\n",
      "E: [ 23/10][1100/1759] L: 0.4240 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.4999\n",
      "E: [ 23/10][1200/1759] L: 0.4135 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.0253\n",
      "E: [ 23/10][1300/1759] L: 0.4078 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 5.3416\n",
      "E: [ 23/10][1400/1759] L: 0.4061 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.5379\n",
      "E: [ 23/10][1500/1759] L: 0.3935 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 10.1632\n",
      "E: [ 23/10][1600/1759] L: 0.3820 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.3132\n",
      "E: [ 23/10][1700/1759] L: 0.3817 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 82.7504\n",
      "L: 863.8005 A: 0.9182 P: 0.5838 R: 0.4698 F: 0.5206\n",
      "E: [ 24/10][  0/1759] L: 0.3172 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 4.0025\n",
      "E: [ 24/10][100/1759] L: 0.3199 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 16.7164\n",
      "E: [ 24/10][200/1759] L: 0.3250 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.4228\n",
      "E: [ 24/10][300/1759] L: 0.2917 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.0640\n",
      "E: [ 24/10][400/1759] L: 0.3258 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.0479\n",
      "E: [ 24/10][500/1759] L: 0.3413 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.3265\n",
      "E: [ 24/10][600/1759] L: 0.3572 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 2.7533\n",
      "E: [ 24/10][700/1759] L: 0.3818 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 5.9595\n",
      "E: [ 24/10][800/1759] L: 0.3655 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 10.2579\n",
      "E: [ 24/10][900/1759] L: 0.3600 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 5.2438\n",
      "E: [ 24/10][1000/1759] L: 0.3549 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.4580\n",
      "E: [ 24/10][1100/1759] L: 0.3530 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.0040\n",
      "E: [ 24/10][1200/1759] L: 0.3699 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.0065\n",
      "E: [ 24/10][1300/1759] L: 0.3545 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.1748\n",
      "E: [ 24/10][1400/1759] L: 0.3516 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.8100\n",
      "E: [ 24/10][1500/1759] L: 0.3520 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.4275\n",
      "E: [ 24/10][1600/1759] L: 0.3554 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 69.5287\n",
      "E: [ 24/10][1700/1759] L: 0.3501 A: 0.0000 P: 0.0000 R: 0.0000 F: 0.0000 N: 0.1476\n",
      "L: 902.2048 A: 0.9186 P: 0.5917 R: 0.4651 F: 0.5208\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(25):\n",
    "    losses = 0.\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = eval_dataset.batch2input(batch)\n",
    "\n",
    "        clauses, keywords = inputs\n",
    "        clauses = torch.from_numpy(clauses).to(device)\n",
    "        keywords = torch.from_numpy(keywords).to(device)\n",
    "\n",
    "        labels = eval_dataset.batch2target(batch)\n",
    "        labels = torch.from_numpy(labels).to(device)\n",
    "\n",
    "        outputs = model(clauses,keywords)\n",
    "        probs = outputs.view(-1, outputs.size(-1))\n",
    "        loss = criterion(probs, labels)\n",
    "        losses += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        grad_norm = model.gradient_noise_and_clip(model.parameters(), device, noise_stddev=1e-3, max_clip=40.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        predicts = probs.max(dim=-1)[1]\n",
    "        total, acc, pre, rec, f1, auc = metrics(predicts.tolist(), labels.tolist(), probs[:, 1].tolist())\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('E: [{:3}/{}][{:3}/{}] L: {:.4f} A: {:.4f} '\n",
    "                                     'P: {:.4f} R: {:.4f} F: {:.4f} N: {:.4f}'.format(epoch, 10, i, len(train_loader),\n",
    "                                                                                      losses/(i+1), auc, pre, rec, f1,\n",
    "                                                                                      grad_norm))\n",
    "    model.eval()\n",
    "    losses = 0.\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    for batch in eval_loader:\n",
    "        inputs = eval_dataset.batch2input(batch)\n",
    "\n",
    "        clauses, keywords = inputs\n",
    "        clauses = torch.from_numpy(clauses).to(device)\n",
    "        keywords = torch.from_numpy(keywords).to(device)\n",
    "\n",
    "        labels = eval_dataset.batch2target(batch)\n",
    "        labels = torch.from_numpy(labels).to(device)\n",
    "\n",
    "\n",
    "        outputs = model(clauses,keywords)\n",
    "        probs = outputs.view(-1, outputs.size(-1))\n",
    "        predicts = probs.max(dim=-1)[1]\n",
    "\n",
    "        loss = criterion(probs.view(-1, probs.size(-1)), labels.view(-1))\n",
    "\n",
    "        losses += loss\n",
    "\n",
    "        probs = F.softmax(probs, dim=-1)\n",
    "\n",
    "        all_probs += probs.tolist()\n",
    "        all_preds += predicts.tolist()\n",
    "        all_targets += labels.tolist()\n",
    "\n",
    "        pos_probs = np.array(all_probs)[:, 1]\n",
    "\n",
    "    total, acc, pre, rec, f1, auc = metrics(all_preds, all_targets, pos_probs)\n",
    "    print('L: {:.4f} A: {:.4f} P: {:.4f} R: {:.4f} F: {:.4f}'.format(losses, auc, pre, rec, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T11:23:07.994083Z",
     "start_time": "2019-03-27T11:23:07.976287Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0201, -0.0440, -0.0023,  ...,  0.0272,  0.0265, -0.0060],\n",
       "        [ 0.0904, -0.0236, -0.1043,  ..., -0.1321, -0.0570,  0.2497],\n",
       "        [-0.0183,  0.1107, -0.0237,  ..., -0.1831, -0.0702,  0.0026],\n",
       "        ...,\n",
       "        [ 0.0278,  0.0905,  0.0961,  ...,  0.0057, -0.1061, -0.1791],\n",
       "        [-0.0235, -0.0097,  0.0345,  ...,  0.0516, -0.0535, -0.0009],\n",
       "        [ 0.1544,  0.0738, -0.0548,  ..., -0.0089,  0.0208,  0.0780]],\n",
       "       device='cuda:3', requires_grad=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.Embedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T11:24:39.239413Z",
     "start_time": "2019-03-27T11:24:39.225351Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0333, 0.0667, 0.1000, 0.1333, 0.1667, 0.2000, 0.2333, 0.2667, 0.3000,\n",
       "         0.3333, 0.3667, 0.4000, 0.4333, 0.4667, 0.5000, 0.5333, 0.5667, 0.6000,\n",
       "         0.6333, 0.6667, 0.7000, 0.7333, 0.7667, 0.8000, 0.8333, 0.8667, 0.9000,\n",
       "         0.9333, 0.9667, 1.0000, 1.0333, 1.0667, 1.1000, 1.1333, 1.1667, 1.2000,\n",
       "         1.2333, 1.2667, 1.3000, 1.3333, 1.3667, 1.4000, 1.4333, 1.4667, 1.5000,\n",
       "         1.5333, 1.5667, 1.6000, 1.6333, 1.6667, 1.7000, 1.7333, 1.7667, 1.8000,\n",
       "         1.8333, 1.8667, 1.9000, 1.9333, 1.9667, 2.0000]], device='cuda:3')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T14:09:28.537186Z",
     "start_time": "2019-03-27T14:09:28.243534Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def metrics(pred_labels, true_labels, probs, ignore_index=-100):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            pred_labels: (bat, n(s)), 0-3\n",
    "            true_labels: (bat, n(s)), 0-3\n",
    "        \"\"\"\n",
    "    if type(pred_labels[0]) != int:\n",
    "        pred_labels = list(itertools.chain.from_iterable(pred_labels))\n",
    "        true_labels = list(itertools.chain.from_iterable(true_labels))\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    all_pred, all_true, all_probs = [], [], []\n",
    "    for i in range(len(pred_labels)):\n",
    "        if true_labels[i] == ignore_index:\n",
    "            continue\n",
    "        if pred_labels[i] == true_labels[i]:\n",
    "            if true_labels[i] == 0:\n",
    "                tn += 1\n",
    "            else:\n",
    "                tp += 1\n",
    "        else:\n",
    "            if true_labels[i] == 0:\n",
    "                fp += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "        all_pred.append(pred_labels[i])\n",
    "        all_true.append(true_labels[i])\n",
    "        all_probs.append(probs[i])\n",
    "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "    pre = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * pre * rec / (pre + rec) if (pre + rec) > 0 else 0\n",
    "    auc = roc_auc_score(all_true, all_probs) if sum(all_true) > 0 else 0.\n",
    "    return tp + tn + fp + fn, acc, pre, rec, f1, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T15:53:03.686644Z",
     "start_time": "2019-03-21T15:53:03.679970Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "losses = 0.\n",
    "all_probs = []\n",
    "all_preds = []\n",
    "all_targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T15:53:14.552202Z",
     "start_time": "2019-03-21T15:53:13.682146Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S, Q, A = [], [], []\n",
    "for batch in eval_loader:\n",
    "    inputs = eval_dataset.batch2input(batch)\n",
    "\n",
    "    clauses, keywords = inputs\n",
    "    S += clauses.tolist()\n",
    "    Q += keywords.tolist()\n",
    "    clauses = torch.from_numpy(clauses).to(device)\n",
    "    keywords = torch.from_numpy(keywords).to(device)\n",
    "\n",
    "    labels = eval_dataset.batch2target(batch)\n",
    "    A += labels.tolist()\n",
    "    labels = torch.from_numpy(labels).to(device)\n",
    "\n",
    "\n",
    "    outputs = model(clauses,keywords)\n",
    "    probs = outputs.view(-1, outputs.size(-1))\n",
    "    predicts = probs.max(dim=-1)[1]\n",
    "    \n",
    "    loss = criterion(probs.view(-1, probs.size(-1)), labels.view(-1))\n",
    "    \n",
    "    losses += loss\n",
    "\n",
    "    probs = F.softmax(probs, dim=-1)\n",
    "\n",
    "    all_probs += probs.tolist()\n",
    "    all_preds += predicts.tolist()\n",
    "    all_targets += labels.tolist()\n",
    "\n",
    "\n",
    "    pos_probs = np.array(all_probs)[:, 1]\n",
    "\n",
    "total, acc, pre, rec, f1, auc = metrics(all_preds, all_targets, pos_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T15:53:17.624474Z",
     "start_time": "2019-03-21T15:53:17.615325Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0879, -0.0325],\n",
       "        [-0.0089, -0.0088],\n",
       "        [-0.0081, -0.0391],\n",
       "        [-0.0091, -0.0224],\n",
       "        [-0.0479, -0.0298],\n",
       "        [-0.0242, -0.0085],\n",
       "        [-0.0275, -0.0233],\n",
       "        [-0.0140, -0.0217],\n",
       "        [-0.0184,  0.0200],\n",
       "        [-0.0279, -0.0604],\n",
       "        [ 0.0760,  0.0065],\n",
       "        [-0.0200, -0.0136],\n",
       "        [-0.0200, -0.0136],\n",
       "        [-0.0200, -0.0136],\n",
       "        [-0.0200, -0.0136],\n",
       "        [-0.0200, -0.0136]], device='cuda:3', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T15:53:20.828534Z",
     "start_time": "2019-03-21T15:53:20.821086Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5301, 0.4699],\n",
       "        [0.5000, 0.5000],\n",
       "        [0.5078, 0.4922],\n",
       "        [0.5033, 0.4967],\n",
       "        [0.4955, 0.5045],\n",
       "        [0.4961, 0.5039],\n",
       "        [0.4990, 0.5010],\n",
       "        [0.5019, 0.4981],\n",
       "        [0.4904, 0.5096],\n",
       "        [0.5081, 0.4919],\n",
       "        [0.5174, 0.4826],\n",
       "        [0.4984, 0.5016],\n",
       "        [0.4984, 0.5016],\n",
       "        [0.4984, 0.5016],\n",
       "        [0.4984, 0.5016],\n",
       "        [0.4984, 0.5016]], device='cuda:3', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T16:07:40.347426Z",
     "start_time": "2019-03-21T16:07:40.309699Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-14f9aec54ed9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
