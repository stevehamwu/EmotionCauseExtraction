{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T13:57:58.306033Z",
     "start_time": "2019-03-25T13:57:58.301819Z"
    },
    "collapsed": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build ELMo Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:46:43.126956Z",
     "start_time": "2019-03-23T19:46:43.039014Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from elmoformanylangs import Embedder, CNEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T19:46:50.513922Z",
     "start_time": "2019-03-23T19:46:43.317910Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-23 19:46:43,394 INFO: char embedding size: 6169\n",
      "2019-03-23 19:46:43,762 INFO: word embedding size: 71222\n",
      "2019-03-23 19:46:50,298 INFO: Model(\n",
      "  (token_embedder): ConvTokenEmbedder(\n",
      "    (word_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(71222, 100, padding_idx=3)\n",
      "    )\n",
      "    (char_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(6169, 50, padding_idx=6166)\n",
      "    )\n",
      "    (convolutions): ModuleList(\n",
      "      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n",
      "      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n",
      "      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n",
      "      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n",
      "      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
      "      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n",
      "      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n",
      "    )\n",
      "    (highways): Highway(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "        (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (projection): Linear(in_features=2148, out_features=512, bias=True)\n",
      "  )\n",
      "  (encoder): ElmobiLm(\n",
      "    (forward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (forward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "e = Embedder('/data/wujipeng/embedding/ELMo/zhs.model/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T14:23:49.161077Z",
     "start_time": "2019-03-22T14:23:49.148246Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/data/wujipeng/ec/data/raw_data/elmo_vocab.txt') as f:\n",
    "    unk = f.readline().strip()\n",
    "    vocab = f.readline().strip().split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T14:23:50.240595Z",
     "start_time": "2019-03-22T14:23:50.214674Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6319926966577065\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for word in vocab:\n",
    "    if e.word_lexicon.get(word):\n",
    "        cnt += 1\n",
    "print(cnt/len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T09:59:30.790563Z",
     "start_time": "2019-03-20T09:59:30.704224Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LTP_DATA_DIR = '/home/wujipeng/data/ltp_data_v3.4.0/'  # ltp模型目录的路径\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "from pyltp import Segmentor\n",
    "segmentor = Segmentor()  # 初始化实例\n",
    "segmentor.load(cws_model_path)  # 加载模型\n",
    "\n",
    "def ltp_cut(sentence):\n",
    "    words = segmentor.segment(sentence)  # 分词\n",
    "    return list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T14:23:58.594903Z",
     "start_time": "2019-03-22T14:23:58.212636Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LTP_DATA_DIR = '/home/wujipeng/data/ltp_data_v3.4.0/'  # ltp模型目录的路径\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')\n",
    "\n",
    "from pyltp import Segmentor, Postagger\n",
    "\n",
    "segmentor = Segmentor()  # 初始化实例\n",
    "segmentor.load(cws_model_path)  # 加载模型\n",
    "postagger = Postagger()\n",
    "postagger.load(pos_model_path)\n",
    "\n",
    "def ltp_seg(sentence):\n",
    "    words, natures = [], []\n",
    "    for sen in sentence.split('\\x01'):\n",
    "        seg_words = segmentor.segment(sen)\n",
    "        seg_natures = postagger.postag(seg_words)\n",
    "        words += list(seg_words) + ['\\x01']\n",
    "        natures += [nature[0] for nature in list(seg_natures)] + ['\\x01']\n",
    "    \n",
    "    return words[:-1], natures[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T20:48:45.461303Z",
     "start_time": "2019-03-19T20:48:45.454992Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "segmentor.release()  # 释放模型\n",
    "postagger.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T20:48:45.706699Z",
     "start_time": "2019-03-19T20:48:45.703208Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T10:14:02.718294Z",
     "start_time": "2019-03-20T10:14:02.688262Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['自己',\n",
       "  '经常',\n",
       "  '请',\n",
       "  '张',\n",
       "  '迎春',\n",
       "  '喝酒',\n",
       "  '看',\n",
       "  '电影',\n",
       "  '\\x01',\n",
       "  '一',\n",
       "  '次',\n",
       "  '聚会',\n",
       "  '酒后',\n",
       "  '在',\n",
       "  '消费单',\n",
       "  '上',\n",
       "  '签',\n",
       "  '上',\n",
       "  '了',\n",
       "  '自己',\n",
       "  '的',\n",
       "  '名字',\n",
       "  '\\x01',\n",
       "  '付',\n",
       "  '钱',\n",
       "  '时',\n",
       "  '才',\n",
       "  '发现',\n",
       "  '消费',\n",
       "  '了',\n",
       "  '5100',\n",
       "  '元',\n",
       "  '\\x01',\n",
       "  '情急之下',\n",
       "  '\\x01',\n",
       "  '向',\n",
       "  '波',\n",
       "  '找到',\n",
       "  '张迎春',\n",
       "  '借',\n",
       "  '钱',\n",
       "  '救急',\n",
       "  '对方',\n",
       "  '没',\n",
       "  '借',\n",
       "  '\\x01',\n",
       "  '向',\n",
       "  '波',\n",
       "  '于是',\n",
       "  '对',\n",
       "  '张心生',\n",
       "  '怨恨',\n",
       "  '\\x01',\n",
       "  '并',\n",
       "  '想',\n",
       "  '找',\n",
       "  '机会',\n",
       "  '好好',\n",
       "  '教训',\n",
       "  '一下',\n",
       "  '他',\n",
       "  '\\x01',\n",
       "  '4月',\n",
       "  '4日',\n",
       "  '\\x01',\n",
       "  '向',\n",
       "  '波',\n",
       "  '邀约',\n",
       "  '张迎春',\n",
       "  '和',\n",
       "  '另外',\n",
       "  '两',\n",
       "  '个',\n",
       "  '朋友',\n",
       "  '到',\n",
       "  '自己',\n",
       "  '老家',\n",
       "  '做客',\n",
       "  '\\x01',\n",
       "  '晚饭',\n",
       "  '后',\n",
       "  '几',\n",
       "  '个',\n",
       "  '朋友',\n",
       "  '玩',\n",
       "  '至',\n",
       "  '凌晨',\n",
       "  '1点',\n",
       "  '左右'],\n",
       " ['r',\n",
       "  'd',\n",
       "  'v',\n",
       "  'n',\n",
       "  'v',\n",
       "  'v',\n",
       "  'v',\n",
       "  'n',\n",
       "  '\\x01',\n",
       "  'm',\n",
       "  'q',\n",
       "  'v',\n",
       "  'n',\n",
       "  'p',\n",
       "  'n',\n",
       "  'n',\n",
       "  'v',\n",
       "  'v',\n",
       "  'u',\n",
       "  'r',\n",
       "  'u',\n",
       "  'n',\n",
       "  '\\x01',\n",
       "  'v',\n",
       "  'n',\n",
       "  'n',\n",
       "  'd',\n",
       "  'v',\n",
       "  'v',\n",
       "  'u',\n",
       "  'm',\n",
       "  'q',\n",
       "  '\\x01',\n",
       "  'i',\n",
       "  '\\x01',\n",
       "  'p',\n",
       "  'n',\n",
       "  'v',\n",
       "  'n',\n",
       "  'v',\n",
       "  'n',\n",
       "  'v',\n",
       "  'n',\n",
       "  'd',\n",
       "  'v',\n",
       "  '\\x01',\n",
       "  'p',\n",
       "  'n',\n",
       "  'c',\n",
       "  'p',\n",
       "  'n',\n",
       "  'v',\n",
       "  '\\x01',\n",
       "  'c',\n",
       "  'v',\n",
       "  'v',\n",
       "  'n',\n",
       "  'd',\n",
       "  'v',\n",
       "  'm',\n",
       "  'r',\n",
       "  '\\x01',\n",
       "  'n',\n",
       "  'n',\n",
       "  '\\x01',\n",
       "  'p',\n",
       "  'n',\n",
       "  'v',\n",
       "  'n',\n",
       "  'c',\n",
       "  'r',\n",
       "  'm',\n",
       "  'q',\n",
       "  'n',\n",
       "  'v',\n",
       "  'r',\n",
       "  'n',\n",
       "  'v',\n",
       "  '\\x01',\n",
       "  'n',\n",
       "  'n',\n",
       "  'm',\n",
       "  'q',\n",
       "  'n',\n",
       "  'v',\n",
       "  'p',\n",
       "  'n',\n",
       "  'n',\n",
       "  'm'])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltp_seg(line.strip().replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T10:13:54.618417Z",
     "start_time": "2019-03-20T10:13:50.334451Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "with open('/data/wujipeng/ec/data/raw_data/corpus.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        corpus += list(itertools.chain.from_iterable([ltp_cut(clause.strip().replace(' ', '')) for clause in line.strip().split('\\x01')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T20:51:01.715736Z",
     "start_time": "2019-03-19T20:51:01.706964Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in vocab:\n",
    "    if '\\x01' in word:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T20:50:35.397375Z",
     "start_time": "2019-03-19T20:50:35.373816Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = set(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T20:50:37.932264Z",
     "start_time": "2019-03-19T20:50:37.903557Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6321664129883308\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for word in vocab:\n",
    "    if e.word_lexicon.get(word):\n",
    "        cnt += 1\n",
    "print(cnt/len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建ELMo数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T14:24:38.616987Z",
     "start_time": "2019-03-22T14:24:38.604319Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_corpus(raw_corpus):\n",
    "    #  Hanlp分词\n",
    "    print('开始分词...')\n",
    "    corpus = []\n",
    "    natures = []\n",
    "    with open('/data/wujipeng/ec/data/raw_data/elmo_corpus.txt', 'w') as fc:\n",
    "        with open('/data/wujipeng/ec/data/raw_data/elmo_natures.txt', 'w') as fn:\n",
    "            for line in raw_corpus:\n",
    "                words, tags = ltp_seg(line)\n",
    "                words = ' '.join(words)\n",
    "                tags = ' '.join(tags)\n",
    "                corpus.append(words)\n",
    "                natures.append(tags)\n",
    "                fc.write(words + '\\n')\n",
    "                fn.write(tags + '\\n')\n",
    "    print('完成分词')\n",
    "    return corpus, natures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T14:25:45.598470Z",
     "start_time": "2019-03-22T14:25:45.579816Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def build_vocab(corpus):\n",
    "    # build vocab\n",
    "    print('Ltp 分词')\n",
    "    vocab_cnt = Counter()\n",
    "    for line in corpus:\n",
    "        for word in line.replace('\\x01', '').split(' '):\n",
    "            if word.strip():\n",
    "                vocab_cnt[word] += 1\n",
    "    vocab = [word for word, freq in vocab_cnt.most_common()]\n",
    "    with open('/data/wujipeng/ec/data/raw_data/elmo_vocab.txt', 'w') as f:\n",
    "        f.write('<unk>\\n')\n",
    "        f.write(' '.join(vocab))\n",
    "    print('词典大小: ', len(vocab) + 2)\n",
    "    print('保存词典')\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T14:24:40.809088Z",
     "start_time": "2019-03-22T14:24:40.803400Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_data(data):\n",
    "    data['id'] = list(range(1, len(data) + 1))\n",
    "    data[['id', 'clause', 'nature', 'keyword', 'emotion', 'clause_pos', 'label']].to_csv(\n",
    "        '/data/wujipeng/ec/data/han/ltp_processed_data.csv', index=False)\n",
    "    print('保存处理后数据')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-22T14:25:53.959648Z",
     "start_time": "2019-03-22T14:25:46.735179Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始分词...\n",
      "完成分词\n",
      "Ltp 分词\n",
      "词典大小:  19719\n",
      "保存词典\n",
      "保存处理后数据\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_root = '/data/wujipeng/ec/data/'\n",
    "data = pd.read_csv(os.path.join(data_root, 'raw_data', 'process_data_3.csv'), index_col=0)\n",
    "corpus = [text.replace(' ', '') for text in data['clause'].tolist()]\n",
    "keyword = [' '.join(ltp_seg(e)[0]) for e in data['keyword'].tolist()]\n",
    "poses = [list(map(int, pos.split(' '))) for pos in data['clause_pos'].tolist()]\n",
    "min_pos = min(min(poses))\n",
    "poses = [' '.join([str(p - min_pos + 1) for p in pos]) for pos in poses]\n",
    "\n",
    "# corpus = load_corpus(corpus)\n",
    "corpus, natures = load_corpus(corpus)\n",
    "vocab = build_vocab(corpus + keyword)\n",
    "\n",
    "data['clause'] = corpus\n",
    "data['nature'] = natures\n",
    "data['keyword'] = keyword\n",
    "data['clause_pos'] = poses\n",
    "save_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T10:36:13.653157Z",
     "start_time": "2019-03-20T10:36:13.644242Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_set = set([int(line.strip().split(',', 1)[0]) for line in open('/data/wujipeng/ec/data/test/val_set.txt').readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T10:36:13.855799Z",
     "start_time": "2019-03-20T10:36:13.850827Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = set(range(1, 2106)) - val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T10:36:14.157155Z",
     "start_time": "2019-03-20T10:36:14.123430Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_root = '/data/wujipeng/ec/data/'\n",
    "path = '/data/wujipeng/ec/data/ltp_test/'\n",
    "with open(os.path.join(data_root, 'han', 'ltp_processed_data.csv'), 'r') as f:\n",
    "    with open(os.path.join(path, 'train_set.txt'), 'w') as ft:\n",
    "        with open(os.path.join(path, 'val_set.txt'), 'w') as fd:\n",
    "            f.readline()\n",
    "            for line in f.readlines():\n",
    "                sid = int(line.strip().split(',')[0])\n",
    "                fw = fd if sid in val_set else ft\n",
    "                fw.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T10:36:16.455213Z",
     "start_time": "2019-03-20T10:36:14.510178Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1, 101):\n",
    "    data_root = '/data/wujipeng/ec/data/static/static.{}'.format(i)\n",
    "    val_set = set([int(line.strip().split(',', 1)[0]) for line in open(os.path.join(data_root, 'val_set.txt')).readlines()])\n",
    "    train_set = set(range(1, 2106)) - val_set\n",
    "    \n",
    "    output_root = '/data/wujipeng/ec/data/ltp_static/'\n",
    "    if not os.path.exists(os.path.join(output_root, 'static.{}'.format(i))):\n",
    "        os.makedirs(os.path.join(output_root, 'static.{}'.format(i)))\n",
    "    with open(os.path.join('/data/wujipeng/ec/data/han/', 'ltp_processed_data.csv'), 'r') as f:\n",
    "        with open(os.path.join(output_root, 'static.{}'.format(i), 'train_set.txt'), 'w') as ft:\n",
    "            with open(os.path.join(output_root, 'static.{}'.format(i), 'val_set.txt'), 'w') as fd:\n",
    "                f.readline()\n",
    "                for line in f.readlines():\n",
    "                    sid = int(line.strip().split(',')[0])\n",
    "                    fw = fd if sid in val_set else ft\n",
    "                    fw.write(line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T22:46:40.895224Z",
     "start_time": "2019-03-17T22:46:40.882731Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(path, 'train_set.txt')) as f:\n",
    "    sid, clauses, natures, keywords, _, pos, label = f.readline().strip().split(',')\n",
    "    sid = int(sid)\n",
    "    clauses = [clause.strip().split(' ') for clause in clauses.split('\\x01')]\n",
    "    natures = [nature.strip().split(' ') for nature in natures.split('\\x01')]\n",
    "    keywords = keywords.strip().split(' ')\n",
    "    pos = [int(p) for p in pos.strip().split(' ')]\n",
    "    label = [int(l) for l in label.strip().split(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T22:47:02.411804Z",
     "start_time": "2019-03-17T22:47:02.402435Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " [['当', '我', '看到', '建议', '被', '采纳'],\n",
       "  ['部委', '领导', '写给', '我', '的', '回信', '时'],\n",
       "  ['我', '知道', '我', '正在', '为', '这个', '国家', '的', '发展', '尽', '着', '一', '份', '力量'],\n",
       "  ['27', '日'],\n",
       "  ['河北省', '邢台', '钢铁', '有限公司', '的', '普通工人', '白金跃'],\n",
       "  ['拿着', '历年来', '国家', '各部委', '反馈', '给', '他', '的', '感谢信'],\n",
       "  ['激动', '地', '对', '中新网', '记者', '说'],\n",
       "  ['27', '年来'],\n",
       "  ['国家公安部',\n",
       "   '国家工商总局',\n",
       "   '国家科学技术委员会',\n",
       "   '科技部',\n",
       "   '卫生部',\n",
       "   '国家',\n",
       "   '发展改革委员会',\n",
       "   '等',\n",
       "   '部委',\n",
       "   '均',\n",
       "   '接受',\n",
       "   '并',\n",
       "   '采纳',\n",
       "   '过',\n",
       "   '的',\n",
       "   '我',\n",
       "   '的',\n",
       "   '建议']],\n",
       " [['p', 'r', 'v', 'n', 'p', 'v'],\n",
       "  ['n', 'n', 'v', 'r', 'u', 'n', 'q'],\n",
       "  ['r', 'v', 'r', 'd', 'p', 'r', 'n', 'u', 'v', 'v', 'u', 'm', 'q', 'n'],\n",
       "  ['m', 'b'],\n",
       "  ['n', 'n', 'n', 'n', 'u', 'n', 'n'],\n",
       "  ['v', 'd', 'n', 'n', 'v', 'p', 'r', 'u', 'n'],\n",
       "  ['a', 'u', 'p', 'n', 'n', 'v'],\n",
       "  ['m', 'n'],\n",
       "  ['n',\n",
       "   'n',\n",
       "   'n',\n",
       "   'n',\n",
       "   'n',\n",
       "   'n',\n",
       "   'n',\n",
       "   'u',\n",
       "   'n',\n",
       "   'd',\n",
       "   'v',\n",
       "   'c',\n",
       "   'v',\n",
       "   'u',\n",
       "   'u',\n",
       "   'r',\n",
       "   'u',\n",
       "   'n']],\n",
       " ['激动'],\n",
       " [63, 64, 65, 66, 67, 68, 69, 70, 71],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid, clauses, natures, keywords, pos, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T15:40:14.697387Z",
     "start_time": "2019-03-25T15:40:14.679080Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError\n",
    "    data = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            data.append(line.strip().split(','))\n",
    "    return data\n",
    "\n",
    "\n",
    "def pad_sequence(sequences, sequence_length=40, pad=0):\n",
    "    paded_sequences = []\n",
    "    for sequence in sequences:\n",
    "        sl = sequence_length - len(sequence)\n",
    "        paded_sequences.append(sequence + [pad] * sl)\n",
    "    return paded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T13:58:01.156164Z",
     "start_time": "2019-03-25T13:58:00.679799Z"
    },
    "collapsed": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "class ELMoECDataset(Dataset):\n",
    "    def __init__(self, data_root, train=True):\n",
    "        super(ELMoECDataset, self).__init__()\n",
    "        self.train = train\n",
    "        self.data_path = os.path.join(data_root, '{}_set.txt'.format('train' if train else 'val'))\n",
    "        self.read_data()\n",
    "\n",
    "    def read_data(self):\n",
    "        self.data = load_data(self.data_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        n_id, n_clauses, n_natures, n_keyword, n_emotion, n_pos, n_label = item\n",
    "        n_clauses = [clause.strip().split(' ') for clause in n_clauses.split('\\x01')]\n",
    "        n_natures = [nature.strip().split(' ') for nature in n_natures.split('\\x01')]\n",
    "        n_keyword = n_keyword.strip().split(' ')\n",
    "        n_pos = n_pos.strip().split(' ')\n",
    "        return tuple([n_id, n_clauses, n_natures, n_keyword, n_emotion, n_pos, n_label])\n",
    "\n",
    "    def collate_fn(self, batch_data, pad=True, clause_length=41, batch_size=16):\n",
    "        batch_data = list(zip(*batch_data))\n",
    "        ids, clauses, natures, keywords, emotions, poses, labels = batch_data\n",
    "        ids = list(map(int, ids))\n",
    "        emotions = list(map(int, emotions))\n",
    "        poses = [list(map(int, pos)) for pos in poses]\n",
    "        labels = [[int(l) for l in label.strip().split(' ')] for label in labels]\n",
    "        if pad:\n",
    "            sentence_length = max([len(label) for label in labels])\n",
    "            clauses = [pad_sequence(clause, clause_length, pad='<pad>') + [['<pad>'] * clause_length] * (sentence_length - len(clause)) for\n",
    "                       clause in clauses]\n",
    "            natures = [\n",
    "                pad_sequence(nature, clause_length, pad='w') + [['w'] * clause_length] * (sentence_length - len(nature)) for\n",
    "                nature in natures]\n",
    "            keywords = pad_sequence(keywords, clause_length, pad='<pad>')\n",
    "            poses = [pos + [0] * (sentence_length - len(pos)) for pos in poses]\n",
    "            labels = [label + [-100] * (sentence_length - len(label)) for label in labels]\n",
    "            if len(ids) < batch_size:\n",
    "                bs = batch_size - len(ids)\n",
    "                ids += [0] * bs\n",
    "                clauses += [[['<pad>'] * clause_length] * sentence_length] * bs\n",
    "                natures += [[['w'] * clause_length] * sentence_length] * bs\n",
    "                keywords += [['<pad>'] * clause_length] * bs\n",
    "                emotions += [0] * bs\n",
    "                poses += [[0] * sentence_length] * bs\n",
    "                labels += [[-100] * sentence_length] * bs\n",
    "\n",
    "        return ids, np.array(clauses), natures, keywords, np.array(emotions), np.array(poses), np.array(labels)\n",
    "\n",
    "    def batch2input(self, batch):\n",
    "        return batch[1], batch[3], batch[5]\n",
    "\n",
    "    def batch2target(self, batch):\n",
    "        return batch[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T15:40:15.927555Z",
     "start_time": "2019-03-25T15:40:15.768323Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# no pad\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "def elmo_mean(arr):\n",
    "    return np.mean(arr, axis=0)\n",
    "\n",
    "\n",
    "class ELMoECDataset(Dataset):\n",
    "    def __init__(self, data_root, elmo_embedding, kw_embedding, train=True):\n",
    "        super(ELMoECDataset, self).__init__()\n",
    "        self.train = train\n",
    "        self.data_path = os.path.join(\n",
    "            data_root, '{}_set.txt'.format('train' if train else 'val'))\n",
    "        self.read_data()\n",
    "        self.elmo_embedding = pickle.load(\n",
    "            open(elmo_embedding, 'rb'))\n",
    "\n",
    "    def read_data(self):\n",
    "        self.data = load_data(self.data_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        n_id, n_clauses, n_natures, n_keyword, n_emotion, n_pos, n_label = item\n",
    "        n_clauses = [\n",
    "            clause.strip().split(' ') for clause in n_clauses.split('\\x01')\n",
    "        ]\n",
    "        n_natures = [\n",
    "            nature.strip().split(' ') for nature in n_natures.split('\\x01')\n",
    "        ]\n",
    "        n_keyword = n_keyword.strip().split(' ')\n",
    "        n_pos = n_pos.strip().split(' ')\n",
    "        return tuple(\n",
    "            [n_id, n_clauses, n_natures, n_keyword, n_emotion, n_pos, n_label])\n",
    "\n",
    "    def avg_collate_fn(self,\n",
    "                       batch_data,\n",
    "                       clause_length=40,\n",
    "                       pad=True,\n",
    "                       batch_size=16):\n",
    "        elmo_dim = 1024\n",
    "        batch_data = list(zip(*batch_data))\n",
    "        ids, sentences, natures, keywords, emotions, poses, labels = batch_data\n",
    "        ids = list(map(int, ids))\n",
    "        elmos = self.elmo_embedding[np.array(ids) - 1]\n",
    "        elmos = [np.array(list(map(elmo_mean, elmo)), dtype=np.float32) for elmo in elmos]\n",
    "        emotions = list(map(int, emotions))\n",
    "        poses = [list(map(int, pos)) for pos in poses]\n",
    "        labels = [[int(l) for l in label.strip().split(' ')]\n",
    "                  for label in labels]\n",
    "        if pad:\n",
    "            sentence_size = max([len(sentence) for sentence in sentences])\n",
    "            elmos = np.array(\n",
    "                [\n",
    "                    np.vstack(\n",
    "                        (elmo, np.zeros(\n",
    "                            (sentence_size - len(elmo), elmo_dim))))\n",
    "                    for elmo in elmos\n",
    "                ],\n",
    "                dtype=np.float32)\n",
    "            poses = [pos + [0] * (sentence_size - len(pos)) for pos in poses]\n",
    "            labels = [\n",
    "                label + [-100] * (sentence_size - len(label))\n",
    "                for label in labels\n",
    "            ]\n",
    "            if len(ids) < batch_size:\n",
    "                bs = batch_size - len(ids)\n",
    "                ids += [0] * bs\n",
    "                elmos = np.vstack((elmos,\n",
    "                                   np.zeros((bs, sentence_size, elmo_dim), dtype=np.float32)))\n",
    "                emotions += [0] * bs\n",
    "                poses += [[0] * sentence_size] * bs\n",
    "                labels += [[-100] * sentence_size] * bs\n",
    "        return ids, elmos, natures, keywords, emotions, np.array(\n",
    "            poses), np.array(labels)\n",
    "\n",
    "    def collate_fn(self, batch_data, pad=True, clause_length=41,\n",
    "                   batch_size=16):\n",
    "        batch_data = list(zip(*batch_data))\n",
    "        ids, sentences, natures, keywords, emotions, poses, labels = batch_data\n",
    "        ids = list(map(int, ids))\n",
    "        emotions = list(map(int, emotions))\n",
    "        poses = [list(map(int, pos)) for pos in poses]\n",
    "        labels = [[int(l) for l in label.strip().split(' ')]\n",
    "                  for label in labels]\n",
    "        if pad:\n",
    "            sentence_length = max([len(label) for label in labels])\n",
    "            sentences = [\n",
    "                pad_sequence(sentence, clause_length, pad='<pad>') +\n",
    "                [['<pad>'] * clause_length] * (sentence_length - len(clause))\n",
    "                for sentence in sentences\n",
    "            ]\n",
    "            natures = [\n",
    "                pad_sequence(nature, clause_length, pad='w') +\n",
    "                [['w'] * clause_length] * (sentence_length - len(nature))\n",
    "                for nature in natures\n",
    "            ]\n",
    "            keywords = pad_sequence(keywords, clause_length, pad='<pad>')\n",
    "            poses = [pos + [0] * (sentence_length - len(pos)) for pos in poses]\n",
    "            labels = [\n",
    "                label + [-100] * (sentence_length - len(label))\n",
    "                for label in labels\n",
    "            ]\n",
    "            if len(ids) < batch_size:\n",
    "                bs = batch_size - len(ids)\n",
    "                ids += [0] * bs\n",
    "                sentences += [[['<pad>'] * clause_length] * sentence_length\n",
    "                              ] * bs\n",
    "                natures += [[['w'] * clause_length] * sentence_length] * bs\n",
    "                keywords += [['<pad>'] * clause_length] * bs\n",
    "                emotions += [0] * bs\n",
    "                poses += [[0] * sentence_length] * bs\n",
    "                labels += [[-100] * sentence_length] * bs\n",
    "\n",
    "        return ids, sentences, natures, keywords, np.array(emotions), np.array(\n",
    "            poses), np.array(labels)\n",
    "\n",
    "    def batch2input(self, batch):\n",
    "        return batch[1], batch[3], batch[5]\n",
    "\n",
    "    def batch2target(self, batch):\n",
    "        return batch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T15:40:18.759832Z",
     "start_time": "2019-03-25T15:40:16.796729Z"
    },
    "collapsed": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "train_dataset = ELMoECDataset('/data/wujipeng/ec/data/ltp_test/', '/data/wujipeng/ec/data/embedding/elmo_sent_embedding1024d.pkl', '/data/wujipeng/ec/data/embedding/elmo_kw_embedding1024d.pkl', train=True)\n",
    "eval_dataset = ELMoECDataset('/data/wujipeng/ec/data/ltp_test/', '/data/wujipeng/ec/data/embedding/elmo_sent_embedding1024d.pkl', '/data/wujipeng/ec/data/embedding/elmo_kw_embedding1024d.pkl', train=False)\n",
    "dataset = train_dataset + eval_dataset\n",
    "sentences = list(zip(*dataset))[1] + list(zip(*dataset))[3]\n",
    "sequence_length = max([max([len(clause) for clause in sentence]) for sentence in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:17:04.127908Z",
     "start_time": "2019-03-25T14:17:03.788903Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataloader = ECDataLoader(train_dataset, sequence_length, batch_size=batch_size, shuffle=False, sort=True, collate_fn=train_dataset.avg_collate_fn)\n",
    "eval_dataloader = ECDataLoader(eval_dataset, sequence_length, batch_size=batch_size, shuffle=False, sort=False, collate_fn=eval_dataset.avg_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:12:09.679421Z",
     "start_time": "2019-03-25T14:12:09.660329Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    clauses, keywords, poses = train_dataset.batch2input(batch)\n",
    "    targets = train_dataset.batch2target(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T15:39:09.910484Z",
     "start_time": "2019-03-25T15:39:09.084224Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "elmo_embedding = pickle.load(open('/data/wujipeng/ec/data/embedding/elmo_sent_embedding1024d.pkl', 'rb'))\n",
    "kw_embedding = pickle.load(open('/data/wujipeng/ec/data/embedding/elmo_kw_embedding1024d.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T15:40:19.810894Z",
     "start_time": "2019-03-25T15:40:19.772716Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def elmo_mean(arr):\n",
    "    return np.mean(arr, axis=0)\n",
    "elmo_dim = 1024\n",
    "batch_data = random.choices(train_dataset, k=10)\n",
    "batch_data = list(zip(*batch_data))\n",
    "ids, sentences, natures, keywords, emotions, poses, labels = batch_data\n",
    "ids = list(map(int, ids))\n",
    "elmos = elmo_embedding[np.array(ids) - 1]\n",
    "elmos = [np.array(list(map(elmo_mean, elmo))) for elmo in elmos]\n",
    "kw_elmos = kw_embedding[np.array(ids) - 1]\n",
    "keywords = np.array(list(map(elmo_mean, kw_elmos)))\n",
    "emotions = list(map(int, emotions))\n",
    "poses = [list(map(int, pos)) for pos in poses]\n",
    "labels = [[int(l) for l in label.strip().split(' ')] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T15:41:35.606381Z",
     "start_time": "2019-03-25T15:41:35.589307Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pad = True\n",
    "if pad:\n",
    "    sentence_size = max([len(sentence) for sentence in sentences])\n",
    "    elmos = np.array([np.vstack((elmo, np.zeros((sentence_size - len(elmo), elmo_dim)))) for elmo in elmos])\n",
    "    poses = [pos + [0] * (sentence_size - len(pos)) for pos in poses]\n",
    "    labels = [label + [-100] * (sentence_size - len(label)) for label in labels]\n",
    "    if len(ids) < batch_size:\n",
    "        bs = batch_size - len(ids)\n",
    "        ids += [0] * bs\n",
    "        elmos = np.vstack((elmos, np.zeros((bs, sentence_size, elmo_dim))))\n",
    "        keywords = np.vstack((keywords, np.zeros((bs, elmo_dim))))\n",
    "        emotions += [0] * bs\n",
    "        poses += [[0] * sentence_size] * bs\n",
    "        labels += [[-100] * sentence_size] * bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T10:51:19.277545Z",
     "start_time": "2019-03-24T10:51:19.258643Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# avg\n",
    "def elmo_mean(arr):\n",
    "    return np.mean(arr, axis=0)\n",
    "elmos = elmo[np.array(ids) - 1]\n",
    "elmos = [np.array(list(map(elmo_mean, elmo))) for elmo in elmos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T20:25:48.415913Z",
     "start_time": "2019-03-23T20:25:48.411588Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = elmo[np.array(ids) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T20:40:49.347721Z",
     "start_time": "2019-03-23T20:40:49.188018Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 30, 40, 1024)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad\n",
    "elmo_dim = 1024\n",
    "sentence_size = max([len(sentence) for sentence in sentences])\n",
    "padded_sentences = []\n",
    "for sentence in sentences:\n",
    "    padded_sentence = []\n",
    "    for clause in sentence:\n",
    "        padded_sentence.append(np.vstack((clause, np.zeros((sequence_length-clause.shape[0], elmo_dim)))))\n",
    "    padded_sentences.append(np.vstack((np.array(padded_sentence), np.zeros((sentence_size - len(padded_sentence), sequence_length, elmo_dim)))))\n",
    "np.array(padded_sentences).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T15:41:21.144872Z",
     "start_time": "2019-03-25T15:41:21.142342Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:19:33.027880Z",
     "start_time": "2019-03-25T14:19:32.939178Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class ECDataLoader:\n",
    "    def __init__(self, dataset, clause_length, batch_size=16, shuffle=True, sort=True, auto_refresh=True,\n",
    "                 collate_fn=None):\n",
    "        self.dataset = dataset\n",
    "        self.size = len(dataset)\n",
    "        self.clause_length = clause_length\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.sort = sort\n",
    "        self.batches = None\n",
    "        self.num_batches = None\n",
    "        self.auto_refresh = auto_refresh\n",
    "        self.collate_fn = collate_fn\n",
    "        self.inst_count = 0\n",
    "        self.batch_count = 0\n",
    "        self._curr_batch = 0\n",
    "        self._curr_num_insts = None\n",
    "        if self.sort:\n",
    "            self.dataset = sorted(dataset, key=lambda item: len(item[1]))\n",
    "        else:\n",
    "            self.dataset = list(dataset)\n",
    "        if self.auto_refresh:\n",
    "            self.refresh()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches\n",
    "\n",
    "    def next(self):\n",
    "        if self._curr_batch and self._curr_batch + 1 >= self.num_batches:\n",
    "            if self.auto_refresh:\n",
    "                self.refresh()\n",
    "            raise StopIteration\n",
    "        data = self.get_data()\n",
    "        return data\n",
    "\n",
    "    def get_data(self):\n",
    "        self._curr_batch = (self._curr_batch + 1) if self._curr_batch is not None else 0\n",
    "        self._curr_num_insts = len(self.batches[self._curr_batch])\n",
    "\n",
    "        self.inst_count += self._curr_num_insts\n",
    "        self.batch_count += 1\n",
    "        data = self.batches[self._curr_batch]\n",
    "        if self.collate_fn:\n",
    "            data = self.collate_fn(data, clause_length=self.clause_length, batch_size=self.batch_size)\n",
    "        return data\n",
    "\n",
    "    def refresh(self):\n",
    "        self.batches = []\n",
    "        batch_start = 0\n",
    "        for i in range(self.size // self.batch_size):\n",
    "            self.batches.append(self.dataset[batch_start: batch_start + self.batch_size])\n",
    "            batch_start += self.batch_size\n",
    "        if batch_start != self.size:\n",
    "            self.batches.append(self.dataset[batch_start:])\n",
    "        if self.shuffle:\n",
    "            random.shuffle(self.batches)\n",
    "        self.num_batches = len(self.batches)\n",
    "        self._curr_batch = None\n",
    "        self._curr_num_insts = None\n",
    "\n",
    "    def state_dict(self):\n",
    "        \"\"\"\n",
    "        Warning! side effect: np_randomstate will influence other\n",
    "            potion of the program.\n",
    "        \"\"\"\n",
    "        state = {\n",
    "            \"batch_size\" : self.batch_size,\n",
    "            \"shuffle\" : self.shuffle,\n",
    "            \"batches\" : self.batches,\n",
    "            \"num_batches\" : self.num_batches,\n",
    "            \"auto_refresh\" : self.auto_refresh,\n",
    "            \"inst_count\" : self.inst_count,\n",
    "            \"batch_count\" : self.batch_count,\n",
    "            \"_curr_batch\" : self._curr_batch,\n",
    "            \"_curr_num_insts\" : self._curr_num_insts,\n",
    "            \"np_randomstate\" : np.random.get_state(),\n",
    "        }\n",
    "        return state\n",
    "\n",
    "    def load_state_dict(self, state):\n",
    "        \"\"\"\n",
    "        Warning! side effect: np_randomstate will influence other\n",
    "            potion of the program.\n",
    "        \"\"\"\n",
    "        self.batch_size = state[\"batch_size\"]\n",
    "        self.shuffle = state[\"shuffle\"]\n",
    "        self.batches = state[\"batches\"]\n",
    "        self.num_batches = state[\"num_batches\"]\n",
    "        self.auto_refresh = state[\"auto_refresh\"]\n",
    "        self.inst_count = state[\"inst_count\"]\n",
    "        self.batch_count = state[\"batch_count\"]\n",
    "        self._curr_batch = state[\"_curr_batch\"]\n",
    "        self._curr_num_insts = state[\"_curr_num_insts\"]\n",
    "        np.random.set_state(state[\"np_randomstate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:19:34.586798Z",
     "start_time": "2019-03-25T14:19:34.448731Z"
    },
    "collapsed": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = ECDataLoader(train_dataset, sequence_length, batch_size=batch_size, shuffle=False, sort=True, collate_fn=train_dataset.avg_collate_fn)\n",
    "eval_dataloader = ECDataLoader(eval_dataset, sequence_length, batch_size=batch_size, shuffle=False, sort=False, collate_fn=eval_dataset.avg_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:27:08.507067Z",
     "start_time": "2019-03-25T14:27:08.492819Z"
    },
    "collapsed": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    clauses, keywords, poses = train_dataset.batch2input(batch)\n",
    "    targets = train_dataset.batch2target(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T15:55:21.810154Z",
     "start_time": "2019-03-18T15:55:21.805571Z"
    }
   },
   "source": [
    "# ELMo Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:19:05.334212Z",
     "start_time": "2019-03-25T14:19:05.328595Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:19:05.554302Z",
     "start_time": "2019-03-25T14:19:05.547444Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path = [\"..\"] + sys.path\n",
    "from models.han.attention import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:35:15.381910Z",
     "start_time": "2019-03-19T10:35:15.372092Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attention = Attention()\n",
    "Dropout = nn.Dropout(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T16:07:54.680608Z",
     "start_time": "2019-03-18T16:07:54.677345Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# params sentences, keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:35:16.901823Z",
     "start_time": "2019-03-19T10:35:16.890703Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = np.array(clauses)\n",
    "queries = np.array(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:35:17.356800Z",
     "start_time": "2019-03-19T10:35:17.348730Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.expand_dims(sentences != '<pad>', -2)\n",
    "q = np.expand_dims(np.expand_dims(queries, 1).repeat(sentences.shape[1], axis=1) != '<pad>', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:35:17.751872Z",
     "start_time": "2019-03-19T10:35:17.740100Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 73, 41, 41])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.from_numpy((q*x).astype(int))\n",
    "mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:36:44.310299Z",
     "start_time": "2019-03-19T10:36:27.452456Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 10:36:30,577 INFO: 19 batches, avg len: 43.0\n",
      "2019-03-19 10:36:42,330 INFO: Finished 1000 sentences.\n",
      "2019-03-19 10:36:43,906 INFO: 1 batches, avg len: 43.0\n"
     ]
    }
   ],
   "source": [
    "s_embed = torch.from_numpy(np.array(ELMo_Embedding.sents2elmo(sentences.reshape(-1, sequence_length))))\n",
    "q_embed = torch.from_numpy(np.array(ELMo_Embedding.sents2elmo(queries)))\n",
    "s_embed.requires_grad = True\n",
    "q_embed.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:36:45.722606Z",
     "start_time": "2019-03-19T10:36:45.716370Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s_embed = s_embed.view(batch_size, -1, s_embed.size(-2), s_embed.size(-1))\n",
    "q_embed = q_embed.unsqueeze(1).expand_as(s_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:36:46.957812Z",
     "start_time": "2019-03-19T10:36:46.948834Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 73, 41, 1024]), torch.Size([16, 73, 41, 1024]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_embed.size(), q_embed.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:36:49.690420Z",
     "start_time": "2019-03-19T10:36:49.319630Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values, word_attn = attention(q_embed, s_embed, s_embed, mask=mask, dropout=Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:36:50.716381Z",
     "start_time": "2019-03-19T10:36:50.660425Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 73, 1024])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = values.sum(dim=2)\n",
    "values.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:30:36.566041Z",
     "start_time": "2019-03-25T14:30:36.552323Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    clauses, keywords, poses = train_dataset.batch2input(batch)\n",
    "    targets = train_dataset.batch2target(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:30:38.066140Z",
     "start_time": "2019-03-25T14:30:37.360109Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_size = 103\n",
    "pos_embedding_size = 128\n",
    "elmo_dim = 1024\n",
    "hidden_dim = 1152\n",
    "dropout = 0.5\n",
    "sent_rnn = nn.GRU(elmo_dim+pos_embedding_size, hidden_dim, num_layers=2, bidirectional=True, batch_first=True, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:30:38.069802Z",
     "start_time": "2019-03-25T14:30:38.067441Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clauses = torch.from_numpy(clauses)\n",
    "poses = torch.from_numpy(poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:30:38.156855Z",
     "start_time": "2019-03-25T14:30:38.070909Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_embedding = nn.Embedding(pos_size, pos_embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:30:38.263946Z",
     "start_time": "2019-03-25T14:30:38.257301Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 13, 128])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses = pos_embedding(poses)\n",
    "poses.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:30:38.521122Z",
     "start_time": "2019-03-25T14:30:38.516605Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 13, 1152])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_elmo_pos = torch.cat([clauses, poses], dim=-1)\n",
    "sent_elmo_pos.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:30:41.491682Z",
     "start_time": "2019-03-25T14:30:39.408076Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 13, 2304])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_hidden, _ = sent_rnn(sent_elmo_pos)\n",
    "sent_hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:30:49.063203Z",
     "start_time": "2019-03-25T14:30:49.040965Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc = nn.Linear(2 * hidden_dim, 2)\n",
    "outputs = fc(sent_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:30:49.974711Z",
     "start_time": "2019-03-25T14:30:49.968665Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 13, 2])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELMo Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:19:00.009367Z",
     "start_time": "2019-03-25T14:19:00.000858Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "elmo_dim = 1024\n",
    "hidden_size = 300\n",
    "layers = 2\n",
    "sequence_length = 40\n",
    "num_classes = 2\n",
    "dropout = 0.5\n",
    "max_epoch = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:19:00.452290Z",
     "start_time": "2019-03-25T14:19:00.447835Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T13:58:24.040434Z",
     "start_time": "2019-03-25T13:58:23.983298Z"
    },
    "collapsed": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "class ELMoModel(nn.Module):\n",
    "    def __init__(self, batch_size, elmo_dim, sequence_length, num_classes, device, dropout=0.5):\n",
    "        super(ELMoModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.elmo_dim = elmo_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        self.ELMo_Embedding = Embedder('/data/wujipeng/embedding/ELMo/zhs.model/')\n",
    "        self.ELMo_Embedding.model.to(device)\n",
    "        self.attention = Attention()\n",
    "        self.fc = nn.Linear(elmo_dim, num_classes)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def train(self):\n",
    "        super(ELMoModel, self).train()\n",
    "        self.ELMo_Embedding.model.train()\n",
    "        \n",
    "    def eval(self):\n",
    "        super(ELMoModel, self).eval()\n",
    "        self.ELMo_Embedding.model.eval()\n",
    "    \n",
    "    def forward(self, clauses, keywords):\n",
    "        sentences = np.array(clauses)\n",
    "        queries = np.array(keywords)\n",
    "        \n",
    "        s_embed = torch.from_numpy(np.array(self.ELMo_Embedding.sents2elmo(sentences.reshape(-1, sequence_length)))).to(self.device)\n",
    "        q_embed = torch.from_numpy(np.array(self.ELMo_Embedding.sents2elmo(queries))).to(self.device)\n",
    "        s_embed.requires_grad = True\n",
    "        q_embed.requires_grad = True\n",
    "        \n",
    "        s_embed = s_embed.view(batch_size, -1, s_embed.size(-2), s_embed.size(-1))\n",
    "        q_embed = q_embed.unsqueeze(1).expand_as(s_embed)\n",
    "        \n",
    "        x = np.expand_dims(sentences != '<pad>', -2)\n",
    "        q = np.expand_dims(np.expand_dims(queries, 1).repeat(sentences.shape[1], axis=1) != '<pad>', -1)\n",
    "        mask = torch.from_numpy((q*x).astype(int)).to(self.device)\n",
    "        \n",
    "        values, word_attn = self.attention(q_embed, s_embed, s_embed, mask=mask, dropout=self.dropout1)\n",
    "        values = values.sum(dim=2)\n",
    "        word_attn = word_attn.sum(dim=2)\n",
    "        \n",
    "        outputs = self.fc(values)\n",
    "        return outputs, word_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T13:58:25.624714Z",
     "start_time": "2019-03-25T13:58:25.557933Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ELMoModel(nn.Module):\n",
    "    def __init__(self, batch_size, elmo_dim, sequence_length, hidden_size, layers, num_classes, device, dropout=0.5):\n",
    "        super(ELMoModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.elmo_dim = elmo_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = layers\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        self.ELMo_Embedding = Embedder('/data/wujipeng/embedding/ELMo/zhs.model/')\n",
    "        self.ELMo_Embedding.model.to(device)\n",
    "        self.attention = Attention()\n",
    "        self.fc1 = nn.Linear(elmo_dim, hidden_size)\n",
    "        self.sent_gru = nn.GRU(hidden_size, hidden_size, num_layers=layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.fc2 = nn.Linear(2*hidden_size, num_classes)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def train(self):\n",
    "        super(ELMoModel, self).train()\n",
    "        self.ELMo_Embedding.model.train()\n",
    "        \n",
    "    def eval(self):\n",
    "        super(ELMoModel, self).eval()\n",
    "        self.ELMo_Embedding.model.eval()\n",
    "    \n",
    "    def forward(self, clauses, keywords):\n",
    "        sentences = np.array(clauses)\n",
    "        queries = np.array(keywords)\n",
    "        \n",
    "        s_embed = torch.from_numpy(np.array(self.ELMo_Embedding.sents2elmo(sentences.reshape(-1, sequence_length)))).to(self.device)\n",
    "        q_embed = torch.from_numpy(np.array(self.ELMo_Embedding.sents2elmo(queries))).to(self.device)\n",
    "        \n",
    "        s_embed = s_embed.view(batch_size, -1, s_embed.size(-2), s_embed.size(-1))\n",
    "        q_embed = q_embed.unsqueeze(1).expand_as(s_embed)\n",
    "        \n",
    "        x = np.expand_dims(sentences != '<pad>', -2)\n",
    "        q = np.expand_dims(np.expand_dims(queries, 1).repeat(sentences.shape[1], axis=1) != '<pad>', -1)\n",
    "        mask = torch.from_numpy((q*x).astype(int)).to(self.device)\n",
    "        \n",
    "        mask_x = torch.from_numpy(np.array(sentences != '<pad>', dtype=np.float32)).to(self.device)\n",
    "        values, word_attn = self.attention(q_embed, s_embed, s_embed, mask=mask, dropout=self.dropout1)\n",
    "        mask_x = mask_x.unsqueeze(-1).expand_as(values)\n",
    "        values = (values * mask_x).mean(dim=2)\n",
    "        word_attn = word_attn.sum(dim=2)\n",
    "        \n",
    "        sentences = self.fc1(self.dropout2(values))\n",
    "        \n",
    "        documents, _ = self.sent_gru(sentences)\n",
    "        outputs = self.fc2(self.dropout3(documents))\n",
    "        return outputs, word_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:58:03.596177Z",
     "start_time": "2019-03-25T14:58:03.575560Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ELMoModel(nn.Module):\n",
    "    def __init__(self, batch_size, elmo_dim, pos_size, pos_embedding_size, hidden_size, layers, num_classes, dropout=0.5):\n",
    "        super(ELMoModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.elmo_dim = elmo_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers = layers\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.pos_embedding = nn.Embedding(pos_size, pos_embedding_size, padding_idx=0)\n",
    "        self.sent_gru = nn.GRU(elmo_dim + pos_embedding_size, hidden_size+pos_embedding_size, num_layers=layers, bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(2*(hidden_size + pos_embedding_size) + elmo_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, clauses, poses):\n",
    "        poses = self.pos_embedding(poses)\n",
    "        clauses_with_pos = torch.cat([clauses, poses], dim=-1)\n",
    "        sent_hidden, _ = self.sent_gru(clauses_with_pos)\n",
    "        sent_res = torch.cat([sent_hidden, clauses], dim=-1)\n",
    "        outputs = self.fc(self.dropout(sent_res))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:58:05.296720Z",
     "start_time": "2019-03-25T14:58:04.940344Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "elmo_dim = 1024\n",
    "pos_size = 103\n",
    "pos_embedding_size = 128\n",
    "hidden_size = 1024\n",
    "layers = 2\n",
    "num_classes = 2\n",
    "dropout = 0.5\n",
    "model = ELMoModel(batch_size, elmo_dim, pos_size, pos_embedding_size, hidden_size, layers, num_classes, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:58:06.686939Z",
     "start_time": "2019-03-25T14:58:06.679480Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(reduction='mean', ignore_index=-100).to(device)\n",
    "# optimizer = torch.optim.Adam([{'params': model.ELMo_Embedding.model.parameters(), 'lr': 1e-3, 'betas': 0.8},\n",
    "#                               {'params': model.parameters()}], lr=3e-4, betas=[0.9, 0.999], eps=1e-8)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4, betas=[0.9, 0.999], eps=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T15:00:51.996467Z",
     "start_time": "2019-03-25T14:58:07.861947Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: [  0/25][  0/119] L: 0.7035 A: 0.5262 P: 0.4074 R: 0.6111 F: 0.4889 N: 0.0000\n",
      "E: [  0/25][ 15/119] L: 0.3347 A: 0.8398 P: 0.4211 R: 0.5000 F: 0.4571 N: 0.0000\n",
      "E: [  0/25][ 30/119] L: 0.1919 A: 0.8934 P: 0.6667 R: 0.3750 F: 0.4800 N: 0.0000\n",
      "E: [  0/25][ 45/119] L: 0.0992 A: 0.9932 P: 1.0000 R: 0.3750 F: 0.5455 N: 0.0000\n",
      "E: [  0/25][ 60/119] L: 0.1092 A: 0.9763 P: 0.6000 R: 0.5625 F: 0.5806 N: 0.0000\n",
      "E: [  0/25][ 75/119] L: 0.1032 A: 0.9764 P: 0.6250 R: 0.5882 F: 0.6061 N: 0.0000\n",
      "E: [  0/25][ 90/119] L: 0.1065 A: 0.9414 P: 0.7273 R: 0.5000 F: 0.5926 N: 0.0000\n",
      "E: [  0/25][105/119] L: 0.1275 A: 0.9141 P: 0.5556 R: 0.3125 F: 0.4000 N: 0.0000\n",
      "L: 0.0000 A: 0.9603 P: 0.8103 R: 0.4273 F: 0.5595 N: 0.0000\n",
      "E: [  1/25][  1/119] L: 0.3264 A: 0.9061 P: 0.8462 R: 0.6471 F: 0.7333 N: 0.0000\n",
      "E: [  1/25][ 16/119] L: 0.1462 A: 0.9468 P: 0.7368 R: 0.8750 F: 0.8000 N: 0.0000\n",
      "E: [  1/25][ 31/119] L: 0.1136 A: 0.9793 P: 0.7333 R: 0.6875 F: 0.7097 N: 0.0000\n",
      "E: [  1/25][ 46/119] L: 0.1533 A: 0.9489 P: 0.6154 R: 0.5000 F: 0.5517 N: 0.0000\n",
      "E: [  1/25][ 61/119] L: 0.1257 A: 0.9561 P: 0.6923 R: 0.5625 F: 0.6207 N: 0.0000\n",
      "E: [  1/25][ 76/119] L: 0.0441 A: 0.9986 P: 1.0000 R: 0.8333 F: 0.9091 N: 0.0000\n",
      "E: [  1/25][ 91/119] L: 0.0712 A: 0.9860 P: 0.8333 R: 0.6250 F: 0.7143 N: 0.0000\n",
      "E: [  1/25][106/119] L: 0.0689 A: 0.9748 P: 0.9231 R: 0.7500 F: 0.8276 N: 0.0000\n",
      "L: 0.0000 A: 0.9688 P: 0.7882 R: 0.6091 F: 0.6872 N: 0.0000\n",
      "E: [  2/25][  2/119] L: 0.2268 A: 0.9507 P: 0.8235 R: 0.8235 F: 0.8235 N: 0.0000\n",
      "E: [  2/25][ 17/119] L: 0.2824 A: 0.8934 P: 1.0000 R: 0.3889 F: 0.5600 N: 0.0000\n",
      "E: [  2/25][ 32/119] L: 0.0759 A: 0.9879 P: 0.9286 R: 0.8125 F: 0.8667 N: 0.0000\n",
      "E: [  2/25][ 47/119] L: 0.0702 A: 0.9902 P: 0.8571 R: 0.7500 F: 0.8000 N: 0.0000\n",
      "E: [  2/25][ 62/119] L: 0.0779 A: 0.9871 P: 0.8000 R: 0.7500 F: 0.7742 N: 0.0000\n",
      "E: [  2/25][ 77/119] L: 0.0214 A: 0.9990 P: 0.9412 R: 1.0000 F: 0.9697 N: 0.0000\n",
      "E: [  2/25][ 92/119] L: 0.0295 A: 1.0000 P: 1.0000 R: 0.9375 F: 0.9677 N: 0.0000\n",
      "E: [  2/25][107/119] L: 0.0489 A: 0.9926 P: 0.8000 R: 0.7500 F: 0.7742 N: 0.0000\n",
      "L: 0.0000 A: 0.9702 P: 0.7935 R: 0.6636 F: 0.7228 N: 0.0000\n",
      "E: [  3/25][  3/119] L: 0.1141 A: 0.9926 P: 0.8000 R: 0.9412 F: 0.8649 N: 0.0000\n",
      "E: [  3/25][ 18/119] L: 0.0989 A: 0.9744 P: 1.0000 R: 0.7500 F: 0.8571 N: 0.0000\n",
      "E: [  3/25][ 33/119] L: 0.2694 A: 0.9154 P: 0.6923 R: 0.4500 F: 0.5455 N: 0.0000\n",
      "E: [  3/25][ 48/119] L: 0.0809 A: 0.9732 P: 0.9231 R: 0.7059 F: 0.8000 N: 0.0000\n",
      "E: [  3/25][ 63/119] L: 0.1747 A: 0.9256 P: 0.5000 R: 0.3750 F: 0.4286 N: 0.0000\n",
      "E: [  3/25][ 78/119] L: 0.1194 A: 0.9682 P: 0.5789 R: 0.6875 F: 0.6286 N: 0.0000\n",
      "E: [  3/25][ 93/119] L: 0.1357 A: 0.9226 P: 0.6000 R: 0.3750 F: 0.4615 N: 0.0000\n",
      "E: [  3/25][108/119] L: 0.0531 A: 0.9883 P: 0.8000 R: 0.7059 F: 0.7500 N: 0.0000\n",
      "L: 0.0000 A: 0.9708 P: 0.8032 R: 0.6864 F: 0.7402 N: 0.0000\n",
      "E: [  4/25][  4/119] L: 0.0563 A: 0.9992 P: 0.8889 R: 1.0000 F: 0.9412 N: 0.0000\n",
      "E: [  4/25][ 19/119] L: 0.0527 A: 0.9965 P: 0.9286 R: 0.8125 F: 0.8667 N: 0.0000\n",
      "E: [  4/25][ 34/119] L: 0.1505 A: 0.9748 P: 0.7273 R: 0.4706 F: 0.5714 N: 0.0000\n",
      "E: [  4/25][ 49/119] L: 0.0613 A: 0.9948 P: 0.9231 R: 0.7500 F: 0.8276 N: 0.0000\n",
      "E: [  4/25][ 64/119] L: 0.0297 A: 0.9961 P: 1.0000 R: 0.9375 F: 0.9677 N: 0.0000\n",
      "E: [  4/25][ 79/119] L: 0.0987 A: 0.9714 P: 0.7333 R: 0.6875 F: 0.7097 N: 0.0000\n",
      "E: [  4/25][ 94/119] L: 0.0519 A: 0.9837 P: 0.8750 R: 0.8750 F: 0.8750 N: 0.0000\n",
      "E: [  4/25][109/119] L: 0.0342 A: 0.9966 P: 0.7857 R: 0.6875 F: 0.7333 N: 0.0000\n",
      "L: 0.0000 A: 0.9734 P: 0.8272 R: 0.7182 F: 0.7689 N: 0.0000\n",
      "E: [  5/25][  5/119] L: 0.1187 A: 0.9773 P: 0.9375 R: 0.8333 F: 0.8824 N: 0.0000\n",
      "E: [  5/25][ 20/119] L: 0.0975 A: 0.9748 P: 0.9375 R: 0.9375 F: 0.9375 N: 0.0000\n",
      "E: [  5/25][ 35/119] L: 0.1046 A: 0.9761 P: 1.0000 R: 0.5294 F: 0.6923 N: 0.0000\n",
      "E: [  5/25][ 50/119] L: 0.0868 A: 0.9717 P: 0.9286 R: 0.7647 F: 0.8387 N: 0.0000\n",
      "E: [  5/25][ 65/119] L: 0.0396 A: 0.9947 P: 0.9333 R: 0.8750 F: 0.9032 N: 0.0000\n",
      "E: [  5/25][ 80/119] L: 0.0562 A: 0.9930 P: 0.8824 R: 0.8824 F: 0.8824 N: 0.0000\n",
      "E: [  5/25][ 95/119] L: 0.0513 A: 0.9900 P: 0.9231 R: 0.7059 F: 0.8000 N: 0.0000\n",
      "E: [  5/25][110/119] L: 0.0477 A: 0.9914 P: 0.8462 R: 0.6875 F: 0.7586 N: 0.0000\n",
      "L: 0.0000 A: 0.9749 P: 0.7927 R: 0.6955 F: 0.7409 N: 0.0000\n",
      "E: [  6/25][  6/119] L: 0.1327 A: 0.9789 P: 0.8235 R: 0.8235 F: 0.8235 N: 0.0000\n",
      "E: [  6/25][ 21/119] L: 0.1040 A: 0.9800 P: 0.8571 R: 0.7500 F: 0.8000 N: 0.0000\n",
      "E: [  6/25][ 36/119] L: 0.0876 A: 0.9840 P: 1.0000 R: 0.7778 F: 0.8750 N: 0.0000\n",
      "E: [  6/25][ 51/119] L: 0.0364 A: 0.9966 P: 1.0000 R: 0.8824 F: 0.9375 N: 0.0000\n",
      "E: [  6/25][ 66/119] L: 0.0564 A: 0.9900 P: 0.8750 R: 0.8750 F: 0.8750 N: 0.0000\n",
      "E: [  6/25][ 81/119] L: 0.0482 A: 0.9929 P: 0.9231 R: 0.7500 F: 0.8276 N: 0.0000\n",
      "E: [  6/25][ 96/119] L: 0.0426 A: 0.9939 P: 0.8667 R: 0.8125 F: 0.8387 N: 0.0000\n",
      "E: [  6/25][111/119] L: 0.0422 A: 0.9953 P: 0.8667 R: 0.7647 F: 0.8125 N: 0.0000\n",
      "L: 0.0000 A: 0.9743 P: 0.7979 R: 0.6818 F: 0.7353 N: 0.0000\n",
      "E: [  7/25][  7/119] L: 0.0942 A: 0.9953 P: 0.9444 R: 0.9444 F: 0.9444 N: 0.0000\n",
      "E: [  7/25][ 22/119] L: 0.1414 A: 0.9774 P: 0.7647 R: 0.8125 F: 0.7879 N: 0.0000\n",
      "E: [  7/25][ 37/119] L: 0.0366 A: 0.9979 P: 0.9231 R: 0.7500 F: 0.8276 N: 0.0000\n",
      "E: [  7/25][ 52/119] L: 0.0127 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.0000\n",
      "E: [  7/25][ 67/119] L: 0.0358 A: 0.9990 P: 0.9333 R: 0.7778 F: 0.8485 N: 0.0000\n",
      "E: [  7/25][ 82/119] L: 0.0306 A: 0.9980 P: 0.8750 R: 0.8750 F: 0.8750 N: 0.0000\n",
      "E: [  7/25][ 97/119] L: 0.0197 A: 1.0000 P: 1.0000 R: 0.8824 F: 0.9375 N: 0.0000\n",
      "E: [  7/25][112/119] L: 0.0442 A: 0.9924 P: 0.8571 R: 0.7059 F: 0.7742 N: 0.0000\n",
      "L: 0.0000 A: 0.9725 P: 0.7901 R: 0.6500 F: 0.7132 N: 0.0000\n",
      "E: [  8/25][  8/119] L: 0.0436 A: 0.9981 P: 0.9500 R: 0.9500 F: 0.9500 N: 0.0000\n",
      "E: [  8/25][ 23/119] L: 0.0557 A: 0.9938 P: 1.0000 R: 0.8824 F: 0.9375 N: 0.0000\n",
      "E: [  8/25][ 38/119] L: 0.0906 A: 0.9751 P: 0.8667 R: 0.8125 F: 0.8387 N: 0.0000\n",
      "E: [  8/25][ 53/119] L: 0.0784 A: 0.9831 P: 0.8667 R: 0.8125 F: 0.8387 N: 0.0000\n",
      "E: [  8/25][ 68/119] L: 0.0359 A: 0.9995 P: 1.0000 R: 0.8235 F: 0.9032 N: 0.0000\n",
      "E: [  8/25][ 83/119] L: 0.0390 A: 0.9983 P: 0.9375 R: 0.9375 F: 0.9375 N: 0.0000\n",
      "E: [  8/25][ 98/119] L: 0.0599 A: 0.9826 P: 0.9167 R: 0.6875 F: 0.7857 N: 0.0000\n",
      "E: [  8/25][113/119] L: 0.0401 A: 0.9945 P: 0.8000 R: 0.7059 F: 0.7500 N: 0.0000\n",
      "L: 0.0000 A: 0.9654 P: 0.7778 R: 0.6682 F: 0.7188 N: 0.0000\n",
      "E: [  9/25][  9/119] L: 0.0431 A: 0.9978 P: 1.0000 R: 0.9375 F: 0.9677 N: 0.0000\n",
      "E: [  9/25][ 24/119] L: 0.0207 A: 0.9987 P: 1.0000 R: 0.9375 F: 0.9677 N: 0.0000\n",
      "E: [  9/25][ 39/119] L: 0.0119 A: 1.0000 P: 1.0000 R: 0.9375 F: 0.9677 N: 0.0000\n",
      "E: [  9/25][ 54/119] L: 0.0352 A: 0.9959 P: 1.0000 R: 0.8824 F: 0.9375 N: 0.0000\n",
      "E: [  9/25][ 69/119] L: 0.0367 A: 0.9967 P: 0.9231 R: 0.7500 F: 0.8276 N: 0.0000\n",
      "E: [  9/25][ 84/119] L: 0.0312 A: 0.9983 P: 0.8750 R: 0.8750 F: 0.8750 N: 0.0000\n",
      "E: [  9/25][ 99/119] L: 0.0372 A: 0.9967 P: 0.8333 R: 0.8333 F: 0.8333 N: 0.0000\n",
      "E: [  9/25][114/119] L: 0.0474 A: 0.9896 P: 0.8571 R: 0.7500 F: 0.8000 N: 0.0000\n",
      "L: 0.0000 A: 0.9590 P: 0.7024 R: 0.6545 F: 0.6776 N: 0.0000\n",
      "E: [ 10/25][ 10/119] L: 0.0084 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.0000\n",
      "E: [ 10/25][ 25/119] L: 0.0143 A: 1.0000 P: 1.0000 R: 0.9444 F: 0.9714 N: 0.0000\n",
      "E: [ 10/25][ 40/119] L: 0.0098 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.0000\n",
      "E: [ 10/25][ 55/119] L: 0.0110 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.0000\n",
      "E: [ 10/25][ 70/119] L: 0.0109 A: 1.0000 P: 0.9412 R: 1.0000 F: 0.9697 N: 0.0000\n",
      "E: [ 10/25][ 85/119] L: 0.0432 A: 0.9961 P: 0.8235 R: 0.8235 F: 0.8235 N: 0.0000\n",
      "E: [ 10/25][100/119] L: 0.0099 A: 1.0000 P: 1.0000 R: 0.9375 F: 0.9677 N: 0.0000\n",
      "E: [ 10/25][115/119] L: 0.0531 A: 0.9903 P: 0.7143 R: 0.6250 F: 0.6667 N: 0.0000\n",
      "L: 0.0000 A: 0.9658 P: 0.7528 R: 0.6091 F: 0.6734 N: 0.0000\n",
      "E: [ 11/25][ 11/119] L: 0.0082 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.0000\n",
      "E: [ 11/25][ 26/119] L: 0.0319 A: 0.9984 P: 1.0000 R: 0.9375 F: 0.9677 N: 0.0000\n",
      "E: [ 11/25][ 41/119] L: 0.0206 A: 0.9996 P: 0.9375 R: 0.9375 F: 0.9375 N: 0.0000\n",
      "E: [ 11/25][ 56/119] L: 0.0423 A: 0.9963 P: 0.9286 R: 0.7647 F: 0.8387 N: 0.0000\n",
      "E: [ 11/25][ 71/119] L: 0.0417 A: 0.9944 P: 0.9286 R: 0.8125 F: 0.8667 N: 0.0000\n",
      "E: [ 11/25][ 86/119] L: 0.0202 A: 0.9990 P: 0.8889 R: 1.0000 F: 0.9412 N: 0.0000\n",
      "E: [ 11/25][101/119] L: 0.0507 A: 0.9850 P: 0.9333 R: 0.8750 F: 0.9032 N: 0.0000\n",
      "E: [ 11/25][116/119] L: 0.0183 A: 0.9978 P: 1.0000 R: 0.8750 F: 0.9333 N: 0.0000\n",
      "L: 0.0000 A: 0.9636 P: 0.7259 R: 0.6500 F: 0.6859 N: 0.0000\n",
      "E: [ 12/25][ 12/119] L: 0.0130 A: 1.0000 P: 1.0000 R: 0.9412 F: 0.9697 N: 0.0000\n",
      "E: [ 12/25][ 27/119] L: 0.0470 A: 0.9974 P: 0.8824 R: 0.8824 F: 0.8824 N: 0.0000\n",
      "E: [ 12/25][ 42/119] L: 0.0138 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.0000\n",
      "E: [ 12/25][ 57/119] L: 0.0315 A: 0.9982 P: 0.9375 R: 0.9375 F: 0.9375 N: 0.0000\n",
      "E: [ 12/25][ 72/119] L: 0.0095 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.0000\n",
      "E: [ 12/25][ 87/119] L: 0.0441 A: 0.9947 P: 0.7647 R: 0.8125 F: 0.7879 N: 0.0000\n",
      "E: [ 12/25][102/119] L: 0.0147 A: 0.9994 P: 1.0000 R: 0.9375 F: 0.9677 N: 0.0000\n",
      "E: [ 12/25][117/119] L: 0.0188 A: 0.9982 P: 0.9286 R: 0.8125 F: 0.8667 N: 0.0000\n",
      "L: 0.0000 A: 0.9663 P: 0.8111 R: 0.6636 F: 0.7300 N: 0.0000\n",
      "E: [ 13/25][ 13/119] L: 0.0338 A: 0.9995 P: 0.9375 R: 0.9375 F: 0.9375 N: 0.0000\n",
      "E: [ 13/25][ 28/119] L: 0.0010 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.0000\n",
      "E: [ 13/25][ 43/119] L: 0.0679 A: 0.9896 P: 0.9375 R: 0.9375 F: 0.9375 N: 0.0000\n",
      "E: [ 13/25][ 58/119] L: 0.0020 A: 1.0000 P: 1.0000 R: 1.0000 F: 1.0000 N: 0.0000\n",
      "E: [ 13/25][ 73/119] L: 0.0210 A: 0.9990 P: 0.9375 R: 0.8824 F: 0.9091 N: 0.0000\n",
      "E: [ 13/25][ 88/119] L: 0.0050 A: 1.0000 P: 0.9412 R: 1.0000 F: 0.9697 N: 0.0000\n",
      "E: [ 13/25][103/119] L: 0.0386 A: 0.9758 P: 1.0000 R: 0.9412 F: 0.9697 N: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-f87d3deec955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mclauses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch2input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mclauses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclauses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mposes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch2target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_epoch = 25\n",
    "global_cnt = 0\n",
    "for epoch in range(max_epoch):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        clauses, keywords, poses = train_dataset.batch2input(batch)\n",
    "        clauses = torch.from_numpy(clauses).to(device)\n",
    "        poses = torch.from_numpy(poses).to(device)\n",
    "        targets = torch.from_numpy(train_dataset.batch2target(batch)).to(device)\n",
    "        \n",
    "        probs = model(clauses, poses)\n",
    "        \n",
    "        loss = criterion(probs.view(-1, num_classes), targets.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if global_cnt % 15 == 0:\n",
    "            roc_auc, pre, rec, f1, pre_ranking, rec_ranking, f_ranking = metrics(probs, targets)\n",
    "            print('E: [{:3}/{}][{:3}/{}] L: {:.4f} A: {:.4f} P: {:.4f} R: {:.4f} F: {:.4f} N: {:.4f}'.format(epoch, max_epoch, i, len(train_dataloader), loss.item(), roc_auc, pre, rec, f1, 0.))\n",
    "        global_cnt += 1\n",
    "    \n",
    "    evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-23T17:59:08.728384Z",
     "start_time": "2019-03-23T17:59:08.716175Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    clauses, keywords, poses = train_dataset.batch2input(batch)\n",
    "    targets = torch.from_numpy(train_dataset.batch2target(batch)).to(device)\n",
    "    masks = clauses == '<pad>'\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:38:05.090322Z",
     "start_time": "2019-03-19T10:37:47.966081Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 10:37:51,183 INFO: 19 batches, avg len: 43.0\n",
      "2019-03-19 10:38:03,075 INFO: Finished 1000 sentences.\n",
      "2019-03-19 10:38:04,674 INFO: 1 batches, avg len: 43.0\n"
     ]
    }
   ],
   "source": [
    "probs, word_attn = model(clauses, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:38:08.731869Z",
     "start_time": "2019-03-19T10:38:08.727190Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targets = torch.from_numpy(train_dataset.batch2target(batch)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:38:09.191542Z",
     "start_time": "2019-03-19T10:38:09.183819Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08774048835039139"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = criterion(probs.view(-1, num_classes), targets.view(-1))\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T10:38:51.020140Z",
     "start_time": "2019-03-19T10:38:50.997420Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:18:46.017853Z",
     "start_time": "2019-03-25T14:18:46.008665Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def ranking(probs, targets):\n",
    "    preds_rank = []\n",
    "    for prob, target in zip(probs, targets):\n",
    "        prob_rank = prob * np.vstack((np.zeros_like(target), target != -100)).T\n",
    "\n",
    "        mask = np.zeros_like(prob)\n",
    "        mask[prob_rank[:, 1].argmax(), 1] = 1\n",
    "\n",
    "        pred_rank = np.argmax(prob * mask, axis=1)\n",
    "        preds_rank.append(pred_rank)\n",
    "    return np.array(preds_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T18:54:28.746943Z",
     "start_time": "2019-03-18T18:54:28.736954Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:0.4043 P:0.3333 R:0.4444 F:0.3810\n",
      "Ranking  P:0.4043 R:0.3333 F:0.4444\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "mask = targets != -100\n",
    "p, r, f, _ = precision_recall_fscore_support(targets[mask], predicts[mask], average='binary')\n",
    "auc = roc_auc_score(targets[mask], probs[mask][:, 1])\n",
    "print('A:{:.4f} P:{:.4f} R:{:.4f} F:{:.4f}'.format(auc, p, r, f))\n",
    "predicts_rank = ranking(probs, targets)\n",
    "pr, rr, fr, _ = precision_recall_fscore_support(targets[mask], predicts_rank[mask], average='binary')\n",
    "print('Ranking  P:{:.4f} R:{:.4f} F:{:.4f}'.format(auc, p, r, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:18:47.066005Z",
     "start_time": "2019-03-25T14:18:46.775646Z"
    },
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "def metrics(probs, targets):\n",
    "    probs = np.array(F.softmax(probs, dim=-1).tolist())\n",
    "    targets = np.array(targets.tolist())\n",
    "    predicts = probs.argmax(-1)\n",
    "    mask = targets != -100\n",
    "    p, r, f, _ = precision_recall_fscore_support(targets[mask], predicts[mask], average='binary')\n",
    "    auc = roc_auc_score(targets[mask], probs[mask][:, 1])\n",
    "    predicts_rank = ranking(probs, targets)\n",
    "    pr, rr, fr, _ = precision_recall_fscore_support(targets[mask], predicts_rank[mask], average='binary')\n",
    "    \n",
    "    return auc, p, r, f, pr, rr, fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-18T18:58:11.547376Z",
     "start_time": "2019-03-18T18:58:11.540661Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roc_auc, pre, rec, f1, pre_ranking, rec_ranking, f_ranking = metrics(probs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:18:31.377286Z",
     "start_time": "2019-03-25T14:18:31.360894Z"
    },
    "collapsed": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses, all_probs, all_targets = [], [], []\n",
    "    for batch in eval_dataloader:\n",
    "        clauses, keywords, poses = eval_dataset.batch2input(batch)\n",
    "        targets = torch.from_numpy(eval_dataset.batch2target(batch)).to(device)\n",
    "        \n",
    "        probs, word_attn = model(clauses, keywords)\n",
    "        \n",
    "        loss = criterion(probs.view(-1, num_classes), targets.view(-1))\n",
    "        \n",
    "        all_probs.append(probs)\n",
    "        all_targets.append(targets)\n",
    "    roc_auc, pre, rec, f1, pre_ranking, rec_ranking, f_ranking = metrics(torch.cat(all_probs, dim=1), torch.cat(all_targets, dim=1))\n",
    "    print('L: {:.4f} A: {:.4f} P: {:.4f} R: {:.4f} F: {:.4f} N: {:.4f}'.format(sum(losses), roc_auc, pre, rec, f1, 0.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T14:18:42.098212Z",
     "start_time": "2019-03-25T14:18:42.082395Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses, all_probs, all_targets = [], [], []\n",
    "    for batch in eval_dataloader:\n",
    "        clauses, keywords, poses = eval_dataset.batch2input(batch)\n",
    "        clauses = torch.from_numpy(clauses).to(device)\n",
    "        poses = torch.from_numpy(poses).to(device)\n",
    "        targets = torch.from_numpy(eval_dataset.batch2target(batch)).to(device)\n",
    "        \n",
    "        probs = model(clauses, poses)\n",
    "        \n",
    "        loss = criterion(probs.view(-1, num_classes), targets.view(-1))\n",
    "        \n",
    "        all_probs.append(probs)\n",
    "        all_targets.append(targets)\n",
    "    roc_auc, pre, rec, f1, pre_ranking, rec_ranking, f_ranking = metrics(torch.cat(all_probs, dim=1), torch.cat(all_targets, dim=1))\n",
    "    print('L: {:.4f} A: {:.4f} P: {:.4f} R: {:.4f} F: {:.4f} N: {:.4f}'.format(sum(losses), roc_auc, pre, rec, f1, 0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T15:26:38.605073Z",
     "start_time": "2019-03-25T15:26:09.950069Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-25 15:26:10,747 INFO: char embedding size: 6169\n",
      "2019-03-25 15:26:11,149 INFO: word embedding size: 71222\n",
      "2019-03-25 15:26:37,353 INFO: Model(\n",
      "  (token_embedder): ConvTokenEmbedder(\n",
      "    (word_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(71222, 100, padding_idx=3)\n",
      "    )\n",
      "    (char_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(6169, 50, padding_idx=6166)\n",
      "    )\n",
      "    (convolutions): ModuleList(\n",
      "      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n",
      "      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n",
      "      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n",
      "      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n",
      "      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
      "      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n",
      "      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n",
      "    )\n",
      "    (highways): Highway(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "        (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (projection): Linear(in_features=2148, out_features=512, bias=True)\n",
      "  )\n",
      "  (encoder): ElmobiLm(\n",
      "    (forward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (forward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from elmoformanylangs import Embedder\n",
    "e = Embedder('/data/wujipeng/embedding/ELMo/zhs.model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T15:37:53.249316Z",
     "start_time": "2019-03-25T15:37:53.244288Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T15:26:40.213944Z",
     "start_time": "2019-03-25T15:26:40.180240Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/data/wujipeng/ec/data/han/elmo_processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T15:26:42.277167Z",
     "start_time": "2019-03-25T15:26:42.225929Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>clause</th>\n",
       "      <th>nature</th>\n",
       "      <th>keyword</th>\n",
       "      <th>emotion</th>\n",
       "      <th>clause_pos</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>当 我 看到 建议 被 采纳 \u0001 部委 领导 写 给 我 的 回信 时 \u0001 我 知道 我 正...</td>\n",
       "      <td>p r v n p v \u0001 j n v v r u n n \u0001 r v r d p r n ...</td>\n",
       "      <td>激动</td>\n",
       "      <td>0</td>\n",
       "      <td>63 64 65 66 67 68 69 70 71</td>\n",
       "      <td>0 0 0 0 0 0 0 0 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>据 白金跃 介绍 \u0001 自 1988年 至今 \u0001 他 向 国家 各 部委 提出 合理化 建议 ...</td>\n",
       "      <td>p n v \u0001 p n d \u0001 r p n r j v v v m m q \u0001 c m p ...</td>\n",
       "      <td>激动</td>\n",
       "      <td>0</td>\n",
       "      <td>61 62 63 64 65 66 67 68 69</td>\n",
       "      <td>0 0 0 0 0 0 0 1 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2002年 6月 3日 上午 \u0001 当值 的 曾友蔚 接报 \u0001 狮山镇 小塘 走马营村 一 树...</td>\n",
       "      <td>n n n n \u0001 n u n v \u0001 n n n m n n v m q n d m q ...</td>\n",
       "      <td>心疼</td>\n",
       "      <td>3</td>\n",
       "      <td>58 59 60 61 62 63 64 65 66 67 68 69 70 71 72</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 1 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>为 尽快 将 女子 救 下 \u0001 指挥员 立即 制订 了 救援 方案 \u0001 第一 组 在 楼下 ...</td>\n",
       "      <td>p d p n v v \u0001 n d v u v n \u0001 m q p n v v n \u0001 c ...</td>\n",
       "      <td>无奈</td>\n",
       "      <td>3</td>\n",
       "      <td>58 59 60 61 62 63 64 65 66 67 68 69</td>\n",
       "      <td>0 0 0 0 0 0 0 0 1 1 1 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>依靠 父母 支持 和 在 学校 打工 积攒 下来 的 钱 \u0001 两 人 共 投资 50 多 万...</td>\n",
       "      <td>v n v c p n v v v u n \u0001 m n d v m m m q \u0001 v v ...</td>\n",
       "      <td>自豪</td>\n",
       "      <td>0</td>\n",
       "      <td>63 64 65 66 67 68 69</td>\n",
       "      <td>0 0 0 0 0 1 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>2013年 6月 \u0001 在 深圳 打拼 10 年 的 吴树梁 终于 拿 到 大红 的 深圳市 ...</td>\n",
       "      <td>n n \u0001 p n v m q u n d v v b u n n \u0001 n n d d v ...</td>\n",
       "      <td>欣喜</td>\n",
       "      <td>0</td>\n",
       "      <td>64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "      <td>0 0 0 0 0 1 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>2013年 6月 \u0001 在 深圳 打拼 10 年 的 吴树梁 终于 拿 到 大红 的 深圳市 ...</td>\n",
       "      <td>n n \u0001 p n v m q u n d v v b u n n \u0001 n n d d v ...</td>\n",
       "      <td>忧虑</td>\n",
       "      <td>3</td>\n",
       "      <td>61 62 63 64 65 66 67 68 69 70 71 72</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>生 不 如 死 是 老吴 患癌 后 常 有的 想法 \u0001 由于 大面 积骨 转移 \u0001 老吴 每...</td>\n",
       "      <td>v d v v v n v n d r n \u0001 c n n v \u0001 n r d p n n ...</td>\n",
       "      <td>生 不 如 死</td>\n",
       "      <td>5</td>\n",
       "      <td>69 70 71 72 73 74 75 76</td>\n",
       "      <td>0 1 1 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>生 不 如 死 是 老吴 患癌 后 常 有的 想法 \u0001 由于 大面 积骨 转移 \u0001 老吴 每...</td>\n",
       "      <td>v d v v v n v n d r n \u0001 c n n v \u0001 n r d p n n ...</td>\n",
       "      <td>惊诧</td>\n",
       "      <td>6</td>\n",
       "      <td>62 63 64 65 66 67 68 69</td>\n",
       "      <td>0 0 0 0 1 0 1 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>为了 坚持 到 妻子 入 户 \u0001 老吴 已经 用尽 了 全力 \u0001 老吴 因为 其 抗癌 事迹...</td>\n",
       "      <td>p v v n v n \u0001 n d v u n \u0001 n p r v n i \u0001 c n a ...</td>\n",
       "      <td>担心</td>\n",
       "      <td>4</td>\n",
       "      <td>65 66 67 68 69 70 71 72 73 74 75 76</td>\n",
       "      <td>0 0 0 1 0 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>目前 全家 依靠 她 两千 多 元 工资 生活 \u0001 看病 已 负债累累 \u0001 如果 我 不 在...</td>\n",
       "      <td>n n v r m m q n v \u0001 v d i \u0001 c r d v u \u0001 n v n ...</td>\n",
       "      <td>无奈</td>\n",
       "      <td>3</td>\n",
       "      <td>65 66 67 68 69 70 71 72 73</td>\n",
       "      <td>0 1 0 1 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>目前 全家 依靠 她 两千 多 元 工资 生活 \u0001 看病 已 负债累累 \u0001 如果 我 不 在...</td>\n",
       "      <td>n n v r m m q n v \u0001 v d i \u0001 c r d v u \u0001 n v n ...</td>\n",
       "      <td>感谢</td>\n",
       "      <td>1</td>\n",
       "      <td>63 64 65 66 67 68 69 70 71 72</td>\n",
       "      <td>0 0 0 0 0 0 1 1 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>4月 14日 下午 \u0001 有 微博 网友 爆料 称 \u0001 通州 台湖 某 制药厂 内 悬挂 着 ...</td>\n",
       "      <td>n n n \u0001 v j n v v \u0001 n n r n n v u m q v n \u0001 c ...</td>\n",
       "      <td>没有 人性</td>\n",
       "      <td>5</td>\n",
       "      <td>64 65 66 67 68 69 70</td>\n",
       "      <td>0 0 0 0 1 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>老公 站 在 楼栋 底下 同 站 在 6 楼 的 妻子 隔空 拌嘴 \u0001 妻子 一时 激动 竟...</td>\n",
       "      <td>n v p n n d v p m n u n v v \u0001 n n a d p n v v ...</td>\n",
       "      <td>激动</td>\n",
       "      <td>2</td>\n",
       "      <td>68 69 70 71 72 73 74</td>\n",
       "      <td>1 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>老公 站 在 楼栋 底下 同 站 在 6 楼 的 妻子 隔空 拌嘴 \u0001 妻子 一时 激动 竟...</td>\n",
       "      <td>n v p n n d v p m n u n v v \u0001 n n a d p n v v ...</td>\n",
       "      <td>感动</td>\n",
       "      <td>0</td>\n",
       "      <td>64 65 66 67 68 69 70</td>\n",
       "      <td>0 0 0 0 0 1 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>同月 19日 深夜 11时 \u0001 韩 男 去 电 给 小雯 约 在 一 处 便利店 前 碰面 ...</td>\n",
       "      <td>d n n n \u0001 j b v n p n d p m q n n v \u0001 n v n v ...</td>\n",
       "      <td>诧异</td>\n",
       "      <td>6</td>\n",
       "      <td>65 66 67 68 69 70 71 72 73 74</td>\n",
       "      <td>0 0 1 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>五 年 了 \u0001 孩子 一 滴 尿 也 没 排 过 \u0001 每天 身上 发痒 疼痛 胸口 憋 闷 ...</td>\n",
       "      <td>m q u \u0001 n m q n d d v u \u0001 r n v a n v a v d v ...</td>\n",
       "      <td>心疼</td>\n",
       "      <td>3</td>\n",
       "      <td>63 64 65 66 67 68 69 70 71 72 73</td>\n",
       "      <td>0 0 0 0 0 1 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>直到 现在 \u0001 张治英 还 因为 给 女儿 买 不 起 大碗面 自责 不已</td>\n",
       "      <td>v n \u0001 n d c p n v d v n v v</td>\n",
       "      <td>自责</td>\n",
       "      <td>5</td>\n",
       "      <td>68 69</td>\n",
       "      <td>0 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>前段 时间 \u0001 在 北京大学 人民 医院 治疗 时 的 大夫 打 来 电话 询问 了 琼元 ...</td>\n",
       "      <td>n n \u0001 p n n n v n u n v v n v u n u n n \u0001 v n ...</td>\n",
       "      <td>高兴 不 起来</td>\n",
       "      <td>3</td>\n",
       "      <td>64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "      <td>0 0 0 0 0 0 1 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>瞒 不 住 了 \u0001 张治英 问 她 想 不 想 看望 亲生 父母 \u0001 张琼元 哭 着 说 我...</td>\n",
       "      <td>v d v u \u0001 n v r v d v v b n \u0001 n v u v r v r u ...</td>\n",
       "      <td>泣不成声</td>\n",
       "      <td>3</td>\n",
       "      <td>63 64 65 66 67 68 69</td>\n",
       "      <td>0 0 0 0 0 1 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>民警 将 老人 送 到 南钢 厂区 \u0001 问 了 半天 \u0001 陈 大爷 也 没有 说 清楚 他家...</td>\n",
       "      <td>n p n v v n n \u0001 v u m \u0001 n n d d v a r d v r v ...</td>\n",
       "      <td>笑 了</td>\n",
       "      <td>0</td>\n",
       "      <td>62 63 64 65 66 67 68 69 70 71 72</td>\n",
       "      <td>0 0 0 0 0 0 1 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>对于 鲁 女士 遇见 老人 跌倒 立刻 出手 相 助 \u0001 还 陪 着 老人 一起 等 民警 ...</td>\n",
       "      <td>p n n v n v d v d v \u0001 d v u n d v n u n \u0001 n v ...</td>\n",
       "      <td>赞扬</td>\n",
       "      <td>1</td>\n",
       "      <td>67 68 69 70 71 72 73</td>\n",
       "      <td>1 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>在 老一辈 村民 的 记忆 里 \u0001 谢德友镇 定自若 为 溺亡 村民 黄某 入殓 的 场景 ...</td>\n",
       "      <td>p n n u n n \u0001 n v v v n n v u n \u0001 v m d i \u0001 v n v</td>\n",
       "      <td>尊敬</td>\n",
       "      <td>1</td>\n",
       "      <td>66 67 68 69</td>\n",
       "      <td>0 1 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>眼前 厚厚 几 沓 病历 相册 证件 \u0001 白纸黑字 如此 刺眼 绝望 \u0001 才 敢 相信 她 ...</td>\n",
       "      <td>n z m q n n n \u0001 n r a a \u0001 d v v r a v u n n \u0001 ...</td>\n",
       "      <td>绝望</td>\n",
       "      <td>3</td>\n",
       "      <td>62 63 64 65 66 67 68 69</td>\n",
       "      <td>0 0 0 0 0 0 0 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>李芳 抹 着 眼泪 告诉 小溪 \u0001 2002年 \u0001 她 的 儿子 逝去 了 \u0001 儿子 才 1...</td>\n",
       "      <td>n v u n v n \u0001 n \u0001 r u n v u \u0001 n d m q \u0001 a a \u0001 ...</td>\n",
       "      <td>痛不欲生</td>\n",
       "      <td>3</td>\n",
       "      <td>61 62 63 64 65 66 67 68 69 70</td>\n",
       "      <td>0 0 0 0 0 0 1 1 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>上上周 \u0001 安德烈 突然 找到 我 \u0001 说 想 要 在 医院 举行 婚礼 \u0001 我 先是 吓 ...</td>\n",
       "      <td>n \u0001 n a v r \u0001 v v v p n v n \u0001 r d v u m v \u0001 d ...</td>\n",
       "      <td>感动</td>\n",
       "      <td>0</td>\n",
       "      <td>65 66 67 68 69 70 71 72 73 74 75 76 77 78</td>\n",
       "      <td>0 0 1 0 0 0 0 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>1952年 夏 \u0001 她 和 另 7 名 日籍 护士 \u0001 从 广西 南宁 军区 303 医院 ...</td>\n",
       "      <td>n n \u0001 r c r m q n n \u0001 p n n n m n v v n n n n ...</td>\n",
       "      <td>着急</td>\n",
       "      <td>4</td>\n",
       "      <td>56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 7...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>不仅 是 对 尹家 的 3 个 孩子 \u0001 在 街坊 邻居 中 \u0001 张崇贞 也 有着 他人 不...</td>\n",
       "      <td>c v p n u m q n \u0001 p n n n \u0001 n d v r d v v u a ...</td>\n",
       "      <td>敬重</td>\n",
       "      <td>1</td>\n",
       "      <td>63 64 65 66 67 68 69 70 71 72 73 74</td>\n",
       "      <td>0 0 0 0 0 0 1 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>2010 年 \u0001 母亲 去世 \u0001 尹家 的 3 个 孩子 将 母亲 和 奶奶 正式 合葬 \u0001...</td>\n",
       "      <td>m q \u0001 n v \u0001 n u m q n p n c n a v \u0001 n n \u0001 v u ...</td>\n",
       "      <td>欣慰</td>\n",
       "      <td>0</td>\n",
       "      <td>52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 6...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>朱洪生 今年 55 岁 \u0001 1979年 参加 工作 时 才 19 岁 \u0001 已 有 36 年 ...</td>\n",
       "      <td>n n m q \u0001 n v n n d m q \u0001 d v m q u n \u0001 r n p ...</td>\n",
       "      <td>自豪</td>\n",
       "      <td>0</td>\n",
       "      <td>62 63 64 65 66 67 68 69</td>\n",
       "      <td>0 0 0 0 0 0 1 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>2076</td>\n",
       "      <td>公安部 将 此案 定 为 海燕 3 号 专案 \u0001 由 广东省 挂牌 督办 \u0001 2014年 5...</td>\n",
       "      <td>n p r v v n m q n \u0001 p n v v \u0001 n n p n \u0001 r v n ...</td>\n",
       "      <td>吃惊</td>\n",
       "      <td>6</td>\n",
       "      <td>65 66 67 68 69 70 71 72 73 74</td>\n",
       "      <td>0 0 0 0 0 1 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>2077</td>\n",
       "      <td>涉嫌 故意 杀人 被 诉 同年 10月 17日 \u0001 贾某 因 涉嫌 故意 杀人罪 被 公安 ...</td>\n",
       "      <td>v d v p v d n n \u0001 n p v d v p n n b v \u0001 n n p ...</td>\n",
       "      <td>气愤 至极</td>\n",
       "      <td>2</td>\n",
       "      <td>59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 7...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2077</th>\n",
       "      <td>2078</td>\n",
       "      <td>这 辆 越野车 够 野逃 了 3 公里 被 抓住 4月 15日 下午 \u0001 一 辆 越野车 因...</td>\n",
       "      <td>r q n v a u m q p v n n n \u0001 m q n p v n c r m ...</td>\n",
       "      <td>心虚</td>\n",
       "      <td>4</td>\n",
       "      <td>66 67 68 69 70 71 72 73 74</td>\n",
       "      <td>0 0 1 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>2079</td>\n",
       "      <td>去年 10月 \u0001 浙江 永康 27 岁 姑娘 黄燕 被 确诊 患 上 急性 淋巴细胞 白血病...</td>\n",
       "      <td>n n \u0001 n n m q n n p v v v b n n \u0001 n m m q i \u0001 ...</td>\n",
       "      <td>感动</td>\n",
       "      <td>1</td>\n",
       "      <td>65 66 67 68 69 70 71 72</td>\n",
       "      <td>0 0 0 0 1 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2079</th>\n",
       "      <td>2080</td>\n",
       "      <td>雨梅 他 平时 用 电脑 上网 总是 背着 我 \u0001 7 年 前 有 一 次 我 出门 忘带 ...</td>\n",
       "      <td>n r n p n v d v r \u0001 m q n v m q r v v n v v v ...</td>\n",
       "      <td>恐怖</td>\n",
       "      <td>4</td>\n",
       "      <td>63 64 65 66 67 68 69 70 71 72 73 74 75 76</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 1 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>2081</td>\n",
       "      <td>原来 \u0001 贺某 一直 随 父 居住 在 大足 城区 \u0001 由于 父母 离异 \u0001 贺某 父亲 待...</td>\n",
       "      <td>d \u0001 n d v n v p n n \u0001 p n v \u0001 n n v n p v i \u0001 ...</td>\n",
       "      <td>愤然</td>\n",
       "      <td>2</td>\n",
       "      <td>61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 7...</td>\n",
       "      <td>0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081</th>\n",
       "      <td>2082</td>\n",
       "      <td>出 家 后 的 林后 领 本 应 洗心革面 \u0001 然而 来自 老家 的 一 则 喜讯 却 再次...</td>\n",
       "      <td>v n n u n v d v n \u0001 c v n u m q n d d v u r n ...</td>\n",
       "      <td>羡慕</td>\n",
       "      <td>1</td>\n",
       "      <td>65 66 67 68 69 70 71 72 73 74</td>\n",
       "      <td>0 0 0 1 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082</th>\n",
       "      <td>2083</td>\n",
       "      <td>经 审查 \u0001 嫌疑人 方某 \u0001 男 \u0001 25 岁 \u0001 湖南 岳阳人 \u0001 据 其 交代 \u0001 2...</td>\n",
       "      <td>p v \u0001 n n \u0001 b \u0001 m q \u0001 n n \u0001 p r v \u0001 n n n \u0001 c ...</td>\n",
       "      <td>郁闷</td>\n",
       "      <td>5</td>\n",
       "      <td>61 62 63 64 65 66 67 68 69 70 71 72 73</td>\n",
       "      <td>0 0 0 0 0 0 0 1 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>2084</td>\n",
       "      <td>为 救 老伴 \u0001 63 岁 的 老齐不顾 众人 劝阻 \u0001 两 次 从 隔壁 人家 翻 墙上 ...</td>\n",
       "      <td>p v n \u0001 m q u n n v \u0001 m q p n r v n n v n u n ...</td>\n",
       "      <td>心有余悸</td>\n",
       "      <td>4</td>\n",
       "      <td>62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "      <td>0 0 0 0 0 0 1 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>2085</td>\n",
       "      <td>两 度 入 火海 救 老伴 逃 出来 的 老齐 看 着 被 火光 与 浓烟 笼罩 着 的 三...</td>\n",
       "      <td>m q v n v n v v u n v u p n c n v u u m q a n ...</td>\n",
       "      <td>心焦</td>\n",
       "      <td>4</td>\n",
       "      <td>64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79</td>\n",
       "      <td>0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>2086</td>\n",
       "      <td>然而 \u0001 就 是 这么 简单 的 一 句 回话 \u0001 让 李某 认为 就是 丈母娘 一 家 将...</td>\n",
       "      <td>c \u0001 d v r a u m q n \u0001 v n v v n m q p n v u v ...</td>\n",
       "      <td>窝火</td>\n",
       "      <td>2</td>\n",
       "      <td>59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2086</th>\n",
       "      <td>2087</td>\n",
       "      <td>康某 逐渐 意识 到 情况 不妙 \u0001 悄悄 跑 到 买 的 房子 里 一 探 究竟 \u0001 发现...</td>\n",
       "      <td>n d v v n a \u0001 d v v v u n n m v d \u0001 v n d d v ...</td>\n",
       "      <td>惊讶</td>\n",
       "      <td>6</td>\n",
       "      <td>63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 7...</td>\n",
       "      <td>0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2087</th>\n",
       "      <td>2088</td>\n",
       "      <td>11日 下午 \u0001 翁某 在 横峰 一 家 小鞋厂 做 了 两 天 半 的 工 \u0001 不 想 再...</td>\n",
       "      <td>n n \u0001 n p n m q n v u m q m u n \u0001 d v d v v u ...</td>\n",
       "      <td>害怕</td>\n",
       "      <td>4</td>\n",
       "      <td>57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 7...</td>\n",
       "      <td>0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>2089</td>\n",
       "      <td>她家 住 海曙 翠柏东巷 \u0001 儿子 今年 4 岁 \u0001 4月 4日 下午 1点 多 \u0001 她 带...</td>\n",
       "      <td>r v n n \u0001 n n m q \u0001 n n n n m \u0001 r v u n v v u ...</td>\n",
       "      <td>兴奋</td>\n",
       "      <td>0</td>\n",
       "      <td>64 65 66 67 68 69 70 71 72 73 74 75 76</td>\n",
       "      <td>0 0 0 1 0 0 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2089</th>\n",
       "      <td>2090</td>\n",
       "      <td>广东 清远 法院 14日 通报 \u0001 该 名酒店 保安 日前 被 法院 以 故意 伤害 罪 判...</td>\n",
       "      <td>n n n n v \u0001 r n n n p n p d v n v m q m q n \u0001 ...</td>\n",
       "      <td>气 不 过</td>\n",
       "      <td>2</td>\n",
       "      <td>63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78</td>\n",
       "      <td>0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>2091</td>\n",
       "      <td>楚天 都市 报 记者 朱蕾 @郭小貓 777 就 是 这个 人 \u0001 公车 上 偷偷 摸 我 ...</td>\n",
       "      <td>n n n n n n m d v r n \u0001 n n d v r \u0001 n v n n p ...</td>\n",
       "      <td>恼火</td>\n",
       "      <td>2</td>\n",
       "      <td>66 67 68 69 70 71 72 73 74 75 76 77</td>\n",
       "      <td>0 0 1 0 0 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2091</th>\n",
       "      <td>2092</td>\n",
       "      <td>校方 为 保护 学生 才 打 狗 昨日 下午 \u0001 记者 联系 了 通州 高级 中学 进行 核...</td>\n",
       "      <td>n p v n d v n n n \u0001 n v u n a n v v \u0001 p r \u0001 r ...</td>\n",
       "      <td>惊吓</td>\n",
       "      <td>4</td>\n",
       "      <td>63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78</td>\n",
       "      <td>0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092</th>\n",
       "      <td>2093</td>\n",
       "      <td>40 分钟 后 两 人 被 救 出 我 这 心 吓 得 扑通 扑通 直跳 \u0001 孙女 吓 得 ...</td>\n",
       "      <td>m q n m n p v v r r n v u o o v \u0001 n v u d v \u0001 ...</td>\n",
       "      <td>担惊受怕</td>\n",
       "      <td>4</td>\n",
       "      <td>57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 7...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2093</th>\n",
       "      <td>2094</td>\n",
       "      <td>目前 \u0001 安内格雷特 已 怀孕 21 周 \u0001 卢森堡 广播 电视台 称 她 无 任何 并发症...</td>\n",
       "      <td>n \u0001 n d v m q \u0001 n v n v r v r n \u0001 n v a \u0001 c r ...</td>\n",
       "      <td>震惊</td>\n",
       "      <td>6</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74</td>\n",
       "      <td>0 0 0 0 0 0 0 0 1 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2094</th>\n",
       "      <td>2095</td>\n",
       "      <td>据 张 女士 介绍 \u0001 女儿 在 班里 成绩 并 不 差 \u0001 能够 稳定 在 前 15 名 ...</td>\n",
       "      <td>p n n v \u0001 n p n n d d v \u0001 v v p n m q \u0001 n n d ...</td>\n",
       "      <td>心慌</td>\n",
       "      <td>4</td>\n",
       "      <td>58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 7...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2095</th>\n",
       "      <td>2096</td>\n",
       "      <td>报道 \u0001 基隆市 殡葬 管理所 刚 启用 运 棺木 电梯 昨天 12日 突卡 住 \u0001 27 ...</td>\n",
       "      <td>n \u0001 n v n d v v n n n n n v \u0001 m q n v m q \u0001 v ...</td>\n",
       "      <td>饱受 惊吓</td>\n",
       "      <td>4</td>\n",
       "      <td>66 67 68 69 70 71 72 73 74 75 76 77</td>\n",
       "      <td>0 0 1 0 0 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2096</th>\n",
       "      <td>2097</td>\n",
       "      <td>一度 因 等 不 到 家属 \u0001 棺木 无法 送 火化 \u0001 直到 消防 人员 赶到 撬 开 电...</td>\n",
       "      <td>d p v d v n \u0001 n v v v \u0001 v b n v v v n \u0001 d p n ...</td>\n",
       "      <td>人心惶惶</td>\n",
       "      <td>4</td>\n",
       "      <td>61 62 63 64 65 66 67 68 69 70 71 72</td>\n",
       "      <td>0 0 0 0 0 0 0 1 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2097</th>\n",
       "      <td>2098</td>\n",
       "      <td>陈玉梅 说 \u0001 约斯 夫妇 向 她 发 来 了 电子 邮件 \u0001 大意 是 希望 她 能 帮助...</td>\n",
       "      <td>n v \u0001 n n p r v v u n n \u0001 n v v r v v a n \u0001 v ...</td>\n",
       "      <td>内疚</td>\n",
       "      <td>3</td>\n",
       "      <td>63 64 65 66 67 68 69 70 71 72 73 74</td>\n",
       "      <td>0 0 0 0 0 0 1 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>2099</td>\n",
       "      <td>他 称 \u0001 学 医 的 梦想 要 实现 了 \u0001 我 肯定 要 尝试 \u0001 对于 淦菊保 的 决...</td>\n",
       "      <td>r v \u0001 v n u n v v u \u0001 r d v v \u0001 p j u n \u0001 n c ...</td>\n",
       "      <td>佩服</td>\n",
       "      <td>1</td>\n",
       "      <td>59 60 61 62 63 64 65 66 67 68 69 70 71 72 73</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 1 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2099</th>\n",
       "      <td>2100</td>\n",
       "      <td>在 五洲 装饰城 旁边 的 紫金东路 一侧 \u0001 停 着 10 多 辆 蓝色 货车 \u0001 大多 ...</td>\n",
       "      <td>p j n n u n n \u0001 v u m m q n n \u0001 d v n p v v \u0001 ...</td>\n",
       "      <td>愤怒</td>\n",
       "      <td>2</td>\n",
       "      <td>50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 6...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2100</th>\n",
       "      <td>2101</td>\n",
       "      <td>被害人 李 女士 与 赵某 交往 过程 中 \u0001 应 赵某 要求 \u0001 通过 微信 将 自己 的...</td>\n",
       "      <td>n n n p n v n n \u0001 v n v \u0001 p n p r u m n v n c ...</td>\n",
       "      <td>愤恨</td>\n",
       "      <td>2</td>\n",
       "      <td>64 65 66 67 68 69 70 71 72 73 74 75 76</td>\n",
       "      <td>0 0 0 0 0 1 0 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>2102</td>\n",
       "      <td>空地 与 路 之间 的 路牙 埂子 挡住 右 前轮 \u0001 车轮 过 不 了 路 牙埂子 \u0001 于...</td>\n",
       "      <td>n c n n u n n v n n \u0001 n u d v n n \u0001 c r d v u ...</td>\n",
       "      <td>心慌</td>\n",
       "      <td>4</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74</td>\n",
       "      <td>0 0 0 0 0 0 0 0 1 0 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>2103</td>\n",
       "      <td>据 安徽 卫视 公共 频道 报道 \u0001 4月 6日 \u0001 定远县 总医院 住院部 发生 了 一 ...</td>\n",
       "      <td>p n j b n n \u0001 n n \u0001 n n n v u m q n \u0001 m q n v ...</td>\n",
       "      <td>气愤</td>\n",
       "      <td>2</td>\n",
       "      <td>59 60 61 62 63 64 65 66 67 68 69</td>\n",
       "      <td>0 0 0 0 0 0 0 0 1 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2103</th>\n",
       "      <td>2104</td>\n",
       "      <td>我们 夫妻 吵架 \u0001 你们 来 干 什么 \u0001 朱 某某 质问道 \u0001 小 隐嚣张 地 说 她 ...</td>\n",
       "      <td>r n v \u0001 r v v r \u0001 n r n \u0001 a a u v r d r n n u ...</td>\n",
       "      <td>怒火 中 烧</td>\n",
       "      <td>2</td>\n",
       "      <td>62 63 64 65 66 67 68 69 70 71 72</td>\n",
       "      <td>0 0 0 0 1 1 1 0 0 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2104</th>\n",
       "      <td>2105</td>\n",
       "      <td>自己 经常 请 张 迎春 喝酒 看 电影 \u0001 一 次 聚会 酒后 在 消费单 上 签 上 了...</td>\n",
       "      <td>r d v n v v v n \u0001 m q v n p n n v v u r u n \u0001 ...</td>\n",
       "      <td>怨恨</td>\n",
       "      <td>5</td>\n",
       "      <td>64 65 66 67 68 69 70 71 72 73</td>\n",
       "      <td>0 0 0 0 1 0 0 0 0 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2105 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             clause  \\\n",
       "0        1  当 我 看到 建议 被 采纳 \u0001 部委 领导 写 给 我 的 回信 时 \u0001 我 知道 我 正...   \n",
       "1        2  据 白金跃 介绍 \u0001 自 1988年 至今 \u0001 他 向 国家 各 部委 提出 合理化 建议 ...   \n",
       "2        3  2002年 6月 3日 上午 \u0001 当值 的 曾友蔚 接报 \u0001 狮山镇 小塘 走马营村 一 树...   \n",
       "3        4  为 尽快 将 女子 救 下 \u0001 指挥员 立即 制订 了 救援 方案 \u0001 第一 组 在 楼下 ...   \n",
       "4        5  依靠 父母 支持 和 在 学校 打工 积攒 下来 的 钱 \u0001 两 人 共 投资 50 多 万...   \n",
       "5        6  2013年 6月 \u0001 在 深圳 打拼 10 年 的 吴树梁 终于 拿 到 大红 的 深圳市 ...   \n",
       "6        7  2013年 6月 \u0001 在 深圳 打拼 10 年 的 吴树梁 终于 拿 到 大红 的 深圳市 ...   \n",
       "7        8  生 不 如 死 是 老吴 患癌 后 常 有的 想法 \u0001 由于 大面 积骨 转移 \u0001 老吴 每...   \n",
       "8        9  生 不 如 死 是 老吴 患癌 后 常 有的 想法 \u0001 由于 大面 积骨 转移 \u0001 老吴 每...   \n",
       "9       10  为了 坚持 到 妻子 入 户 \u0001 老吴 已经 用尽 了 全力 \u0001 老吴 因为 其 抗癌 事迹...   \n",
       "10      11  目前 全家 依靠 她 两千 多 元 工资 生活 \u0001 看病 已 负债累累 \u0001 如果 我 不 在...   \n",
       "11      12  目前 全家 依靠 她 两千 多 元 工资 生活 \u0001 看病 已 负债累累 \u0001 如果 我 不 在...   \n",
       "12      13  4月 14日 下午 \u0001 有 微博 网友 爆料 称 \u0001 通州 台湖 某 制药厂 内 悬挂 着 ...   \n",
       "13      14  老公 站 在 楼栋 底下 同 站 在 6 楼 的 妻子 隔空 拌嘴 \u0001 妻子 一时 激动 竟...   \n",
       "14      15  老公 站 在 楼栋 底下 同 站 在 6 楼 的 妻子 隔空 拌嘴 \u0001 妻子 一时 激动 竟...   \n",
       "15      16  同月 19日 深夜 11时 \u0001 韩 男 去 电 给 小雯 约 在 一 处 便利店 前 碰面 ...   \n",
       "16      17  五 年 了 \u0001 孩子 一 滴 尿 也 没 排 过 \u0001 每天 身上 发痒 疼痛 胸口 憋 闷 ...   \n",
       "17      18              直到 现在 \u0001 张治英 还 因为 给 女儿 买 不 起 大碗面 自责 不已   \n",
       "18      19  前段 时间 \u0001 在 北京大学 人民 医院 治疗 时 的 大夫 打 来 电话 询问 了 琼元 ...   \n",
       "19      20  瞒 不 住 了 \u0001 张治英 问 她 想 不 想 看望 亲生 父母 \u0001 张琼元 哭 着 说 我...   \n",
       "20      21  民警 将 老人 送 到 南钢 厂区 \u0001 问 了 半天 \u0001 陈 大爷 也 没有 说 清楚 他家...   \n",
       "21      22  对于 鲁 女士 遇见 老人 跌倒 立刻 出手 相 助 \u0001 还 陪 着 老人 一起 等 民警 ...   \n",
       "22      23  在 老一辈 村民 的 记忆 里 \u0001 谢德友镇 定自若 为 溺亡 村民 黄某 入殓 的 场景 ...   \n",
       "23      24  眼前 厚厚 几 沓 病历 相册 证件 \u0001 白纸黑字 如此 刺眼 绝望 \u0001 才 敢 相信 她 ...   \n",
       "24      25  李芳 抹 着 眼泪 告诉 小溪 \u0001 2002年 \u0001 她 的 儿子 逝去 了 \u0001 儿子 才 1...   \n",
       "25      26  上上周 \u0001 安德烈 突然 找到 我 \u0001 说 想 要 在 医院 举行 婚礼 \u0001 我 先是 吓 ...   \n",
       "26      27  1952年 夏 \u0001 她 和 另 7 名 日籍 护士 \u0001 从 广西 南宁 军区 303 医院 ...   \n",
       "27      28  不仅 是 对 尹家 的 3 个 孩子 \u0001 在 街坊 邻居 中 \u0001 张崇贞 也 有着 他人 不...   \n",
       "28      29  2010 年 \u0001 母亲 去世 \u0001 尹家 的 3 个 孩子 将 母亲 和 奶奶 正式 合葬 \u0001...   \n",
       "29      30  朱洪生 今年 55 岁 \u0001 1979年 参加 工作 时 才 19 岁 \u0001 已 有 36 年 ...   \n",
       "...    ...                                                ...   \n",
       "2075  2076  公安部 将 此案 定 为 海燕 3 号 专案 \u0001 由 广东省 挂牌 督办 \u0001 2014年 5...   \n",
       "2076  2077  涉嫌 故意 杀人 被 诉 同年 10月 17日 \u0001 贾某 因 涉嫌 故意 杀人罪 被 公安 ...   \n",
       "2077  2078  这 辆 越野车 够 野逃 了 3 公里 被 抓住 4月 15日 下午 \u0001 一 辆 越野车 因...   \n",
       "2078  2079  去年 10月 \u0001 浙江 永康 27 岁 姑娘 黄燕 被 确诊 患 上 急性 淋巴细胞 白血病...   \n",
       "2079  2080  雨梅 他 平时 用 电脑 上网 总是 背着 我 \u0001 7 年 前 有 一 次 我 出门 忘带 ...   \n",
       "2080  2081  原来 \u0001 贺某 一直 随 父 居住 在 大足 城区 \u0001 由于 父母 离异 \u0001 贺某 父亲 待...   \n",
       "2081  2082  出 家 后 的 林后 领 本 应 洗心革面 \u0001 然而 来自 老家 的 一 则 喜讯 却 再次...   \n",
       "2082  2083  经 审查 \u0001 嫌疑人 方某 \u0001 男 \u0001 25 岁 \u0001 湖南 岳阳人 \u0001 据 其 交代 \u0001 2...   \n",
       "2083  2084  为 救 老伴 \u0001 63 岁 的 老齐不顾 众人 劝阻 \u0001 两 次 从 隔壁 人家 翻 墙上 ...   \n",
       "2084  2085  两 度 入 火海 救 老伴 逃 出来 的 老齐 看 着 被 火光 与 浓烟 笼罩 着 的 三...   \n",
       "2085  2086  然而 \u0001 就 是 这么 简单 的 一 句 回话 \u0001 让 李某 认为 就是 丈母娘 一 家 将...   \n",
       "2086  2087  康某 逐渐 意识 到 情况 不妙 \u0001 悄悄 跑 到 买 的 房子 里 一 探 究竟 \u0001 发现...   \n",
       "2087  2088  11日 下午 \u0001 翁某 在 横峰 一 家 小鞋厂 做 了 两 天 半 的 工 \u0001 不 想 再...   \n",
       "2088  2089  她家 住 海曙 翠柏东巷 \u0001 儿子 今年 4 岁 \u0001 4月 4日 下午 1点 多 \u0001 她 带...   \n",
       "2089  2090  广东 清远 法院 14日 通报 \u0001 该 名酒店 保安 日前 被 法院 以 故意 伤害 罪 判...   \n",
       "2090  2091  楚天 都市 报 记者 朱蕾 @郭小貓 777 就 是 这个 人 \u0001 公车 上 偷偷 摸 我 ...   \n",
       "2091  2092  校方 为 保护 学生 才 打 狗 昨日 下午 \u0001 记者 联系 了 通州 高级 中学 进行 核...   \n",
       "2092  2093  40 分钟 后 两 人 被 救 出 我 这 心 吓 得 扑通 扑通 直跳 \u0001 孙女 吓 得 ...   \n",
       "2093  2094  目前 \u0001 安内格雷特 已 怀孕 21 周 \u0001 卢森堡 广播 电视台 称 她 无 任何 并发症...   \n",
       "2094  2095  据 张 女士 介绍 \u0001 女儿 在 班里 成绩 并 不 差 \u0001 能够 稳定 在 前 15 名 ...   \n",
       "2095  2096  报道 \u0001 基隆市 殡葬 管理所 刚 启用 运 棺木 电梯 昨天 12日 突卡 住 \u0001 27 ...   \n",
       "2096  2097  一度 因 等 不 到 家属 \u0001 棺木 无法 送 火化 \u0001 直到 消防 人员 赶到 撬 开 电...   \n",
       "2097  2098  陈玉梅 说 \u0001 约斯 夫妇 向 她 发 来 了 电子 邮件 \u0001 大意 是 希望 她 能 帮助...   \n",
       "2098  2099  他 称 \u0001 学 医 的 梦想 要 实现 了 \u0001 我 肯定 要 尝试 \u0001 对于 淦菊保 的 决...   \n",
       "2099  2100  在 五洲 装饰城 旁边 的 紫金东路 一侧 \u0001 停 着 10 多 辆 蓝色 货车 \u0001 大多 ...   \n",
       "2100  2101  被害人 李 女士 与 赵某 交往 过程 中 \u0001 应 赵某 要求 \u0001 通过 微信 将 自己 的...   \n",
       "2101  2102  空地 与 路 之间 的 路牙 埂子 挡住 右 前轮 \u0001 车轮 过 不 了 路 牙埂子 \u0001 于...   \n",
       "2102  2103  据 安徽 卫视 公共 频道 报道 \u0001 4月 6日 \u0001 定远县 总医院 住院部 发生 了 一 ...   \n",
       "2103  2104  我们 夫妻 吵架 \u0001 你们 来 干 什么 \u0001 朱 某某 质问道 \u0001 小 隐嚣张 地 说 她 ...   \n",
       "2104  2105  自己 经常 请 张 迎春 喝酒 看 电影 \u0001 一 次 聚会 酒后 在 消费单 上 签 上 了...   \n",
       "\n",
       "                                                 nature  keyword  emotion  \\\n",
       "0     p r v n p v \u0001 j n v v r u n n \u0001 r v r d p r n ...       激动        0   \n",
       "1     p n v \u0001 p n d \u0001 r p n r j v v v m m q \u0001 c m p ...       激动        0   \n",
       "2     n n n n \u0001 n u n v \u0001 n n n m n n v m q n d m q ...       心疼        3   \n",
       "3     p d p n v v \u0001 n d v u v n \u0001 m q p n v v n \u0001 c ...       无奈        3   \n",
       "4     v n v c p n v v v u n \u0001 m n d v m m m q \u0001 v v ...       自豪        0   \n",
       "5     n n \u0001 p n v m q u n d v v b u n n \u0001 n n d d v ...       欣喜        0   \n",
       "6     n n \u0001 p n v m q u n d v v b u n n \u0001 n n d d v ...       忧虑        3   \n",
       "7     v d v v v n v n d r n \u0001 c n n v \u0001 n r d p n n ...  生 不 如 死        5   \n",
       "8     v d v v v n v n d r n \u0001 c n n v \u0001 n r d p n n ...       惊诧        6   \n",
       "9     p v v n v n \u0001 n d v u n \u0001 n p r v n i \u0001 c n a ...       担心        4   \n",
       "10    n n v r m m q n v \u0001 v d i \u0001 c r d v u \u0001 n v n ...       无奈        3   \n",
       "11    n n v r m m q n v \u0001 v d i \u0001 c r d v u \u0001 n v n ...       感谢        1   \n",
       "12    n n n \u0001 v j n v v \u0001 n n r n n v u m q v n \u0001 c ...    没有 人性        5   \n",
       "13    n v p n n d v p m n u n v v \u0001 n n a d p n v v ...       激动        2   \n",
       "14    n v p n n d v p m n u n v v \u0001 n n a d p n v v ...       感动        0   \n",
       "15    d n n n \u0001 j b v n p n d p m q n n v \u0001 n v n v ...       诧异        6   \n",
       "16    m q u \u0001 n m q n d d v u \u0001 r n v a n v a v d v ...       心疼        3   \n",
       "17                          v n \u0001 n d c p n v d v n v v       自责        5   \n",
       "18    n n \u0001 p n n n v n u n v v n v u n u n n \u0001 v n ...  高兴 不 起来        3   \n",
       "19    v d v u \u0001 n v r v d v v b n \u0001 n v u v r v r u ...     泣不成声        3   \n",
       "20    n p n v v n n \u0001 v u m \u0001 n n d d v a r d v r v ...      笑 了        0   \n",
       "21    p n n v n v d v d v \u0001 d v u n d v n u n \u0001 n v ...       赞扬        1   \n",
       "22    p n n u n n \u0001 n v v v n n v u n \u0001 v m d i \u0001 v n v       尊敬        1   \n",
       "23    n z m q n n n \u0001 n r a a \u0001 d v v r a v u n n \u0001 ...       绝望        3   \n",
       "24    n v u n v n \u0001 n \u0001 r u n v u \u0001 n d m q \u0001 a a \u0001 ...     痛不欲生        3   \n",
       "25    n \u0001 n a v r \u0001 v v v p n v n \u0001 r d v u m v \u0001 d ...       感动        0   \n",
       "26    n n \u0001 r c r m q n n \u0001 p n n n m n v v n n n n ...       着急        4   \n",
       "27    c v p n u m q n \u0001 p n n n \u0001 n d v r d v v u a ...       敬重        1   \n",
       "28    m q \u0001 n v \u0001 n u m q n p n c n a v \u0001 n n \u0001 v u ...       欣慰        0   \n",
       "29    n n m q \u0001 n v n n d m q \u0001 d v m q u n \u0001 r n p ...       自豪        0   \n",
       "...                                                 ...      ...      ...   \n",
       "2075  n p r v v n m q n \u0001 p n v v \u0001 n n p n \u0001 r v n ...       吃惊        6   \n",
       "2076  v d v p v d n n \u0001 n p v d v p n n b v \u0001 n n p ...    气愤 至极        2   \n",
       "2077  r q n v a u m q p v n n n \u0001 m q n p v n c r m ...       心虚        4   \n",
       "2078  n n \u0001 n n m q n n p v v v b n n \u0001 n m m q i \u0001 ...       感动        1   \n",
       "2079  n r n p n v d v r \u0001 m q n v m q r v v n v v v ...       恐怖        4   \n",
       "2080  d \u0001 n d v n v p n n \u0001 p n v \u0001 n n v n p v i \u0001 ...       愤然        2   \n",
       "2081  v n n u n v d v n \u0001 c v n u m q n d d v u r n ...       羡慕        1   \n",
       "2082  p v \u0001 n n \u0001 b \u0001 m q \u0001 n n \u0001 p r v \u0001 n n n \u0001 c ...       郁闷        5   \n",
       "2083  p v n \u0001 m q u n n v \u0001 m q p n r v n n v n u n ...     心有余悸        4   \n",
       "2084  m q v n v n v v u n v u p n c n v u u m q a n ...       心焦        4   \n",
       "2085  c \u0001 d v r a u m q n \u0001 v n v v n m q p n v u v ...       窝火        2   \n",
       "2086  n d v v n a \u0001 d v v v u n n m v d \u0001 v n d d v ...       惊讶        6   \n",
       "2087  n n \u0001 n p n m q n v u m q m u n \u0001 d v d v v u ...       害怕        4   \n",
       "2088  r v n n \u0001 n n m q \u0001 n n n n m \u0001 r v u n v v u ...       兴奋        0   \n",
       "2089  n n n n v \u0001 r n n n p n p d v n v m q m q n \u0001 ...    气 不 过        2   \n",
       "2090  n n n n n n m d v r n \u0001 n n d v r \u0001 n v n n p ...       恼火        2   \n",
       "2091  n p v n d v n n n \u0001 n v u n a n v v \u0001 p r \u0001 r ...       惊吓        4   \n",
       "2092  m q n m n p v v r r n v u o o v \u0001 n v u d v \u0001 ...     担惊受怕        4   \n",
       "2093  n \u0001 n d v m q \u0001 n v n v r v r n \u0001 n v a \u0001 c r ...       震惊        6   \n",
       "2094  p n n v \u0001 n p n n d d v \u0001 v v p n m q \u0001 n n d ...       心慌        4   \n",
       "2095  n \u0001 n v n d v v n n n n n v \u0001 m q n v m q \u0001 v ...    饱受 惊吓        4   \n",
       "2096  d p v d v n \u0001 n v v v \u0001 v b n v v v n \u0001 d p n ...     人心惶惶        4   \n",
       "2097  n v \u0001 n n p r v v u n n \u0001 n v v r v v a n \u0001 v ...       内疚        3   \n",
       "2098  r v \u0001 v n u n v v u \u0001 r d v v \u0001 p j u n \u0001 n c ...       佩服        1   \n",
       "2099  p j n n u n n \u0001 v u m m q n n \u0001 d v n p v v \u0001 ...       愤怒        2   \n",
       "2100  n n n p n v n n \u0001 v n v \u0001 p n p r u m n v n c ...       愤恨        2   \n",
       "2101  n c n n u n n v n n \u0001 n u d v n n \u0001 c r d v u ...       心慌        4   \n",
       "2102  p n j b n n \u0001 n n \u0001 n n n v u m q n \u0001 m q n v ...       气愤        2   \n",
       "2103  r n v \u0001 r v v r \u0001 n r n \u0001 a a u v r d r n n u ...   怒火 中 烧        2   \n",
       "2104  r d v n v v v n \u0001 m q v n p n n v v u r u n \u0001 ...       怨恨        5   \n",
       "\n",
       "                                             clause_pos  \\\n",
       "0                            63 64 65 66 67 68 69 70 71   \n",
       "1                            61 62 63 64 65 66 67 68 69   \n",
       "2          58 59 60 61 62 63 64 65 66 67 68 69 70 71 72   \n",
       "3                   58 59 60 61 62 63 64 65 66 67 68 69   \n",
       "4                                  63 64 65 66 67 68 69   \n",
       "5                   64 65 66 67 68 69 70 71 72 73 74 75   \n",
       "6                   61 62 63 64 65 66 67 68 69 70 71 72   \n",
       "7                               69 70 71 72 73 74 75 76   \n",
       "8                               62 63 64 65 66 67 68 69   \n",
       "9                   65 66 67 68 69 70 71 72 73 74 75 76   \n",
       "10                           65 66 67 68 69 70 71 72 73   \n",
       "11                        63 64 65 66 67 68 69 70 71 72   \n",
       "12                                 64 65 66 67 68 69 70   \n",
       "13                                 68 69 70 71 72 73 74   \n",
       "14                                 64 65 66 67 68 69 70   \n",
       "15                        65 66 67 68 69 70 71 72 73 74   \n",
       "16                     63 64 65 66 67 68 69 70 71 72 73   \n",
       "17                                                68 69   \n",
       "18                  64 65 66 67 68 69 70 71 72 73 74 75   \n",
       "19                                 63 64 65 66 67 68 69   \n",
       "20                     62 63 64 65 66 67 68 69 70 71 72   \n",
       "21                                 67 68 69 70 71 72 73   \n",
       "22                                          66 67 68 69   \n",
       "23                              62 63 64 65 66 67 68 69   \n",
       "24                        61 62 63 64 65 66 67 68 69 70   \n",
       "25            65 66 67 68 69 70 71 72 73 74 75 76 77 78   \n",
       "26    56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 7...   \n",
       "27                  63 64 65 66 67 68 69 70 71 72 73 74   \n",
       "28    52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 6...   \n",
       "29                              62 63 64 65 66 67 68 69   \n",
       "...                                                 ...   \n",
       "2075                      65 66 67 68 69 70 71 72 73 74   \n",
       "2076  59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 7...   \n",
       "2077                         66 67 68 69 70 71 72 73 74   \n",
       "2078                            65 66 67 68 69 70 71 72   \n",
       "2079          63 64 65 66 67 68 69 70 71 72 73 74 75 76   \n",
       "2080  61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 7...   \n",
       "2081                      65 66 67 68 69 70 71 72 73 74   \n",
       "2082             61 62 63 64 65 66 67 68 69 70 71 72 73   \n",
       "2083          62 63 64 65 66 67 68 69 70 71 72 73 74 75   \n",
       "2084    64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79   \n",
       "2085    59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74   \n",
       "2086  63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 7...   \n",
       "2087  57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 7...   \n",
       "2088             64 65 66 67 68 69 70 71 72 73 74 75 76   \n",
       "2089    63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78   \n",
       "2090                66 67 68 69 70 71 72 73 74 75 76 77   \n",
       "2091    63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78   \n",
       "2092  57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 7...   \n",
       "2093       60 61 62 63 64 65 66 67 68 69 70 71 72 73 74   \n",
       "2094  58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 7...   \n",
       "2095                66 67 68 69 70 71 72 73 74 75 76 77   \n",
       "2096                61 62 63 64 65 66 67 68 69 70 71 72   \n",
       "2097                63 64 65 66 67 68 69 70 71 72 73 74   \n",
       "2098       59 60 61 62 63 64 65 66 67 68 69 70 71 72 73   \n",
       "2099  50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 6...   \n",
       "2100             64 65 66 67 68 69 70 71 72 73 74 75 76   \n",
       "2101       60 61 62 63 64 65 66 67 68 69 70 71 72 73 74   \n",
       "2102                   59 60 61 62 63 64 65 66 67 68 69   \n",
       "2103                   62 63 64 65 66 67 68 69 70 71 72   \n",
       "2104                      64 65 66 67 68 69 70 71 72 73   \n",
       "\n",
       "                                            label  \n",
       "0                               0 0 0 0 0 0 0 0 1  \n",
       "1                               0 0 0 0 0 0 0 1 0  \n",
       "2                   0 0 0 0 0 0 0 0 0 0 0 1 0 0 0  \n",
       "3                         0 0 0 0 0 0 0 0 1 1 1 0  \n",
       "4                                   0 0 0 0 0 1 0  \n",
       "5                         0 0 0 0 0 1 0 0 0 0 0 0  \n",
       "6                         0 0 0 0 0 0 0 0 0 0 0 1  \n",
       "7                                 0 1 1 0 0 0 0 0  \n",
       "8                                 0 0 0 0 1 0 1 0  \n",
       "9                         0 0 0 1 0 0 0 0 0 0 0 0  \n",
       "10                              0 1 0 1 0 0 0 0 0  \n",
       "11                            0 0 0 0 0 0 1 1 0 0  \n",
       "12                                  0 0 0 0 1 0 0  \n",
       "13                                  1 0 0 0 0 0 0  \n",
       "14                                  0 0 0 0 0 1 0  \n",
       "15                            0 0 1 0 0 0 0 0 0 0  \n",
       "16                          0 0 0 0 0 1 0 0 0 0 0  \n",
       "17                                            0 1  \n",
       "18                        0 0 0 0 0 0 1 0 0 0 0 0  \n",
       "19                                  0 0 0 0 0 1 0  \n",
       "20                          0 0 0 0 0 0 1 0 0 0 0  \n",
       "21                                  1 0 0 0 0 0 0  \n",
       "22                                        0 1 0 0  \n",
       "23                                0 0 0 0 0 0 0 1  \n",
       "24                            0 0 0 0 0 0 1 1 0 0  \n",
       "25                    0 0 1 0 0 0 0 0 0 0 0 0 0 0  \n",
       "26              0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0  \n",
       "27                        0 0 0 0 0 0 1 0 0 0 0 0  \n",
       "28            0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0  \n",
       "29                                0 0 0 0 0 0 1 0  \n",
       "...                                           ...  \n",
       "2075                          0 0 0 0 0 1 0 0 0 0  \n",
       "2076            0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0  \n",
       "2077                            0 0 1 0 0 0 0 0 0  \n",
       "2078                              0 0 0 0 1 0 0 0  \n",
       "2079                  0 0 0 0 0 0 0 0 0 1 0 0 0 0  \n",
       "2080          0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0  \n",
       "2081                          0 0 0 1 0 0 0 0 0 0  \n",
       "2082                    0 0 0 0 0 0 0 1 0 0 0 0 0  \n",
       "2083                  0 0 0 0 0 0 1 0 0 0 0 0 0 0  \n",
       "2084              0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0  \n",
       "2085              0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0  \n",
       "2086        0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0  \n",
       "2087    0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0  \n",
       "2088                    0 0 0 1 0 0 0 0 0 0 0 0 0  \n",
       "2089              0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0  \n",
       "2090                      0 0 1 0 0 0 0 0 0 0 0 0  \n",
       "2091              0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0  \n",
       "2092        0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0  \n",
       "2093                0 0 0 0 0 0 0 0 1 0 0 0 0 0 0  \n",
       "2094            0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0  \n",
       "2095                      0 0 1 0 0 0 0 0 0 0 0 0  \n",
       "2096                      0 0 0 0 0 0 0 1 0 0 0 0  \n",
       "2097                      0 0 0 0 0 0 1 0 0 0 0 0  \n",
       "2098                0 0 0 0 0 0 0 0 0 1 0 0 0 0 0  \n",
       "2099  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0  \n",
       "2100                    0 0 0 0 0 1 0 0 0 0 0 0 0  \n",
       "2101                0 0 0 0 0 0 0 0 1 0 0 0 0 0 0  \n",
       "2102                        0 0 0 0 0 0 0 0 1 0 0  \n",
       "2103                        0 0 0 0 1 1 1 0 0 0 0  \n",
       "2104                          0 0 0 0 1 0 0 0 0 0  \n",
       "\n",
       "[2105 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T15:27:26.161427Z",
     "start_time": "2019-03-25T15:27:26.156723Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [keywords.strip().split(' ') for keywords in data.keyword.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T15:37:33.109660Z",
     "start_time": "2019-03-25T15:37:30.635595Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-25 15:37:30,955 INFO: 33 batches, avg len: 3.1\n",
      "2019-03-25 15:37:32,031 INFO: Finished 1000 sentences.\n",
      "2019-03-25 15:37:32,979 INFO: Finished 2000 sentences.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2105,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo = np.array(e.sents2elmo(corpus))\n",
    "elmo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T15:38:18.237039Z",
     "start_time": "2019-03-25T15:38:18.205341Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(elmo, open('/data/wujipeng/ec/data/embedding/elmo_kw_embedding1024d.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
