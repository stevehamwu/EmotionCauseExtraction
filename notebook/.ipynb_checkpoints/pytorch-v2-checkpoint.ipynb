{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-19T19:23:19.501264Z",
     "start_time": "2019-03-19T19:23:18.654070Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from itertools import chain\n",
    "from six.moves import reduce\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:3\" if USE_CUDA else \"cpu\")\n",
    "random_state = 42\n",
    "learning_rate = 0.0002\n",
    "batch_size = 16\n",
    "epsilon = 1e-8\n",
    "max_grad_norm = 40.0\n",
    "evaluation_interval = 1\n",
    "hops = 3\n",
    "epochs = 20\n",
    "embedding_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T19:38:55.047775Z",
     "start_time": "2019-03-24T19:38:52.983954Z"
    },
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyhanlp import *\n",
    "from pyltp import Segmentor, Postagger\n",
    "import random\n",
    "\n",
    "\n",
    "LTP_DATA_DIR = '/home/wujipeng/data/ltp_data_v3.4.0/'  # ltp模型目录的路径\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "segmentor = Segmentor()  # 初始化实例\n",
    "segmentor.load(cws_model_path)  # 加载模型\n",
    "\n",
    "\n",
    "def hanlp_cut(sentence):\n",
    "    return [term.word for term in HanLP.segment(sentence)]\n",
    "\n",
    "\n",
    "def ltp_cut(sentence):\n",
    "    return segmentor.segment(sentence)\n",
    "\n",
    "\n",
    "def sampling():\n",
    "    numOfSentence = 2105\n",
    "    testSentence = set()\n",
    "    for i in range(1, numOfSentence + 1):\n",
    "        if random.random() > 0.9:\n",
    "            testSentence.add(i)\n",
    "    rate = float(len(testSentence)) / numOfSentence\n",
    "    #    print(rate)\n",
    "    if rate > 0.1 and rate - 0.1 < 0.0003:\n",
    "        print(len(testSentence))\n",
    "        return testSentence\n",
    "    else:\n",
    "        return sampling()\n",
    "\n",
    "\n",
    "def division(testSentence, n, mode='train'):\n",
    "    data_path = '/home/wujipeng/git/ECA/ec/MemNet/' + 'data' if mode == 'train' else 'test_data'\n",
    "    testSentenceFile = open(os.path.join(data_path, 'testSentenceFile_{}.csv'.format(n)), 'w')\n",
    "    for index, item in enumerate(testSentence):\n",
    "        testSentenceFile.write(str(item) + '\\n')\n",
    "    testSentenceFile.close()\n",
    "\n",
    "    inputFile = open(os.path.join(data_path, 'datacsv_2105.csv'), 'r')\n",
    "    outputFile1 = open(os.path.join(data_path, 'clause_test_{}.csv'.format(n)), 'w')\n",
    "    outputFile2 = open(os.path.join(data_path, 'clause_train_{}.csv'.format(n)), 'w')\n",
    "    test_pos_count, test_neg_count, train_pos_count, train_neg_count = 0, 0, 0, 0\n",
    "    for _, line in enumerate(inputFile):\n",
    "        sentenceID = int(line.strip().split(',')[0])\n",
    "        sentence = line.strip().split(',')[-1]\n",
    "        keyword = None\n",
    "        keyPos = -1\n",
    "        clauseList = re.split('，|。|？|！|；|……', sentence)\n",
    "        causeOfSent = set()\n",
    "        for index, item in enumerate(clauseList):\n",
    "            match = re.search(r'\\[f\\]([^\\[]*)\\[/f\\]', item)\n",
    "            if match:\n",
    "                keyword = match.group(1)\n",
    "                keyPos = index\n",
    "            match = re.search(r'\\[\\d[nv]\\][^\\[]*\\[/\\d[nv]\\]', item)\n",
    "            if match:\n",
    "                causeOfSent.add(index)\n",
    "            match = re.search(r'\\[[-\\d]*\\*\\d[nv]\\][^\\[]*\\[/[-\\d]*\\*\\d[nv]\\]', item)\n",
    "            if match:\n",
    "                causeOfSent.add(index)\n",
    "        for index, item in enumerate(clauseList):\n",
    "            clause = re.sub(r'\\[[^\\[]*\\]', '', item)\n",
    "            clause = re.sub(r'“', '', clause)\n",
    "            clause = re.sub(r'”', '', clause)\n",
    "            clause = re.sub(r'：', '', clause)\n",
    "            clause = re.sub(r'\\(', '', clause)\n",
    "            clause = re.sub(r'\\)', '', clause)\n",
    "            clause = re.sub(r'、', '', clause)\n",
    "            clause = re.sub(r'’', '', clause)\n",
    "            clause = re.sub(r'‘', '', clause)\n",
    "            clause = re.sub(r'》', '', clause)\n",
    "            clause = re.sub(r'《', '', clause)\n",
    "            clause = re.sub(r'~', '', clause)\n",
    "            words = hanlp_cut(clause)\n",
    "            pos = index - keyPos\n",
    "            if (sentenceID in testSentence) and (clause.split()):\n",
    "                if index in causeOfSent:\n",
    "                    test_pos_count += 1\n",
    "                    outputFile1.write(str(sentenceID) + ',' + str(index + 1) + ',' + keyword + ',')\n",
    "                    outputFile1.write(str(pos) + ',yes,' + ' '.join(words) + '\\n')\n",
    "                else:\n",
    "                    test_neg_count += 1\n",
    "                    outputFile1.write(str(sentenceID) + ',' + str(index + 1) + ',' + keyword + ',')\n",
    "                    outputFile1.write(str(pos) + ',no,' + ' '.join(words) + '\\n')\n",
    "            elif (sentenceID not in testSentence) and (clause.split()):\n",
    "                if index in causeOfSent:\n",
    "                    train_pos_count += 1\n",
    "                    outputFile2.write(str(sentenceID) + ',' + str(index + 1) + ',' + keyword + ',')\n",
    "                    outputFile2.write(str(pos) + ',yes,' + ' '.join(words) + '\\n')\n",
    "                else:\n",
    "                    train_neg_count += 1\n",
    "                    outputFile2.write(str(sentenceID) + ',' + str(index + 1) + ',' + keyword + ',')\n",
    "                    outputFile2.write(str(pos) + ',no,' + ' '.join(words) + '\\n')\n",
    "    print('********************************')\n",
    "    print('test_pos_count', test_pos_count)\n",
    "    print('test_neg_count', test_neg_count)\n",
    "    print('train_pos_count', train_pos_count)\n",
    "    print('train_neg_count', train_neg_count)\n",
    "    print('********************************')\n",
    "    inputFile.close()\n",
    "    outputFile1.close()\n",
    "    outputFile2.close()\n",
    "\n",
    "\n",
    "def statisticPos(mode='train'):\n",
    "    data_path = '/home/wujipeng/git/ECA/ec/MemNet/' + 'data' if mode == 'train' else 'test_data'\n",
    "    inputFile = open(os.path.join(data_path, 'datacsv_2105.csv'), 'r')\n",
    "    posDict = dict()\n",
    "    for _, line in enumerate(inputFile):\n",
    "        content = line.strip().split(',')[-1]\n",
    "        clauseList = re.split('，|。|？|！|；|……', content)\n",
    "        keyPos = -1\n",
    "        for index, item in enumerate(clauseList):\n",
    "            match = re.search(r'\\[f\\]([^\\[]*)\\[/f\\]', item)\n",
    "            if match:\n",
    "                keyPos = index\n",
    "        for index, item in enumerate(clauseList):\n",
    "            pos = index - keyPos\n",
    "            if posDict.get(pos):\n",
    "                posDict[pos] += 1\n",
    "            else:\n",
    "                posDict[pos] = 1\n",
    "    inputFile.close()\n",
    "    return posDict\n",
    "\n",
    "\n",
    "def changePos(posDict):\n",
    "    count = 0\n",
    "    countDict = dict()\n",
    "    for key, value in posDict.items():\n",
    "        countDict[key] = count\n",
    "        count += 1\n",
    "    posList = ['AAAA', 'AAAB', 'AAAC', 'AAAD', 'AABA', 'AABB', 'AABC', 'AABD', 'AACA', 'AACB', 'AACC', 'AACD',\n",
    "               'AADA', 'AADB', 'AADC', 'AADD', 'ABAA', 'ABAB', 'ABAC', 'ABAD', 'ABBA', 'ABBB', 'ABBC', 'ABBD',\n",
    "               'ABCA', 'ABCB', 'ABCC', 'ABCD', 'ABDA', 'ABDB', 'ABDC', 'ABDD', 'ACAA', 'ACAB', 'ACAC', 'ACAD',\n",
    "               'ACBA', 'ACBB', 'ACBC', 'ACBD', 'ACCA', 'ACCB', 'ACCC', 'ACCD', 'ACDA', 'ACDB', 'ACDC', 'ACDD',\n",
    "               'ADAA', 'ADAB', 'ADAC', 'ADAD', 'ADBA', 'ADBB', 'ADBC', 'ADBD', 'ADCA', 'ADCB', 'ADCC', 'ADCD',\n",
    "               'ADDA', 'ADDB', 'ADDC', 'ADDD', 'BAAA', 'BAAB', 'BAAC', 'BAAD', 'BABA', 'BABB', 'BABC', 'BABD',\n",
    "               'BACA', 'BACB', 'BACC', 'BACD', 'BADA', 'BADB', 'BADC', 'BADD', 'BBAA', 'BBAB', 'BBAC', 'BBAD',\n",
    "               'BBBA', 'BBBB', 'BBBC', 'BBBD', 'BBCA', 'BBCB', 'BBCC', 'BBCD', 'BCAA', 'BCAB', 'BCAC', 'BCAD',\n",
    "               'BDAA', 'BDAB', 'BDAC', 'BDAD', 'BCBA', 'BCBB', 'BCBC']\n",
    "    strPosDict = dict()\n",
    "    for key, value in countDict.items():\n",
    "        strPosDict[key] = posList[value]\n",
    "    return strPosDict\n",
    "\n",
    "\n",
    "def construction(strPosDict, n, mode='train'):\n",
    "    data_path = '/home/wujipeng/git/ECA/ec/MemNet/' + 'data' if mode == 'train' else 'test_data'\n",
    "    inputFile1 = open(os.path.join(data_path, 'clause_test_{}.csv'.format(n)), 'r')\n",
    "    inputFile2 = open(os.path.join(data_path, 'clause_train_{}.csv'.format(n)), 'r')\n",
    "    outputFile1 = open(os.path.join(data_path, 'emotion_cause_clause_level_test_{}.csv'.format(n)), 'w')\n",
    "    outputFile2 = open(os.path.join(data_path, 'emotion_cause_clause_level_train_{}.csv'.format(n)), 'w')\n",
    "    for _, line in enumerate(inputFile1):\n",
    "        keyword = line.strip().split(',')[2]\n",
    "        position = int(line.strip().split(',')[3])\n",
    "        posStr = strPosDict[position]\n",
    "        label = line.strip().split(',')[4]\n",
    "        clause = line.strip().split(',')[5]\n",
    "        wordList = clause.strip().split(' ')\n",
    "        phraseList = []\n",
    "        if len(wordList) >= 3:\n",
    "            window = 3\n",
    "            begin = 0\n",
    "            for index, item in enumerate(wordList):\n",
    "                end = begin + window\n",
    "                if end <= len(wordList):\n",
    "                    phraseList.append(wordList[begin: end])\n",
    "                begin += 1\n",
    "        else:\n",
    "            phraseList.append(wordList[:])\n",
    "        lineNum = 1\n",
    "        for index, item in enumerate(phraseList):\n",
    "            outputFile1.write(str(lineNum))\n",
    "            for i in range(len(item)):\n",
    "                outputFile1.write(' ' + item[i])\n",
    "            outputFile1.write('\\n')\n",
    "            lineNum += 1\n",
    "        outputFile1.write(str(lineNum) + ' ' + posStr + ' ' + posStr + ' ' + posStr + '\\n')\n",
    "        lineNum += 1\n",
    "        outputFile1.write(str(lineNum) + ' ' + keyword + ' ' + keyword + ' ' + keyword + '\\t' + label + '\\n')\n",
    "    inputFile1.close()\n",
    "    outputFile1.close()\n",
    "    for _, line in enumerate(inputFile2):\n",
    "        keyword = line.strip().split(',')[2]\n",
    "        position = int(line.strip().split(',')[3])\n",
    "        posStr = strPosDict[position]\n",
    "        label = line.strip().split(',')[4]\n",
    "        clause = line.strip().split(',')[5]\n",
    "        wordList = clause.strip().split(' ')\n",
    "        phraseList = []\n",
    "        if len(wordList) >= 3:\n",
    "            window = 3\n",
    "            begin = 0\n",
    "            for index, item in enumerate(wordList):\n",
    "                end = begin + window\n",
    "                if end <= len(wordList):\n",
    "                    phraseList.append(wordList[begin: end])\n",
    "                begin += 1\n",
    "        else:\n",
    "            phraseList.append(wordList[:])\n",
    "        lineNum = 1\n",
    "        for index, item in enumerate(phraseList):\n",
    "            outputFile2.write(str(lineNum))\n",
    "            for i in range(len(item)):\n",
    "                outputFile2.write(' ' + item[i])\n",
    "            outputFile2.write('\\n')\n",
    "            lineNum += 1\n",
    "        outputFile2.write(str(lineNum) + ' ' + posStr + ' ' + posStr + ' ' + posStr + '\\n')\n",
    "        lineNum += 1\n",
    "        outputFile2.write(str(lineNum) + ' ' + keyword + ' ' + keyword + ' ' + keyword + '\\t' + label + '\\n')\n",
    "    inputFile2.close()\n",
    "    outputFile2.close()\n",
    "\n",
    "\n",
    "def extractAll(mode='train'):\n",
    "    data_path = '/home/wujipeng/git/ECA/ec/MemNet/' + 'data' if mode == 'train' else 'test_data'\n",
    "    inputFile = open(os.path.join(data_path, 'datacsv_2105.csv'), 'r')\n",
    "    allClause = []\n",
    "    for index, line in enumerate(inputFile):\n",
    "        sent = int(line.strip().split(',')[0])  # int\n",
    "        content = line.strip().split(',')[-1]\n",
    "        clauseList = re.split('，|。|？|！|；|……', content)\n",
    "        for index, clause in enumerate(clauseList):\n",
    "            cause = 'n'\n",
    "            match = re.search(r'\\[\\d[nv]\\][^\\[]*\\[/\\d[nv]\\]', clause)\n",
    "            if match:\n",
    "                cause = 'c'\n",
    "            match = re.search(r'\\[[-\\d]*\\*\\d[nv]\\][^\\[]*\\[/[-\\d]*\\*\\d[nv]\\]', clause)\n",
    "            if match:\n",
    "                cause = 'c'\n",
    "            clause = re.sub(r'\\[[^\\[]*\\]', '', clause)\n",
    "            clause = re.sub(r'“', '', clause)\n",
    "            clause = re.sub(r'”', '', clause)\n",
    "            clause = re.sub(r'：', '', clause)\n",
    "            clause = re.sub(r'\\(', '', clause)\n",
    "            clause = re.sub(r'\\)', '', clause)\n",
    "            clause = re.sub(r'、', '', clause)\n",
    "            clause = re.sub(r'’', '', clause)\n",
    "            clause = re.sub(r'‘', '', clause)\n",
    "            clause = re.sub(r'》', '', clause)\n",
    "            clause = re.sub(r'《', '', clause)\n",
    "            if clause.split():\n",
    "                allClause.append([sent, index + 1, cause])\n",
    "    print('allClause', len(allClause))\n",
    "    inputFile.close()\n",
    "    return allClause\n",
    "\n",
    "\n",
    "def extractRealRight(allClause, n, mode='train'):\n",
    "    data_path = '/home/wujipeng/git/ECA/ec/MemNet/' + 'data' if mode == 'train' else 'test_data'\n",
    "    testSentenceFile = open(os.path.join(data_path, 'testSentenceFile_{}.csv'.format(n)), 'r')\n",
    "    testSentence = set()\n",
    "    for index, item in enumerate(testSentenceFile):\n",
    "        item = int(item.strip())\n",
    "        testSentence.add(item)\n",
    "    testSentenceFile.close()\n",
    "    testClause = []\n",
    "    for index, item in enumerate(allClause):\n",
    "        sent = allClause[index][0]\n",
    "        if sent in testSentence:\n",
    "            testClause.append(item)\n",
    "    print('testClause(all)', len(testClause))\n",
    "    realRight = []\n",
    "    realSet = set()\n",
    "    for index, item in enumerate(testClause):\n",
    "        cSent = item[0]\n",
    "        cClause = item[1]\n",
    "        cCause = item[2]\n",
    "        if cCause == 'c':\n",
    "            realRight.append([cSent, cClause])\n",
    "            key = str(cSent) + ',' + str(cClause)\n",
    "            realSet.add(key)\n",
    "    print('realRight', len(realRight))\n",
    "    print('realSet', len(realSet))\n",
    "    return realRight, realSet\n",
    "\n",
    "\n",
    "def extractPredRight(n, m, mode='train'):\n",
    "    data_path = '/home/wujipeng/git/ECA/ec/MemNet/' + 'data' if mode == 'train' else 'test_data'\n",
    "    inputFile = open(os.path.join(data_path, 'clause_test_{}.csv'.format(n)), 'r')\n",
    "    sentAndClause = []\n",
    "    clauseTest = []\n",
    "    keywords = []\n",
    "    for index, line in enumerate(inputFile):\n",
    "        sent = line.strip().split(',')[0]\n",
    "        clause = line.strip().split(',')[1]\n",
    "        keyword = line.strip().split(',')[2]\n",
    "        content = []\n",
    "        content = line.strip().split(',')[-1].split(' ')\n",
    "        sentAndClause.append([sent, clause])\n",
    "        keywords.append(keyword)\n",
    "        clauseTest.append(content)\n",
    "    print('sentAndClause', len(sentAndClause))  # the num of testing events\n",
    "    print('clauseTest', len(clauseTest))\n",
    "    print('keywords', len(keywords))\n",
    "    inputFile.close()\n",
    "    inputFile = open(os.path.join(data_path, 'prediction_{}_{}.csv'.format(n, m)), 'r')\n",
    "    scoreList = []\n",
    "    for index, item in enumerate(inputFile):\n",
    "        score = float(item.strip())\n",
    "        scoreList.append(score)\n",
    "    print('score', len(scoreList))\n",
    "    inputFile.close()\n",
    "    predict = dict()\n",
    "    maxScore = 0\n",
    "    for index, item in enumerate(sentAndClause):\n",
    "        cSent = int(item[0])\n",
    "        cClause = int(item[1])\n",
    "        score = scoreList[index]\n",
    "        if predict.get(cSent):\n",
    "            if score > maxScore:\n",
    "                predict[cSent] = cClause\n",
    "                maxScore = score\n",
    "        else:\n",
    "            predict[cSent] = cClause\n",
    "            maxScore = score\n",
    "    print('predict', len(predict))\n",
    "    predictRight = []\n",
    "    predSet = set()\n",
    "    for key, value in predict.items():\n",
    "        predictRight.append([key, value])\n",
    "        key = str(key) + ',' + str(value)\n",
    "        predSet.add(key)\n",
    "    print('predictRight', len(predictRight))\n",
    "    print('predSet', len(predSet))\n",
    "    return predictRight, predSet, sentAndClause, keywords, clauseTest\n",
    "\n",
    "\n",
    "def statistics(n, m, realRight, realSet, predictRight, predSet, sentAndClause, keywords, clauseTest, mode='train'):\n",
    "    data_path = '/home/wujipeng/git/ECA/ec/MemNet/' + 'data' if mode == 'train' else 'test_data'\n",
    "    suc = 0\n",
    "    sucSet = set()\n",
    "    for i in range(len(realRight)):\n",
    "        for j in range(len(predictRight)):\n",
    "            if realRight[i] == predictRight[j]:\n",
    "                suc += 1\n",
    "                key = str(realRight[i][0]) + ',' + str(realRight[i][1])\n",
    "                sucSet.add(key)\n",
    "    print('suc', suc)\n",
    "    precision = float(suc) / len(predictRight)\n",
    "    recall = float(suc) / len(realRight)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print('precision', precision)\n",
    "    print('recall', recall)\n",
    "    print('f1', f1)\n",
    "    print('****************************************')\n",
    "    inputFile = open(os.path.join(data_path, 'statistics_{}_{}.csv'.format(n, m)), 'r')\n",
    "    outputFile = open(os.path.join(data_path, 'statistics_final_{}_{}.csv'.format(n, m)), 'w')\n",
    "    for index, line in enumerate(inputFile):\n",
    "        if index == 0:\n",
    "            line = line.strip() + '\\tsentence\\tclause\\tpredict\\treal\\tsucc\\tkeyword\\tcontent\\n'\n",
    "        else:\n",
    "            line = line.strip() + '\\t' + str(sentAndClause[index - 1][0]) + '\\t' + str(\n",
    "                sentAndClause[index - 1][1]) + '\\t'\n",
    "            key = str(sentAndClause[index - 1][0]) + ',' + str(sentAndClause[index - 1][1])\n",
    "            if key in predSet:\n",
    "                line += 'c' + '\\t'\n",
    "            else:\n",
    "                line += 'n' + '\\t'\n",
    "            if key in realSet:\n",
    "                line += 'c' + '\\t'\n",
    "            else:\n",
    "                line += 'n' + '\\t'\n",
    "            if key in sucSet:\n",
    "                line += 'suc' + '\\t'\n",
    "            else:\n",
    "                line += '###' + '\\t'\n",
    "            line += keywords[index - 1] + '\\t' + ' '.join(clauseTest[index - 1]) + '\\n'\n",
    "        outputFile.write(line)\n",
    "    inputFile.close()\n",
    "    outputFile.close()\n",
    "    return precision, recall, f1\n",
    "\n",
    "import numpy as np\n",
    "def load_data(n, mode='train'):\n",
    "    data_path = '/home/wujipeng/git/ECA/ec/MemNet/' + 'data' if mode == 'train' else 'test_data'\n",
    "    testFile = open(os.path.join(data_path, 'emotion_cause_clause_level_test_{}.csv'.format(n)), 'r')\n",
    "    trainFile = open(os.path.join(data_path, 'emotion_cause_clause_level_train_{}.csv'.format(n)), 'r')\n",
    "    testData = []\n",
    "    testStory = []\n",
    "    for _, line in enumerate(testFile):\n",
    "        nid = line.strip().split(' ')[0]\n",
    "        nLine = line.strip().split(' ')[1:]\n",
    "        if nid == '1':\n",
    "            testStory = []\n",
    "            items = nLine[:]\n",
    "            testStory.append(items)\n",
    "        if '\\t' in nLine[-1]:\n",
    "            q = []\n",
    "            q.append(nLine[0])\n",
    "            q.append(nLine[1])\n",
    "            q.append(nLine[-1].split('\\t')[0])\n",
    "            a = [nLine[-1].split('\\t')[1]]\n",
    "            subStory = [x for x in testStory if x]\n",
    "            testData.append((subStory, q, a))\n",
    "        else:\n",
    "            items = nLine[:]\n",
    "            testStory.append(items)\n",
    "    testFile.close()\n",
    "    trainData = []\n",
    "    trainStory = []\n",
    "    for _, line in enumerate(trainFile):\n",
    "        nid = line.strip().split(' ')[0]\n",
    "        nLine = line.strip().split(' ')[1:]\n",
    "        if nid == '1':\n",
    "            trainStory = []\n",
    "            items = nLine[:]\n",
    "            trainStory.append(items)\n",
    "        if '\\t' in nLine[-1]:\n",
    "            q = []\n",
    "            q.append(nLine[0])\n",
    "            q.append(nLine[1])\n",
    "            q.append(nLine[-1].split('\\t')[0])\n",
    "            a = [nLine[-1].split('\\t')[1]]\n",
    "            subStory = [x for x in trainStory if x]\n",
    "            trainData.append((subStory, q, a))\n",
    "        else:\n",
    "            items = nLine[:]\n",
    "            trainStory.append(items)\n",
    "    trainFile.close()\n",
    "    return testData, trainData\n",
    "\n",
    "\n",
    "def vectorize_data(data, word_idx, memory_size, sentence_size):\n",
    "    S = []\n",
    "    Q = []\n",
    "    A = []\n",
    "    for story, query, answer in data:\n",
    "        ss = []\n",
    "        for index, sentence in enumerate(story):\n",
    "            ls = max(0, sentence_size - len(sentence))\n",
    "            ss.append([word_idx[w] for w in sentence] + [0] * ls)\n",
    "        # take only the most recent sentences that fit in memory\n",
    "        ss = ss[::-1][:memory_size][::-1]\n",
    "\n",
    "        # pad to memory_size\n",
    "        lm = max(0, memory_size - len(ss))\n",
    "        for _ in range(lm):\n",
    "            ss.append([0] * sentence_size)\n",
    "\n",
    "        q = [word_idx[w] for w in query]\n",
    "\n",
    "        a = np.zeros(2)  # the answer is yes or no\n",
    "        for an in answer:\n",
    "            if an == 'yes':\n",
    "                a[1] = 1\n",
    "            elif an == 'no':\n",
    "                a[0] = 1\n",
    "\n",
    "        S.append(ss)\n",
    "        Q.append(q)\n",
    "        A.append(a)\n",
    "    return np.array(S), np.array(Q), np.array(A)\n",
    "\n",
    "\n",
    "def create_embedding(vocab, mode='train'):\n",
    "    from gensim.models import KeyedVectors\n",
    "    print('读取预训练Embbeding')\n",
    "    word2vec = KeyedVectors.load_word2vec_format('/data/wujipeng/embedding/Wikipedia/sgns.wiki.word', binary=False)\n",
    "    dim = word2vec.vector_size\n",
    "    embedding = [np.zeros(dim)]  # pad\n",
    "    cnt = 0\n",
    "    for word in vocab:\n",
    "        if word2vec.vocab.get(word):\n",
    "            embedding.append(word2vec.get_vector(word))\n",
    "            cnt += 1\n",
    "        else:\n",
    "            embedding.append(np.random.normal(loc=0., scale=0.1, size=dim))\n",
    "    embedding = np.array(embedding)\n",
    "    print('Embedding shape', embedding.shape)\n",
    "    print('Embedding rate: {:.2f}%'.format(cnt / len(vocab) * 100))\n",
    "    return np.array(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T19:30:46.531297Z",
     "start_time": "2019-03-24T19:29:23.073266Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取预训练Embbeding\n",
      "Embedding shape (23150, 300)\n",
      "Embedding rate: 84.62%\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "print('读取预训练Embbeding')\n",
    "word2vec = KeyedVectors.load_word2vec_format('/data/wujipeng/embedding/Wikipedia/sgns.wiki.word', binary=False)\n",
    "dim = word2vec.vector_size\n",
    "embedding = [np.zeros(dim)]  # pad\n",
    "cnt = 0\n",
    "for word in vocab:\n",
    "    if word2vec.vocab.get(word):\n",
    "        embedding.append(word2vec.get_vector(word))\n",
    "        cnt += 1\n",
    "    else:\n",
    "        embedding.append(np.random.normal(loc=0., scale=0.1, size=dim))\n",
    "embedding = np.array(embedding)\n",
    "print('Embedding shape', embedding.shape)\n",
    "print('Embedding rate: {:.2f}%'.format(cnt / len(vocab) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T19:05:12.207054Z",
     "start_time": "2019-03-24T19:05:12.108756Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def load_data(n, mode='train'):\n",
    "    data_path = '/home/wujipeng/git/ECA/ec/MemNet/' + 'data' if mode == 'train' else 'test_data'\n",
    "    testFile = open(os.path.join('/home/wujipeng/git/ECA/ec/MemNet/', data_path, 'emotion_cause_clause_level_test_{}.csv'.format(n)), 'r')\n",
    "    trainFile = open(os.path.join('/home/wujipeng/git/ECA/ec/MemNet/', data_path, 'emotion_cause_clause_level_train_{}.csv'.format(n)), 'r')\n",
    "    testData = []\n",
    "    testStory = []\n",
    "    for _, line in enumerate(testFile):\n",
    "        nid = line.strip().split(' ')[0]\n",
    "        nLine = line.strip().split(' ')[1:]\n",
    "        if nid == '1':\n",
    "            testStory = []\n",
    "            items = nLine[:]\n",
    "            testStory.append(items)\n",
    "        if '\\t' in nLine[-1]:\n",
    "            q = []\n",
    "            q.append(nLine[0])\n",
    "            q.append(nLine[1])\n",
    "            q.append(nLine[-1].split('\\t')[0])\n",
    "            a = [nLine[-1].split('\\t')[1]]\n",
    "            subStory = [x for x in testStory if x]\n",
    "            testData.append((subStory, q, a))\n",
    "        else:\n",
    "            items = nLine[:]\n",
    "            testStory.append(items)\n",
    "    testFile.close()\n",
    "    trainData = []\n",
    "    trainStory = []\n",
    "    for _, line in enumerate(trainFile):\n",
    "        nid = line.strip().split(' ')[0]\n",
    "        nLine = line.strip().split(' ')[1:]\n",
    "        if nid == '1':\n",
    "            trainStory = []\n",
    "            items = nLine[:]\n",
    "            trainStory.append(items)\n",
    "        if '\\t' in nLine[-1]:\n",
    "            q = []\n",
    "            q.append(nLine[0])\n",
    "            q.append(nLine[1])\n",
    "            q.append(nLine[-1].split('\\t')[0])\n",
    "            a = [nLine[-1].split('\\t')[1]]\n",
    "            subStory = [x for x in trainStory if x]\n",
    "            trainData.append((subStory, q, a))\n",
    "        else:\n",
    "            items = nLine[:]\n",
    "            trainStory.append(items)\n",
    "    trainFile.close()\n",
    "    return testData, trainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T19:38:57.014025Z",
     "start_time": "2019-03-24T19:38:56.524602Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allClause 31296\n"
     ]
    }
   ],
   "source": [
    "posDict = statisticPos()\n",
    "strPosDict = changePos(posDict)\n",
    "allClause = extractAll()\n",
    "pre_all = 0.0\n",
    "rec_all = 0.0\n",
    "f1_all = 0.0\n",
    "pre_all_ranking = 0.0\n",
    "rec_all_ranking = 0.0\n",
    "f1_all_ranking = 0.0\n",
    "pre_all_list = []\n",
    "rec_all_list = []\n",
    "f1_all_list = []\n",
    "pre_all_list_ranking = []\n",
    "rec_all_list_ranking = []\n",
    "f1_all_list_ranking = []\n",
    "time = 1\n",
    "time_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-24T19:40:35.679931Z",
     "start_time": "2019-03-24T19:38:59.029791Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n",
      "********************************\n",
      "test_pos_count 218\n",
      "test_neg_count 2873\n",
      "train_pos_count 1949\n",
      "train_neg_count 26256\n",
      "********************************\n",
      "testClause(all) 3091\n",
      "realRight 218\n",
      "realSet 218\n",
      "读取预训练Embbeding\n",
      "Embedding shape (23150, 300)\n",
      "Embedding rate: 84.62%\n",
      "embedding (23150, 300)\n"
     ]
    }
   ],
   "source": [
    "testSentence = sampling()\n",
    "division(testSentence, time)\n",
    "construction(strPosDict, time)\n",
    "realRight, realSet = extractRealRight(allClause, time)\n",
    "\n",
    "test, train = load_data(time)\n",
    "data = test + train\n",
    "vocab = sorted(reduce(lambda x, y: x | y, (set(list(chain.from_iterable(s)) + q) for s, q, _ in data)))\n",
    "word_idx = dict((c, k + 1) for k, c in enumerate(vocab))\n",
    "\n",
    "embedding = create_embedding(vocab)\n",
    "print('embedding', embedding.shape)\n",
    "memory_size = max(map(len, (s for s, _, _ in data)))\n",
    "sentence_size = 3\n",
    "vocab_size = len(word_idx) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:53:24.781880Z",
     "start_time": "2019-03-21T14:53:24.640876Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def position_encoding(sentence_size, embedding_size):\n",
    "    encoding = np.ones((embedding_size, sentence_size), dtype=np.float32)\n",
    "    ls = sentence_size + 1\n",
    "    le = embedding_size + 1\n",
    "    for i in range(1, le):\n",
    "        for j in range(1, ls):\n",
    "            encoding[i - 1, j - 1] = (i - (le - 1) / 2) * (j - (ls - 1) / 2)\n",
    "    encoding = 1 + 4 * encoding / embedding_size / sentence_size\n",
    "    return np.transpose(encoding)\n",
    "\n",
    "class MemN2N(nn.Module): \n",
    "    def __init__(self, embedding, batch_size, vocab_size, sentence_size, memory_size, embedding_size,\n",
    "                 answer_size=2, hops=3, max_grad_norm=40.0, encoding=position_encoding, name='MemN2N', \n",
    "                dropout=0.9):\n",
    "        super(MemN2N, self).__init__()\n",
    "        self._batch_size = batch_size\n",
    "        self._vocab_size = vocab_size\n",
    "        self._sentence_size = sentence_size\n",
    "        self._memory_size = memory_size\n",
    "        self._embedding_size = embedding_size\n",
    "        self._answer_size = answer_size\n",
    "        self._hops = hops\n",
    "        self._max_grad_norm = max_grad_norm\n",
    "        self._name = name\n",
    "        self._dropout = dropout\n",
    "\n",
    "        self._embedding = embedding\n",
    "        \n",
    "        self._encoding = torch.tensor(encoding(1, 3 * self._embedding_size)).to(device) # (1, 60)\n",
    "        \n",
    "        self.embedding = nn.Embedding(self._vocab_size, self._embedding_size)\n",
    "        self.embedding.weight.data.copy_(torch.tensor(embedding))\n",
    "        self.dropout = nn.Dropout(self._dropout)\n",
    "        self.out = nn.Linear(3 * self._embedding_size, self._answer_size)\n",
    "        \n",
    "    \n",
    "\n",
    "    def forward(self, stories, queries):\n",
    "        q_emb0 = self.embedding(torch.tensor(queries).to(device)) # output_size: (16, 3, 300)\n",
    "        q_emb = q_emb0.view(-1, 1, 3 * self._embedding_size)\n",
    "        u_0 = torch.sum(q_emb * self._encoding, 1)\n",
    "        u = [u_0] # (1, 16, 60)\n",
    "\n",
    "        for i in range(self._hops):\n",
    "            m_emb0 = self.embedding(torch.tensor(stories).to(device)) # (16, 40, 3, 300)\n",
    "            m_emb = m_emb0.view(-1, self._memory_size, 1, 3 * self._embedding_size)\n",
    "            m = torch.sum(m_emb * self._encoding, 2) # (16, 40, 900)\n",
    "            \n",
    "            u_temp = u[-1].unsqueeze(2).transpose(1, 2) # (16, 900, 1) -> (16, 1, 900)\n",
    "            dotted = torch.sum(m * u_temp, 2) # (16, 40, 900) ->  # (16, 40)\n",
    "            probs = F.softmax(dotted, 1) # (16, 40)\n",
    "            probs_temp = probs.unsqueeze(2).transpose(1, 2) # (16, 40, 1) -> (16, 1, 40)\n",
    "            \n",
    "            c_emb0 = self.embedding(torch.tensor(stories).to(device)) # (16, 40, 3, 300)\n",
    "            c_emb = c_emb0.view(-1, self._memory_size, 1, 3 * self._embedding_size) # (16, 40, 1, 900)\n",
    "            c_temp = torch.sum(c_emb * self._encoding, 2) # (16, 40, 900)\n",
    "            c = c_temp.transpose(1, 2) # (16, 900, 40)\n",
    "\n",
    "            o_k = torch.sum(c * probs_temp, 2) # (16, 900)\n",
    "            u_k = u[-1] + o_k # (16, 900)\n",
    "            u.append(u_k)\n",
    "        return self.dropout(self.out(u_k)) # (16, 2)\n",
    "    \n",
    "\n",
    "    def predict_log_proba(self, stories, queries):\n",
    "        feed_dict = {self._stories: stories, self._queries: queries, self._dropout: 1.0}\n",
    "        return self._sess.run(self._predict_log_proba_op, feed_dict=feed_dict)\n",
    "    \n",
    "    def gradient_noise_and_clip(self, parameters,\n",
    "                                 noise_stddev=1e-3, max_clip=40.0):\n",
    "        parameters = list(filter(lambda p: p.grad is not None, parameters))\n",
    "        nn.utils.clip_grad_norm_(parameters, max_clip)\n",
    "\n",
    "        for p in parameters:\n",
    "            noise = torch.randn(p.size()) * noise_stddev\n",
    "            noise = noise.to(device)\n",
    "            p.grad.data.add_(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:55:54.009881Z",
     "start_time": "2019-03-21T14:55:54.006002Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mode = 'train'\n",
    "data_path = '/home/wujipeng/git/ECA/ec/MemNet/' + 'data' if mode == 'train' else 'test_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:55:55.122497Z",
     "start_time": "2019-03-21T14:55:55.119822Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T14:54:52.267567Z",
     "start_time": "2019-03-21T14:53:40.957669Z"
    },
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n",
      "********************************\n",
      "test_pos_count 218\n",
      "test_neg_count 2998\n",
      "train_pos_count 1949\n",
      "train_neg_count 26131\n",
      "********************************\n",
      "testClause(all) 3216\n",
      "realRight 218\n",
      "realSet 218\n",
      "the temp_emb 15736\n",
      "the rate of embedding 0.6242025418194604\n",
      "embedding (19908, 20)\n",
      "memory_size 40\n",
      "sentence_size 3\n",
      "vocab_size 19908\n",
      "testS.shape (3216, 40, 3)\n",
      "testQ.shape (3216, 3)\n",
      "testA.shape (3216, 2)\n",
      "trainS.shape (28080, 40, 3)\n",
      "trainQ.shape (28080, 3)\n",
      "trainA.shape (28080, 2)\n",
      "n_train 28080\n",
      "n_test 3216\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/prediction_0_1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-212bd178ea1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0moutputFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/prediction_{}_{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0moutputFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/prediction_0_1.csv'"
     ]
    }
   ],
   "source": [
    "while (True):    \n",
    "    if (time > 10):\n",
    "        break\n",
    "    testSentence = sampling()\n",
    "    division(testSentence, time)\n",
    "    construction(strPosDict, time)\n",
    "    realRight, realSet = extractRealRight(allClause, time)\n",
    "\n",
    "    test, train = load_data(time)\n",
    "    data = test + train\n",
    "    vocab = sorted(reduce(lambda x, y: x | y, (set(list(chain.from_iterable(s)) + q) for s, q, _ in data)))\n",
    "    word_idx = dict((c, k + 1) for k, c in enumerate(vocab))\n",
    "\n",
    "    embedding = create_embedding(vocab, embedding_size)\n",
    "    print('embedding', embedding.shape)\n",
    "    memory_size = max(map(len, (s for s, _, _ in data)))\n",
    "    sentence_size = 3\n",
    "    vocab_size = len(word_idx) + 1\n",
    "    print('memory_size', memory_size)\n",
    "    print('sentence_size', sentence_size)\n",
    "    print('vocab_size', vocab_size)\n",
    "\n",
    "    trainS, trainQ, trainA = vectorize_data(train, word_idx, memory_size, sentence_size)\n",
    "    testS, testQ, testA = vectorize_data(test, word_idx, memory_size, sentence_size)\n",
    "    print('testS.shape', testS.shape)\n",
    "    print('testQ.shape', testQ.shape)\n",
    "    print('testA.shape', testA.shape)\n",
    "    print('trainS.shape', trainS.shape)\n",
    "    print('trainQ.shape', trainQ.shape)\n",
    "    print('trainA.shape', trainA.shape)\n",
    "\n",
    "    n_train = trainS.shape[0]\n",
    "    n_test = testS.shape[0]\n",
    "    print('n_train', n_train)\n",
    "    print('n_test', n_test)\n",
    "\n",
    "    train_labels = np.argmax(trainA, axis=1)\n",
    "    test_labels = np.argmax(testA, axis=1)\n",
    "\n",
    "    torch.cuda.manual_seed(random_state)\n",
    "    batch_size = 16\n",
    "\n",
    "    batches = zip(range(0, n_train - batch_size, batch_size), range(batch_size, n_train, batch_size))\n",
    "    batches = [(start, end) for start, end in batches]\n",
    "\n",
    "    model = MemN2N(embedding, batch_size, vocab_size, sentence_size, memory_size, \n",
    "                   embedding_size,hops=hops, max_grad_norm=max_grad_norm, dropout=0.1)\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "    pre_list = []\n",
    "    rec_list = []\n",
    "    f1_list = []\n",
    "    pre_ranking_list = []\n",
    "    rec_ranking_list = []\n",
    "    f1_ranking_list = []\n",
    "    for t in range(1, epochs + 1):\n",
    "        np.random.shuffle(batches)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        model.train()\n",
    "        total_cost = 0.0\n",
    "        for start, end in batches:\n",
    "            loss = 0\n",
    "            optimizer.zero_grad()\n",
    "            s = trainS[start:end]\n",
    "            q = trainQ[start:end]\n",
    "            a = trainA[start:end]\n",
    "            output = model(s, q)\n",
    "            labels = torch.tensor(np.argmax(a, axis=1)).to(device)\n",
    "            loss = criterion(output, labels)\n",
    "            total_cost += loss.item()\n",
    "            loss.backward()\n",
    "            # Clip gradients: gradients are modified in place\n",
    "            model.gradient_noise_and_clip(model.parameters(),\n",
    "                    noise_stddev=1e-3, max_clip=max_grad_norm)\n",
    "            # _ = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            # grads_and_vars = [(add_gradient_noise(g), v) for g, v in grads_and_vars] # Todo\n",
    "\n",
    "            # Adjust model weights\n",
    "            optimizer.step()\n",
    "\n",
    "        if t % evaluation_interval == 0:\n",
    "            train_preds = []\n",
    "            model.eval()\n",
    "            for start in range(0, n_train, batch_size):\n",
    "                end = start + batch_size\n",
    "                s = trainS[start:end]\n",
    "                q = trainQ[start:end]\n",
    "                out = model(s, q)\n",
    "                pred = torch.argmax(out, 1)\n",
    "                train_preds += pred.tolist()\n",
    "            train_acc = metrics.accuracy_score(train_labels, np.array(train_preds))\n",
    "\n",
    "            test_out = model(testS, testQ)\n",
    "            test_preds = torch.argmax(test_out, 1)\n",
    "            test_results = torch.softmax(test_out, 1)\n",
    "\n",
    "            outputFile = open(os.path.join(data_path, 'prediction_{}_{}.csv'.format(time, t)), 'w')\n",
    "            for e in range(len(test_results)):\n",
    "                outputFile.write(str(test_results[e][1].item()) + '\\n')\n",
    "            outputFile.close()\n",
    "\n",
    "            test_acc = metrics.accuracy_score(test_labels.tolist(), test_preds.tolist())\n",
    "            test_pre = metrics.precision_score(test_labels.tolist(), test_preds.tolist())\n",
    "            test_rec = metrics.recall_score(test_labels.tolist(), test_preds.tolist())\n",
    "            test_f1 = metrics.f1_score(test_labels.tolist(), test_preds.tolist())\n",
    "            pre_list.append(test_pre)\n",
    "            rec_list.append(test_rec)\n",
    "            f1_list.append(test_f1)\n",
    "\n",
    "            print('****************************************')\n",
    "            print('-----------------------')\n",
    "            print('The time', time)\n",
    "            print('Epoch', t)\n",
    "            print('Total Cost:', total_cost)\n",
    "            print('Training Accuracy:', train_acc)\n",
    "            print('Testing Accuracy:', test_acc)\n",
    "            print('Testing Precision:', test_pre)\n",
    "            print('Testing Recall:', test_rec)\n",
    "            print('Testing F1:', test_f1)\n",
    "            print('-----------------------')\n",
    "\n",
    "            outputFile = open('/statistics_{}_{}.csv'.format(time, t), 'w')\n",
    "            outputFile.write('SUCC\\ttest_labels\\ttest_preds\\ttest_results[0]\\ttest_results[1]\\n')\n",
    "            labelcount = 0\n",
    "            predcount = 0\n",
    "            suc = 0\n",
    "            for j in range(n_test):\n",
    "                if test_labels[j].item() == 1:\n",
    "                    labelcount += 1\n",
    "                if test_preds[j].item() == 1:\n",
    "                    predcount += 1\n",
    "                if (test_labels[j].item() == 1) and (test_preds[j].item() == 1):\n",
    "                    suc += 1\n",
    "                    text = 'SUC\\t'\n",
    "                else:\n",
    "                    text = '###\\t'\n",
    "                text += str(test_labels[j].item()) + '\\t' + str(test_preds[j].item()) + '\\t' + str(\n",
    "                    test_results[j][0].item()) + '\\t' + str(test_results[j][1].item()) + '\\n'\n",
    "                outputFile.write(text)\n",
    "            outputFile.close()\n",
    "            print('test_labels', labelcount)\n",
    "            print('test_pred', predcount)\n",
    "            print('suc', suc)\n",
    "            print('****************************************')\n",
    "\n",
    "            predictRight, predSet, sentAndClause, keywords, clauseTest = extractPredRight(time, t)\n",
    "            pre_ranking, rec_ranking, f1_ranking = statistics(time, t, realRight, realSet, predictRight,\n",
    "                                                              predSet, sentAndClause, keywords, clauseTest)\n",
    "            pre_ranking_list.append(pre_ranking)\n",
    "            rec_ranking_list.append(rec_ranking)\n",
    "            f1_ranking_list.append(f1_ranking)\n",
    "    maxScore, maxIndex = maxS(f1_ranking_list)\n",
    "    print('#########################################')\n",
    "    pre_all += pre_list[maxIndex]\n",
    "    rec_all += rec_list[maxIndex]\n",
    "    f1_all += f1_list[maxIndex]\n",
    "    pre_all_ranking += pre_ranking_list[maxIndex]\n",
    "    rec_all_ranking += rec_ranking_list[maxIndex]\n",
    "    f1_all_ranking += maxScore\n",
    "    print('The time', time)\n",
    "    print('pre', pre_list[maxIndex])\n",
    "    print('rec', rec_list[maxIndex])\n",
    "    print('f1', f1_list[maxIndex])\n",
    "    print('pre_ranking', pre_ranking_list[maxIndex])\n",
    "    print('rec_ranking', rec_ranking_list[maxIndex])\n",
    "    print('f1_ranking', maxScore)\n",
    "    pre_all_list.append(pre_list[maxIndex])\n",
    "    rec_all_list.append(rec_list[maxIndex])\n",
    "    f1_all_list.append(f1_list[maxIndex])\n",
    "    pre_all_list_ranking.append(pre_ranking_list[maxIndex])\n",
    "    rec_all_list_ranking.append(rec_ranking_list[maxIndex])\n",
    "    f1_all_list_ranking.append(maxScore)\n",
    "    time_list.append(time)\n",
    "    print('#########################################')\n",
    "    time += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:23:07.382471Z",
     "start_time": "2019-03-26T14:23:07.368535Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n"
     ]
    }
   ],
   "source": [
    "def sampling():\n",
    "    numOfSentence = 2105\n",
    "    testSentence = set()\n",
    "    for i in range(1, numOfSentence + 1):\n",
    "        if random.random() > 0.9:\n",
    "            testSentence.add(i)\n",
    "    rate = float(len(testSentence)) / numOfSentence\n",
    "    #    print(rate)\n",
    "    if rate > 0.1 and rate - 0.1 < 0.0003:\n",
    "        print(len(testSentence))\n",
    "        return testSentence\n",
    "    else:\n",
    "        return sampling()\n",
    "testSentence = sampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:33:26.639343Z",
     "start_time": "2019-03-26T14:33:26.628902Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time = 'test'\n",
    "testSentence = set(map(int, [line.split(',')[0] for line in open('/data/wujipeng/ec/data/test/val_set.txt').readlines()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:24:18.995980Z",
     "start_time": "2019-03-26T14:24:10.860838Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************\n",
      "test_pos_count 220\n",
      "test_neg_count 3007\n",
      "train_pos_count 1947\n",
      "train_neg_count 26122\n",
      "********************************\n"
     ]
    }
   ],
   "source": [
    "def division(testSentence, n, mode='train'):\n",
    "    data_path = '/home/wujipeng/git/ECA/ec/MemNet/' + 'data' if mode == 'train' else 'test_data'\n",
    "    testSentenceFile = open(os.path.join(data_path, 'testSentenceFile_{}.csv'.format(n)), 'w')\n",
    "    for index, item in enumerate(testSentence):\n",
    "        testSentenceFile.write(str(item) + '\\n')\n",
    "    testSentenceFile.close()\n",
    "\n",
    "    inputFile = open(os.path.join(data_path, 'datacsv_2105.csv'), 'r')\n",
    "    outputFile1 = open(os.path.join(data_path, 'clause_test_{}.csv'.format(n)), 'w')\n",
    "    outputFile2 = open(os.path.join(data_path, 'clause_train_{}.csv'.format(n)), 'w')\n",
    "    test_pos_count, test_neg_count, train_pos_count, train_neg_count = 0, 0, 0, 0\n",
    "    for _, line in enumerate(inputFile):\n",
    "        sentenceID = int(line.strip().split(',')[0])\n",
    "        sentence = line.strip().split(',')[-1]\n",
    "        keyword = None\n",
    "        keyPos = -1\n",
    "        clauseList = re.split('，|。|？|！|；|……', sentence)\n",
    "        causeOfSent = set()\n",
    "        for index, item in enumerate(clauseList):\n",
    "            match = re.search(r'\\[f\\]([^\\[]*)\\[/f\\]', item)\n",
    "            if match:\n",
    "                keyword = match.group(1)\n",
    "                keyPos = index\n",
    "            match = re.search(r'\\[\\d[nv]\\][^\\[]*\\[/\\d[nv]\\]', item)\n",
    "            if match:\n",
    "                causeOfSent.add(index)\n",
    "            match = re.search(r'\\[[-\\d]*\\*\\d[nv]\\][^\\[]*\\[/[-\\d]*\\*\\d[nv]\\]', item)\n",
    "            if match:\n",
    "                causeOfSent.add(index)\n",
    "        for index, item in enumerate(clauseList):\n",
    "            clause = re.sub(r'\\[[^\\[]*\\]', '', item)\n",
    "            clause = re.sub(r'“', '', clause)\n",
    "            clause = re.sub(r'”', '', clause)\n",
    "            clause = re.sub(r'：', '', clause)\n",
    "            clause = re.sub(r'\\(', '', clause)\n",
    "            clause = re.sub(r'\\)', '', clause)\n",
    "            clause = re.sub(r'、', '', clause)\n",
    "            clause = re.sub(r'’', '', clause)\n",
    "            clause = re.sub(r'‘', '', clause)\n",
    "            clause = re.sub(r'》', '', clause)\n",
    "            clause = re.sub(r'《', '', clause)\n",
    "            clause = re.sub(r'~', '', clause)\n",
    "            words = hanlp_cut(clause)\n",
    "            pos = index - keyPos\n",
    "            if (sentenceID in testSentence) and (clause.split()):\n",
    "                if index in causeOfSent:\n",
    "                    test_pos_count += 1\n",
    "                    outputFile1.write(str(sentenceID) + ',' + str(index + 1) + ',' + keyword + ',')\n",
    "                    outputFile1.write(str(pos) + ',yes,' + ' '.join(words) + '\\n')\n",
    "                else:\n",
    "                    test_neg_count += 1\n",
    "                    outputFile1.write(str(sentenceID) + ',' + str(index + 1) + ',' + keyword + ',')\n",
    "                    outputFile1.write(str(pos) + ',no,' + ' '.join(words) + '\\n')\n",
    "            elif (sentenceID not in testSentence) and (clause.split()):\n",
    "                if index in causeOfSent:\n",
    "                    train_pos_count += 1\n",
    "                    outputFile2.write(str(sentenceID) + ',' + str(index + 1) + ',' + keyword + ',')\n",
    "                    outputFile2.write(str(pos) + ',yes,' + ' '.join(words) + '\\n')\n",
    "                else:\n",
    "                    train_neg_count += 1\n",
    "                    outputFile2.write(str(sentenceID) + ',' + str(index + 1) + ',' + keyword + ',')\n",
    "                    outputFile2.write(str(pos) + ',no,' + ' '.join(words) + '\\n')\n",
    "    print('********************************')\n",
    "    print('test_pos_count', test_pos_count)\n",
    "    print('test_neg_count', test_neg_count)\n",
    "    print('train_pos_count', train_pos_count)\n",
    "    print('train_neg_count', train_neg_count)\n",
    "    print('********************************')\n",
    "    inputFile.close()\n",
    "    outputFile1.close()\n",
    "    outputFile2.close()\n",
    "division(testSentence, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:25:17.647361Z",
     "start_time": "2019-03-26T14:25:17.559883Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def statisticPos(mode='train'):\n",
    "    data_path = '/home/wujipeng/git/ECA/ec/MemNet/' + 'data' if mode == 'train' else 'test_data'\n",
    "    inputFile = open(os.path.join(data_path, 'datacsv_2105.csv'), 'r')\n",
    "    posDict = dict()\n",
    "    for _, line in enumerate(inputFile):\n",
    "        content = line.strip().split(',')[-1]\n",
    "        clauseList = re.split('，|。|？|！|；|……', content)\n",
    "        keyPos = -1\n",
    "        for index, item in enumerate(clauseList):\n",
    "            match = re.search(r'\\[f\\]([^\\[]*)\\[/f\\]', item)\n",
    "            if match:\n",
    "                keyPos = index\n",
    "        for index, item in enumerate(clauseList):\n",
    "            pos = index - keyPos\n",
    "            if posDict.get(pos):\n",
    "                posDict[pos] += 1\n",
    "            else:\n",
    "                posDict[pos] = 1\n",
    "    inputFile.close()\n",
    "    return posDict\n",
    "posDict = statisticPos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:28:40.348543Z",
     "start_time": "2019-03-26T14:28:40.325496Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def changePos(posDict):\n",
    "    count = 0\n",
    "    countDict = dict()\n",
    "    for key, value in posDict.items():\n",
    "        countDict[key] = count\n",
    "        count += 1\n",
    "    posList = ['AAAA', 'AAAB', 'AAAC', 'AAAD', 'AABA', 'AABB', 'AABC', 'AABD', 'AACA', 'AACB', 'AACC', 'AACD',\n",
    "               'AADA', 'AADB', 'AADC', 'AADD', 'ABAA', 'ABAB', 'ABAC', 'ABAD', 'ABBA', 'ABBB', 'ABBC', 'ABBD',\n",
    "               'ABCA', 'ABCB', 'ABCC', 'ABCD', 'ABDA', 'ABDB', 'ABDC', 'ABDD', 'ACAA', 'ACAB', 'ACAC', 'ACAD',\n",
    "               'ACBA', 'ACBB', 'ACBC', 'ACBD', 'ACCA', 'ACCB', 'ACCC', 'ACCD', 'ACDA', 'ACDB', 'ACDC', 'ACDD',\n",
    "               'ADAA', 'ADAB', 'ADAC', 'ADAD', 'ADBA', 'ADBB', 'ADBC', 'ADBD', 'ADCA', 'ADCB', 'ADCC', 'ADCD',\n",
    "               'ADDA', 'ADDB', 'ADDC', 'ADDD', 'BAAA', 'BAAB', 'BAAC', 'BAAD', 'BABA', 'BABB', 'BABC', 'BABD',\n",
    "               'BACA', 'BACB', 'BACC', 'BACD', 'BADA', 'BADB', 'BADC', 'BADD', 'BBAA', 'BBAB', 'BBAC', 'BBAD',\n",
    "               'BBBA', 'BBBB', 'BBBC', 'BBBD', 'BBCA', 'BBCB', 'BBCC', 'BBCD', 'BCAA', 'BCAB', 'BCAC', 'BCAD',\n",
    "               'BDAA', 'BDAB', 'BDAC', 'BDAD', 'BCBA', 'BCBB', 'BCBC']\n",
    "    strPosDict = dict()\n",
    "    for key, value in countDict.items():\n",
    "        strPosDict[key] = posList[value]\n",
    "    return strPosDict\n",
    "strPosDict = changePos(posDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:32:46.090966Z",
     "start_time": "2019-03-26T14:32:43.432496Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allClause 31296\n"
     ]
    }
   ],
   "source": [
    "def extractAll(mode='train'):\n",
    "    data_path = '/home/wujipeng/git/ECA/ec/MemNet/' + 'data' if mode == 'train' else 'test_data'\n",
    "    inputFile = open(os.path.join(data_path, 'datacsv_2105.csv'), 'r')\n",
    "    allClause = []\n",
    "    for index, line in enumerate(inputFile):\n",
    "        sent = int(line.strip().split(',')[0])  # int\n",
    "        content = line.strip().split(',')[-1]\n",
    "        clauseList = re.split('，|。|？|！|；|……', content)\n",
    "        for index, clause in enumerate(clauseList):\n",
    "            cause = 'n'\n",
    "            match = re.search(r'\\[\\d[nv]\\][^\\[]*\\[/\\d[nv]\\]', clause)\n",
    "            if match:\n",
    "                cause = 'c'\n",
    "            match = re.search(r'\\[[-\\d]*\\*\\d[nv]\\][^\\[]*\\[/[-\\d]*\\*\\d[nv]\\]', clause)\n",
    "            if match:\n",
    "                cause = 'c'\n",
    "            clause = re.sub(r'\\[[^\\[]*\\]', '', clause)\n",
    "            clause = re.sub(r'“', '', clause)\n",
    "            clause = re.sub(r'”', '', clause)\n",
    "            clause = re.sub(r'：', '', clause)\n",
    "            clause = re.sub(r'\\(', '', clause)\n",
    "            clause = re.sub(r'\\)', '', clause)\n",
    "            clause = re.sub(r'、', '', clause)\n",
    "            clause = re.sub(r'’', '', clause)\n",
    "            clause = re.sub(r'‘', '', clause)\n",
    "            clause = re.sub(r'》', '', clause)\n",
    "            clause = re.sub(r'《', '', clause)\n",
    "            if clause.split():\n",
    "                allClause.append([sent, index + 1, cause])\n",
    "    print('allClause', len(allClause))\n",
    "    inputFile.close()\n",
    "    return allClause\n",
    "allClause = extractAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:35:45.433923Z",
     "start_time": "2019-03-26T14:35:44.844954Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def construction(strPosDict, n, mode='train'):\n",
    "    data_path = '/home/wujipeng/git/ECA/ec/MemNet/' + 'data' if mode == 'train' else 'test_data'\n",
    "    inputFile1 = open(os.path.join(data_path, 'clause_test_{}.csv'.format(n)), 'r')\n",
    "    inputFile2 = open(os.path.join(data_path, 'clause_train_{}.csv'.format(n)), 'r')\n",
    "    outputFile1 = open(os.path.join(data_path, 'emotion_cause_clause_level_test_{}.csv'.format(n)), 'w')\n",
    "    outputFile2 = open(os.path.join(data_path, 'emotion_cause_clause_level_train_{}.csv'.format(n)), 'w')\n",
    "    for _, line in enumerate(inputFile1):\n",
    "        keyword = line.strip().split(',')[2]\n",
    "        position = int(line.strip().split(',')[3])\n",
    "        posStr = strPosDict[position]\n",
    "        label = line.strip().split(',')[4]\n",
    "        clause = line.strip().split(',')[5]\n",
    "        wordList = clause.strip().split(' ')\n",
    "        phraseList = []\n",
    "        if len(wordList) >= 3:\n",
    "            window = 3\n",
    "            begin = 0\n",
    "            for index, item in enumerate(wordList):\n",
    "                end = begin + window\n",
    "                if end <= len(wordList):\n",
    "                    phraseList.append(wordList[begin: end])\n",
    "                begin += 1\n",
    "        else:\n",
    "            phraseList.append(wordList[:])\n",
    "        lineNum = 1\n",
    "        for index, item in enumerate(phraseList):\n",
    "            outputFile1.write(str(lineNum))\n",
    "            for i in range(len(item)):\n",
    "                outputFile1.write(' ' + item[i])\n",
    "            outputFile1.write('\\n')\n",
    "            lineNum += 1\n",
    "        outputFile1.write(str(lineNum) + ' ' + posStr + ' ' + posStr + ' ' + posStr + '\\n')\n",
    "        lineNum += 1\n",
    "        outputFile1.write(str(lineNum) + ' ' + keyword + ' ' + keyword + ' ' + keyword + '\\t' + label + '\\n')\n",
    "    inputFile1.close()\n",
    "    outputFile1.close()\n",
    "    for _, line in enumerate(inputFile2):\n",
    "        keyword = line.strip().split(',')[2]\n",
    "        position = int(line.strip().split(',')[3])\n",
    "        posStr = strPosDict[position]\n",
    "        label = line.strip().split(',')[4]\n",
    "        clause = line.strip().split(',')[5]\n",
    "        wordList = clause.strip().split(' ')\n",
    "        phraseList = []\n",
    "        if len(wordList) >= 3:\n",
    "            window = 3\n",
    "            begin = 0\n",
    "            for index, item in enumerate(wordList):\n",
    "                end = begin + window\n",
    "                if end <= len(wordList):\n",
    "                    phraseList.append(wordList[begin: end])\n",
    "                begin += 1\n",
    "        else:\n",
    "            phraseList.append(wordList[:])\n",
    "        lineNum = 1\n",
    "        for index, item in enumerate(phraseList):\n",
    "            outputFile2.write(str(lineNum))\n",
    "            for i in range(len(item)):\n",
    "                outputFile2.write(' ' + item[i])\n",
    "            outputFile2.write('\\n')\n",
    "            lineNum += 1\n",
    "        outputFile2.write(str(lineNum) + ' ' + posStr + ' ' + posStr + ' ' + posStr + '\\n')\n",
    "        lineNum += 1\n",
    "        outputFile2.write(str(lineNum) + ' ' + keyword + ' ' + keyword + ' ' + keyword + '\\t' + label + '\\n')\n",
    "    inputFile2.close()\n",
    "    outputFile2.close()\n",
    "construction(strPosDict, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:48:46.468663Z",
     "start_time": "2019-03-26T14:48:46.425361Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testClause(all) 3227\n",
      "realRight 220\n",
      "realSet 220\n"
     ]
    }
   ],
   "source": [
    "def extractRealRight(allClause, n, mode='train'):\n",
    "    data_path = '/home/wujipeng/git/ECA/ec/MemNet/' + 'data' if mode == 'train' else 'test_data'\n",
    "    testSentenceFile = open(os.path.join(data_path, 'testSentenceFile_{}.csv'.format(n)), 'r')\n",
    "    testSentence = set()\n",
    "    for index, item in enumerate(testSentenceFile):\n",
    "        item = int(item.strip())\n",
    "        testSentence.add(item)\n",
    "    testSentenceFile.close()\n",
    "    testClause = []\n",
    "    for index, item in enumerate(allClause):\n",
    "        sent = allClause[index][0]\n",
    "        if sent in testSentence:\n",
    "            testClause.append(item)\n",
    "    print('testClause(all)', len(testClause))\n",
    "    realRight = []\n",
    "    realSet = set()\n",
    "    for index, item in enumerate(testClause):\n",
    "        cSent = item[0]\n",
    "        cClause = item[1]\n",
    "        cCause = item[2]\n",
    "        if cCause == 'c':\n",
    "            realdef extractRealRight(allClause, n, mode='train'):\n",
    "    data_path = '/home/wujipeng/git/ECA/ec/MemNet/' + 'data' if mode == 'train' else 'test_data'\n",
    "    testSentenceFile = open(os.path.join(data_path, 'testSentenceFile_{}.csv'.format(n)), 'r')\n",
    "    testSentence = set()\n",
    "    for index, item in enumerate(testSentenceFile):\n",
    "        item = int(item.strip())\n",
    "        testSentence.add(item)\n",
    "    testSentenceFile.close()\n",
    "    testClause = []\n",
    "    for index, item in enumerate(allClause):\n",
    "        sent = allClause[index][0]\n",
    "        if sent in testSentence:\n",
    "            testClause.append(item)\n",
    "    print('testClause(all)', len(testClause))\n",
    "    realRight = []\n",
    "    realSet = set()\n",
    "    for index, item in enumerate(testClause):\n",
    "        cSent = item[0]\n",
    "        cClause = item[1]\n",
    "        cCause = item[2]\n",
    "        if cCause == 'c':\n",
    "            realRight.append([cSent, cClause])\n",
    "            key = str(cSent) + ',' + str(cClause)\n",
    "            realSet.add(key)\n",
    "    print('realRight', len(realRight))\n",
    "    print('realSet', len(realSet))\n",
    "    return realRight, realSetRight.append([cSent, cClause])\n",
    "            key = str(cSent) + ',' + str(cClause)\n",
    "            realSet.add(key)\n",
    "    print('realRight', len(realRight))\n",
    "    print('realSet', len(realSet))\n",
    "    return realRight, realSet\n",
    "realRight, realSet = extractRealRight(allClause, time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T14:57:50.323201Z",
     "start_time": "2019-03-26T14:57:35.415406Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def load_data(n, mode='train'):\n",
    "    data_path = '/home/wujipeng/git/ECA/ec/MemNet/' + 'data' if mode == 'train' else 'test_data'\n",
    "    testFile = open(os.path.join('/home/wujipeng/git/ECA/ec/MemNet/', data_path, 'emotion_cause_clause_level_test_{}.csv'.format(n)), 'r')\n",
    "    trainFile = open(os.path.join('/home/wujipeng/git/ECA/ec/MemNet/', data_path, 'emotion_cause_clause_level_train_{}.csv'.format(n)), 'r')\n",
    "    testData = []\n",
    "    testStory = []\n",
    "    for _, line in enumerate(testFile):\n",
    "        nid = line.strip().split(' ')[0]\n",
    "        nLine = line.strip().split(' ')[1:]\n",
    "        if nid == '1':\n",
    "            testStory = []\n",
    "            items = nLine[:]\n",
    "            testStory.append(items)\n",
    "        if '\\t' in nLine[-1]:\n",
    "            q = []\n",
    "            q.append(nLine[0])\n",
    "            q.append(nLine[1])\n",
    "            q.append(nLine[-1].split('\\t')[0])\n",
    "            a = [nLine[-1].split('\\t')[1]]\n",
    "            subStory = [x for x in testStory if x]\n",
    "            testData.append((subStory, q, a))\n",
    "        else:\n",
    "            items = nLine[:]\n",
    "            testStory.append(items)\n",
    "    testFile.close()\n",
    "    trainData = []\n",
    "    trainStory = []\n",
    "    for _, line in enumerate(trainFile):\n",
    "        nid = line.strip().split(' ')[0]\n",
    "        nLine = line.strip().split(' ')[1:]\n",
    "        if nid == '1':\n",
    "            trainStory = []\n",
    "            items = nLine[:]\n",
    "            trainStory.append(items)\n",
    "        if '\\t' in nLine[-1]:\n",
    "            q = []\n",
    "            q.append(nLine[0])\n",
    "            q.append(nLine[1])\n",
    "            q.append(nLine[-1].split('\\t')[0])\n",
    "            a = [nLine[-1].split('\\t')[1]]\n",
    "            subStory = [x for x in trainStory if x]\n",
    "            trainData.append((subStory, q, a))\n",
    "        else:\n",
    "            items = nLine[:]\n",
    "            trainStory.append(items)\n",
    "    trainFile.close()\n",
    "    return testData, trainData\n",
    "test, train = load_data(time)\n",
    "data = test + train\n",
    "vocab = sorted(reduce(lambda x, y: x | y, (set(list(chain.from_iterable(s)) + q) for s, q, _ in data)))\n",
    "word_idx = dict((c, k + 1) for k, c in enumerate(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:28:39.681455Z",
     "start_time": "2019-03-26T15:27:11.727215Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取预训练Embbeding\n",
      "Embedding shape (23150, 300)\n",
      "Embedding rate: 84.62%\n",
      "embedding (23150, 300)\n"
     ]
    }
   ],
   "source": [
    "def create_embedding(vocab, mode='train'):\n",
    "    from gensim.models import KeyedVectors\n",
    "    print('读取预训练Embbeding')\n",
    "    word2vec = KeyedVectors.load_word2vec_format('/data/wujipeng/embedding/Wikipedia/sgns.wiki.word', binary=False)\n",
    "    dim = word2vec.vector_size\n",
    "    embedding = [np.zeros(dim)]  # pad\n",
    "    cnt = 0\n",
    "    for word in vocab:\n",
    "        if word2vec.vocab.get(word):\n",
    "            embedding.append(word2vec.get_vector(word))\n",
    "            cnt += 1\n",
    "        else:\n",
    "            embedding.append(np.random.normal(loc=0., scale=0.1, size=dim))\n",
    "    embedding = np.array(embedding)\n",
    "    print('Embedding shape', embedding.shape)\n",
    "    print('Embedding rate: {:.2f}%'.format(cnt / len(vocab) * 100))\n",
    "    return np.array(embedding)\n",
    "embedding = create_embedding(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:29:54.886844Z",
     "start_time": "2019-03-26T15:29:54.883107Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23149"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:25:32.383269Z",
     "start_time": "2019-03-26T15:25:30.676134Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取预训练Embbeding\n",
      "Embedding shape (23150, 30)\n",
      "Embedding rate: 74.07%\n",
      "embedding (23150, 30)\n"
     ]
    }
   ],
   "source": [
    "def create_embedding(vocab, mode='train'):\n",
    "    from gensim.models import KeyedVectors\n",
    "    print('读取预训练Embbeding')\n",
    "    word2vec = KeyedVectors.load_word2vec_format('/data/wujipeng/ec/data/embedding/vec_new.txt', binary=False)\n",
    "    dim = word2vec.vector_size\n",
    "    embedding = [np.zeros(dim)]  # pad\n",
    "    cnt = 0\n",
    "    for word in vocab:\n",
    "        if word2vec.vocab.get(word):\n",
    "            embedding.append(word2vec.get_vector(word))\n",
    "            cnt += 1\n",
    "        else:\n",
    "            embedding.append(np.random.normal(loc=0., scale=0.1, size=dim))\n",
    "    embedding = np.array(embedding)\n",
    "    print('Embedding shape', embedding.shape)\n",
    "    print('Embedding rate: {:.2f}%'.format(cnt / len(vocab) * 100))\n",
    "    return np.array(embedding)\n",
    "embedding = create_embedding(vocab)\n",
    "print('embedding', embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:22:22.172681Z",
     "start_time": "2019-03-26T15:22:22.048790Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取预训练Embbeding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15633"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "print('读取预训练Embbeding')\n",
    "temp_emb = dict()\n",
    "with open('/data/wujipeng/ec/data/embedding/seg_resource.bin', 'r') as f:\n",
    "    for index, line in enumerate(f.readlines()):\n",
    "        if index != 0:\n",
    "            key = line.strip().split(' ')[0]\n",
    "            vector = list(line.strip().split(' ')[1:])\n",
    "            temp_emb[key] = vector\n",
    "len(temp_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-26T15:24:19.377042Z",
     "start_time": "2019-03-26T15:24:18.938961Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53349"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_emb = dict()\n",
    "with open('/data/wujipeng/ec/data/embedding/vec_new.txt', 'r') as f:\n",
    "    for index, line in enumerate(f.readlines()):\n",
    "        if index != 0:\n",
    "            key = line.strip().split(' ')[0]\n",
    "            vector = list(line.strip().split(' ')[1:])\n",
    "            temp_emb[key] = vector\n",
    "len(temp_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "404.4px",
    "left": "1025.4px",
    "right": "20px",
    "top": "153px",
    "width": "800px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
